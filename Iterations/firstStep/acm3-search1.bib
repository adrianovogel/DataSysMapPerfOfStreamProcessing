@inproceedings{10.1145/1878537.1878698,
author = {Evans, Nathan S. and GauthierDickey, Chris and Grothoff, Christian and Grothoff, Krista and Keene, Jeff and Rutherford, Matthew J.},
title = {Simplifying Parallel and Distributed Simulation with the DUP System},
year = {2010},
isbn = {9781450300698},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
url = {https://doi.org/10.1145/1878537.1878698},
doi = {10.1145/1878537.1878698},
abstract = {This paper presents how the DUP System, a straightforward POSIX-compatible framework that enables programming-language-agnostic parallel and distributed stream processing, can be used to facilitate parallel and distributed simulations. Specifically, we describe two ways of using DUP to utilize available resources for efficient simulation: (1) a straightforward technique for parallelizing multiple runs of an existing simulation program with minimal changes, and (2) FiDES, a Discrete-Event Simulation (DES) framework built atop DUP that provides a simple, yet powerful, means of implementing a parallel and/or distributed DES. We then describe a toolset for profiling, debugging and visualization that aids the development of DUP simulations. To support these claims, we present various performance benchmarks that collectively demonstrate how DUP and FiDES can make high-performance simulation accessible to everyone.},
booktitle = {Proceedings of the 2010 Spring Simulation Multiconference},
articleno = {154},
numpages = {8},
location = {Orlando, Florida},
series = {SpringSim '10}
}

@inproceedings{10.1145/3185768.3186360,
author = {Ivanov, Todor and Taaffe, Jason},
title = {Exploratory Analysis of Spark Structured Streaming},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186360},
doi = {10.1145/3185768.3186360},
abstract = {In the Big Data era, stream processing has become a common requirement for many data-intensive applications. This has lead to many advances in the development and adaption of large scale streaming systems. Spark and Flink have become a popular choice for many developers as they combine both batch and streaming capabilities in a single system. However, introducing the Spark Structured Streaming in version 2.0 opened up completely new features for SparkSQL, which are alternatively only available in Apache Calcite. This work focuses on the new Spark Structured Streaming and analyses it by diving into its internal functionalities. With the help of a micro-benchmark consisting of streaming queries, we perform initial experiments evaluating the technology. Our results show that Spark Structured Streaming is able to run multiple queries successfully in parallel on data with changing velocity and volume sizes.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {141–146},
numpages = {6},
keywords = {big data benchmarking, spark, spark structured streaming},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1145/2940116.2940117,
author = {Casas, Pedro and D'Alconzo, Alessandro and Zseby, Tanja and Mellia, Marco},
title = {Big-DAMA: Big Data Analytics for Network Traffic Monitoring and Analysis},
year = {2016},
isbn = {9781450344265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940116.2940117},
doi = {10.1145/2940116.2940117},
abstract = {The complexity of the Internet has dramatically increased in the last few years, making it more important and challenging to design scalable Network Traffic Monitoring and Analysis (NTMA) applications and tools. Critical NTMA applications such as the detection of anomalies, network attacks and intrusions, require fast mechanisms for online analysis of thousands of events per second, as well as efficient techniques for offline analysis of massive historical data. We are witnessing a major development in Big Data Analysis Frameworks (BDAFs), but the application of BDAFs and scalable analysis techniques to the NTMA domain remains poorly understood and only in-house and difficult to benchmark solutions are conceived. In this position paper we describe the basis of the Big-DAMA research project, which aims at tackling this growing need by benchmarking and developing novel scalable techniques and frameworks capable to analyze both online network traffic data streams and offline massive traffic datasets.},
booktitle = {Proceedings of the 2016 Workshop on Fostering Latin-American Research in Data Communication Networks},
pages = {1–3},
numpages = {3},
keywords = {Data Stream Processing, Network Traffic Monitoring and Analysis, Big Data, Data Mining, Machine Learning},
location = {Florianopolis, Brazil},
series = {LANCOMM '16}
}

@inproceedings{10.1145/2038633.2038634,
author = {Nguyen, Hiep and Tan, Yongmin and Gu, Xiaohui},
title = {PAL: Propagation-Aware Anomaly Localization for Cloud Hosted Distributed Applications},
year = {2011},
isbn = {9781450309783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038633.2038634},
doi = {10.1145/2038633.2038634},
abstract = {Distributed applications running inside cloud are prone to performance anomalies due to various reasons such as insufficient resource allocations, unexpected workload increases, or software bugs. However, those applications often consist of multiple interacting components where one component anomaly may cause its dependent components to exhibit anomalous behavior as well. It is challenging to identify the faulty components among numerous distributed application components. In this paper, we present a Propagation-aware Anomaly Localization (PAL) system that can pinpoint the source faulty components in distributed applications by extracting anomaly propagation patterns. PAL provides a robust critical change point discovery algorithm to accurately capture the onset of anomaly symptoms at different application components. We then derive the propagation pattern by sorting all critical change points in chronological order. PAL is completely application-agnostic and non-intrusive, which only relies on system-level metrics. We have implemented PAL on top of the Xen platform and tested it on a production cloud computing infrastructure using the RUBiS online auction benchmark application and the IBM System S data streaming processing application with a range of common software bugs. Our experimental results show that PAL can pinpoint faulty components in distributed applications with high accuracy and low overhead.},
booktitle = {Managing Large-Scale Systems via the Analysis of System Logs and the Application of Machine Learning Techniques},
articleno = {1},
numpages = {8},
keywords = {fault localization, anomaly propagation, cloud computing},
location = {Cascais, Portugal},
series = {SLAML '11}
}

@article{10.1145/1352012.1352019,
author = {Hsu, Cheng-Hsin and Hefeeda, Mohamed},
title = {On the Accuracy and Complexity of Rate-Distortion Models for Fine-Grained Scalable Video Sequences},
year = {2008},
issue_date = {May 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/1352012.1352019},
doi = {10.1145/1352012.1352019},
abstract = {Rate-distortion (R-D) models are functions that describe the relationship between the bitrate and expected level of distortion in the reconstructed video stream. R-D models enable optimization of the received video quality in different network conditions. Several R-D models have been proposed for the increasingly popular fine-grained scalable video sequences. However, the models' relative performance has not been thoroughly analyzed. Moreover, the time complexity of each model is not known, nor is the range of bitrates in which the model produces valid results. This lack of quantitative performance analysis makes it difficult to select the model that best suits a target streaming system. In this article, we classify, analyze, and rigorously evaluate all R-D models proposed for FGS coders in the literature. We classify R-D models into three categories: analytic, empirical, and semi-analytic. We describe the characteristics of each category. We analyze the R-D models by following their mathematical derivations, scrutinizing the assumptions made, and explaining when the assumptions fail and why. In addition, we implement all R-D models, a total of eight, and evaluate them using a diverse set of video sequences. In our evaluation, we consider various source characteristics, diverse channel conditions, different encoding/decoding parameters, different frame types, and several performance metrics including accuracy, range of applicability, and time complexity of each model. We also present clear systematic ways (pseudo codes) for constructing various R-D models from a given video sequence. Based on our experimental results, we present a justified list of recommendations on selecting the best R-D models for video-on-demand, video conferencing, real-time, and peer-to-peer streaming systems.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {may},
articleno = {15},
numpages = {22},
keywords = {rate-distortion models, Multimedia streaming, fine-grained scalable coding}
}

@article{10.14778/2733004.2733048,
author = {Cetintemel, Ugur and Du, Jiang and Kraska, Tim and Madden, Samuel and Maier, David and Meehan, John and Pavlo, Andrew and Stonebraker, Michael and Sutherland, Erik and Tatbul, Nesime and Tufte, Kristin and Wang, Hao and Zdonik, Stanley},
title = {S-Store: A Streaming NewSQL System for Big Velocity Applications},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733048},
doi = {10.14778/2733004.2733048},
abstract = {First-generation streaming systems did not pay much attention to state management via ACID transactions (e.g., [3, 4]). S-Store is a data management system that combines OLTP transactions with stream processing. To create S-Store, we begin with H-Store, a main-memory transaction processing engine, and add primitives to support streaming. This includes triggers and transaction workflows to implement push-based processing, windows to provide a way to bound the computation, and tables with hidden state to implement scoping for proper isolation. This demo explores the benefits of this approach by showing how a na\"{\i}ve implementation of our benchmarks using only H-Store can yield incorrect results. We also show that by exploiting push-based semantics and our implementation of triggers, we can achieve significant improvement in transaction throughput. We demo two modern applications: (i) leaderboard maintenance for a version of "American Idol", and (ii) a city-scale bicycle rental scenario.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1633–1636},
numpages = {4}
}

@article{10.14778/3402707.3402752,
author = {Zeitler, Erik and Risch, Tore},
title = {Massive Scale-out of Expensive Continuous Queries},
year = {2020},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402707.3402752},
doi = {10.14778/3402707.3402752},
abstract = {Scalable execution of expensive continuous queries over massive data streams requires input streams to be split into parallel sub-streams. The query operators are continuously executed in parallel over these sub-streams. Stream splitting involves both partitioning and replication of incoming tuples, depending on how the continuous query is parallelized. We provide a stream splitting operator that enables such customized stream splitting. However, it is critical that the stream splitting itself keeps up with input streams of high volume. This is a problem when the stream splitting predicates have some costs. Therefore, to enable customized splitting of high-volume streams, we introduce a parallelized stream splitting operator, called parasplit. We investigate the performance of parasplit using a cost model and experimentally. Based on these results, a heuristic is devised to automatically parallelize the execution of parasplit. We show that the maximum stream rate of parasplit is network bound, and that the parallelization is energy efficient. Finally, the scalability of our approach is experimentally demonstrated on the Linear Road Benchmark, showing an order of magnitude higher stream processing rate over previously published results, allowing at least 512 expressways.},
journal = {Proc. VLDB Endow.},
month = {jun},
pages = {1181–1188},
numpages = {8}
}

@inproceedings{10.1145/2568088.2568103,
author = {Bumgardner, Vernon K.C. and Marek, Victor W.},
title = {Scalable Hybrid Stream and Hadoop Network Analysis System},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2568103},
doi = {10.1145/2568088.2568103},
abstract = {Collections of network traces have long been used in network traffic analysis. Flow analysis can be used in network anomaly discovery, intrusion detection and more generally, discovery of actionable events on the network. The data collected during processing may be also used for prediction and avoidance of traffic congestion, network capacity planning, and the development of software-defined networking rules. As network flow rates increase and new network technologies are introduced on existing hardware platforms, many organizations find themselves either technically or financially unable to generate, collect, and/or analyze network flow data. The continued rapid growth of network trace data, requires new methods of scalable data collection and analysis. We report on our deployment of a system designed and implemented at the University of Kentucky that supports analysis of network traffic across the enterprise. Our system addresses problems of scale in existing systems, by using distributed computing methodologies, and is based on a combination of stream and batch processing techniques. In addition to collection, stream processing using Storm is utilized to enrich the data stream with ephemeral environment data. Enriched stream-data is then used for event detection and near real-time flow analysis by an in-line complex event processor. Batch processing is performed by the Hadoop MapReduce framework, from data stored in HBase BigTable storage.In benchmarks on our 10 node cluster, using actual network data, we were able to stream process over 315k flows/sec. In batch analysis were we able to process over 2.6M flows/sec with a storage compression ratio of 6.7:1.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {219–224},
numpages = {6},
keywords = {netflow},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.1145/1951365.1951426,
author = {Chen, Qiming and Hsu, Meichun and Zeller, Hans},
title = {Experience in Continuous Analytics as a Service (CaaaS)},
year = {2011},
isbn = {9781450305280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1951365.1951426},
doi = {10.1145/1951365.1951426},
abstract = {Mobile applications, such as those on WebOS, increasingly depend on continuous analytics results of real-time events, for monitoring oil &amp; gas production, watching traffic status and detecting accident, etc, which has given rise to the need of providing Continuous analytics as a Service (CaaaS). While representing a paradigm shift in cloud computing, CaaaS poses several challenges in scalability, latency, time-window semantics, transaction control and result-set staging.A data stream is infinite thus can only be analyzed in granules. We propose a continuous query model over both static relations and dynamic streaming data, which allows a long-standing SQL query instance to run cycle by cycle, each cycle for a chunk of data from the data stream, using a cut-and-rewind mechanism. We further support the cycle-based transaction model with cycle-based isolation and visibility, for delivering analytics results to the clients continuously while the query is running. To have the continuously generated analytics results staged efficiently, we developed the table-ring and label switching mechanism characterized by staging data through metadata manipulation without physical data moving and copying. To scale-out analytics computation, we support both parallel database based and network distributed Map-Reduce based infrastructure with multiple cooperating engines.We have built the proposed infrastructure by extending the PostgreSQL engine. We tested the throughput and latency of this service based on a well-known stream processing benchmark; the results show that the proposed approach is highly competitive. Our experiments indicate that the database technology can be extended and applied to real-time continuous analytics service provisioning.},
booktitle = {Proceedings of the 14th International Conference on Extending Database Technology},
pages = {509–514},
numpages = {6},
keywords = {cloud service, continuous query, stream analytics},
location = {Uppsala, Sweden},
series = {EDBT/ICDT '11}
}

@article{10.1145/1870109.1870117,
author = {Yu, Chenjie and Petrov, Peter},
title = {Energy- and Performance-Efficient Communication Framework for Embedded MPSoCs through Application-Driven Release Consistency},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/1870109.1870117},
doi = {10.1145/1870109.1870117},
abstract = {We present a framework for performance-, bandwidth-, and energy-efficient intercore communication in embedded MultiProcessor Systems-on-a-Chip (MPSoC). The methodology seamlessly integrates compiler, operating system, and hardware support to achieve a low-cost communication between synchronized producers and consumers. The technique is especially beneficial for data-streaming applications exploiting pipeline parallelism with computational phases mapped to separate cores. Code transformations utilizing a simple ISA support ensure that producer writes are propagated to consumers with a single interconnect transaction per cache block just prior to the producer exiting its synchronization region. Furthermore, in order to completely eliminate misses to shared data caused by interference with private data and also to minimize the cache energy, we integrate to the proposed framework a cache way partitioning policy based on a simple cache configurability support, which isolates the shared buffers from other cache traffic. This mechanism results in significant power savings since only a subset of the cache ways needs to be looked up for each cache access. The end result of the proposed framework is a single communication transaction per shared cache block between a producer and a consumer with no coherence misses on the consumer caches. Our experiments demonstrate significant reductions in interconnect traffic, cache misses, and energy for a set of multiprocessor benchmarks.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {nov},
articleno = {8},
numpages = {39},
keywords = {Multicore}
}

