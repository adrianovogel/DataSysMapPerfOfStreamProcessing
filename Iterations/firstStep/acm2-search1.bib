@inproceedings{10.1145/3524860.3539647,
author = {Kritharakis, Emmanouil and Luo, Shengyao and Unnikrishnan, Vivek and Vombatkere, Karan},
title = {Detecting Trading Trends in Streaming Financial Data Using Apache Flink},
year = {2022},
isbn = {9781450393089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524860.3539647},
doi = {10.1145/3524860.3539647},
abstract = {Modern financial analytics rely on high-volume streams of event notifications that report live market fluctuations based on supply and demand. Accurately identifying trends or breakout patterns based on the Exponential Moving Average (EMA) in the development of an instrument's price early on is an important challenge, so as to buy while the price is low and sell before a downtrend begins.This paper aims to solve the above challenge with a distributed, event-streaming solution built using Apache Flink. We present and implement a solution that leverages customized window operators to calculate the EMA and find breakout patterns, using event generation parallelism to facilitate the rapid processing of the input stream uses sinks to collect and output results, and scales easily on a distributed Flink cluster. We empirically test our design on metrics specified by the benchmarking platform for the DEBS 2022 Grand Challenge and observe a throughput of 45 batches per second and an average latency of 120 ms.},
booktitle = {Proceedings of the 16th ACM International Conference on Distributed and Event-Based Systems},
pages = {145–150},
numpages = {6},
keywords = {Apache Flink, financial data, stream processing},
location = {Copenhagen, Denmark},
series = {DEBS '22}
}

@inproceedings{10.1145/2463676.2465282,
author = {Castro Fernandez, Raul and Migliavacca, Matteo and Kalyvianaki, Evangelia and Pietzuch, Peter},
title = {Integrating Scale out and Fault Tolerance in Stream Processing Using Operator State Management},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465282},
doi = {10.1145/2463676.2465282},
abstract = {As users of "big data" applications expect fresh results, we witness a new breed of stream processing systems (SPS) that are designed to scale to large numbers of cloud-hosted machines. Such systems face new challenges: (i) to benefit from the "pay-as-you-go" model of cloud computing, they must scale out on demand, acquiring additional virtual machines (VMs) and parallelising operators when the workload increases; (ii) failures are common with deployments on hundreds of VMs-systems must be fault-tolerant with fast recovery times, yet low per-machine overheads. An open question is how to achieve these two goals when stream queries include stateful operators, which must be scaled out and recovered without affecting query results.Our key idea is to expose internal operator state explicitly to the SPS through a set of state management primitives. Based on them, we describe an integrated approach for dynamic scale out and recovery of stateful operators. Externalised operator state is checkpointed periodically by the SPS and backed up to upstream VMs. The SPS identifies individual operator bottlenecks and automatically scales them out by allocating new VMs and partitioning the checkpointed state. At any point, failed operators are recovered by restoring checkpointed state on a new VM and replaying unprocessed tuples. We evaluate this approach with the Linear Road Benchmark on the Amazon EC2 cloud platform and show that it can scale automatically to a load factor of L=350 with 50 VMs, while recovering quickly from failures.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {725–736},
numpages = {12},
keywords = {scalability, fault tolerance, stateful stream processing},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@article{10.1145/3319498,
author = {Izadpanah, Ramin and Allan, Benjamin A. and Dechev, Damian and Brandt, Jim},
title = {Production Application Performance Data Streaming for System Monitoring},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2376-3639},
url = {https://doi.org/10.1145/3319498},
doi = {10.1145/3319498},
abstract = {In this article, we present an approach to streaming collection of application performance data. Practical application performance tuning and troubleshooting in production high-performance computing (HPC) environments requires an understanding of how applications interact with the platform, including (but not limited to) parallel programming libraries such as Message Passing Interface (MPI). Several profiling and tracing tools exist that collect heavy runtime data traces either in memory (released only at application exit) or on a file system (imposing an I/O load that may interfere with the performance being measured). Although these approaches are beneficial in development stages and post-run analysis, a systemwide and low-overhead method is required to monitor deployed applications continuously. This method must be able to collect information at both the application and system levels to yield a complete performance picture.In our approach, an application profiler collects application event counters. A sampler uses an efficient inter-process communication method to periodically extract the application counters and stream them into an infrastructure for performance data collection. We implement a tool-set based on our approach and integrate it with the Lightweight Distributed Metric Service (LDMS) system, a monitoring system used on large-scale computational platforms. LDMS provides the infrastructure to create and gather streams of performance data in a low overhead manner. We demonstrate our approach using applications implemented with MPI, as it is one of the most common standards for the development of large-scale scientific applications.We utilize our tool-set to study the impact of our approach on an open source HPC application, Nalu. Our tool-set enables us to efficiently identify patterns in the behavior of the application without source-level knowledge. We leverage LDMS to collect system-level performance data and explore the correlation between the system and application events. Also, we demonstrate how our tool-set can help detect anomalies with a low latency. We run tests on two different architectures: a system enabled with Intel Xeon Phi and another system equipped with Intel Xeon processor. Our overhead study shows our method imposes at most 0.5% CPU usage overhead on the application in realistic deployment scenarios.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {apr},
articleno = {8},
numpages = {25},
keywords = {performance data streaming, application profiling, Application and system monitoring}
}

@inproceedings{10.1145/2933267.2933516,
author = {Choi, Joong-Hyun and Lee, Kang-Woo and Cho, Eun-Sun},
title = {Experience of Event Stream Processing for Top-k Queries and Dynamic Graphs},
year = {2016},
isbn = {9781450340212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933267.2933516},
doi = {10.1145/2933267.2933516},
abstract = {Solving 2016 ACM DEBS Grand Challenge problems entails both dynamic graph processing and top-k query processing. A straightforward implementation of solutions would not guarantee good performance or prompt responses. This paper shows our experience in implementing solutions of the problems, including rationales of top-k list management techniques we used in our implementation. We also shows the performance evaluation results among three top-k list management schemes and present the reason for our choice for the final result.},
booktitle = {Proceedings of the 10th ACM International Conference on Distributed and Event-Based Systems},
pages = {330–335},
numpages = {6},
keywords = {dynamic graphs, event stream processing, continuous top-k queries},
location = {Irvine, California},
series = {DEBS '16}
}

@inproceedings{10.1145/3366030.3366105,
author = {Shaikh, Salman Ahmed and Lee, Jun and Matono, Akiyoshi and Kim, Kyoung-Sook},
title = {A Robust and Scalable Pipeline for the Real-Time Processing and Analysis of Massive 3D Spatial Streams},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366105},
doi = {10.1145/3366030.3366105},
abstract = {With the increase in the use of 3D scanner to sample the earth surface, there is a surge in the availability of 3D spatial data. 3D spatial data contains a wealth of information and can be of potential use if integrated, processed and analyzed in real-time. The 3D spatial data is generated as continuous data stream, however due to its size, velocity and inherent noise, it is processed offline. Many applications require real-time processing and analysis of spatial stream, for-instance, forest fire management, real-time road traffic analysis, disaster engulfed areas monitoring, etc., however they suffer from slow offline processing of traditional systems. This paper presents and demonstrates a robust and scalable pipeline for the real-time processing and analysis of 3D spatial streams. An experimental evaluation is also presented to prove the effectiveness of the proposed framework.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {622–626},
numpages = {5},
keywords = {Real-time analysis, 3D spatial data, Stream processing, Point cloud data, Real-time processing},
location = {Munich, Germany},
series = {iiWAS2019}
}

@article{10.1145/3170432,
author = {Dayarathna, Miyuru and Perera, Srinath},
title = {Recent Advancements in Event Processing},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3170432},
doi = {10.1145/3170432},
abstract = {Event processing (EP) is a data processing technology that conducts online processing of event information. In this survey, we summarize the latest cutting-edge work done on EP from both industrial and academic research community viewpoints. We divide the entire field of EP into three subareas: EP system architectures, EP use cases, and EP open research topics. Then we deep dive into the details of each subsection. We investigate the system architecture characteristics of novel EP platforms, such as Apache Storm, Apache Spark, and Apache Flink. We found significant advancements made on novel application areas, such as the Internet of Things; streaming machine learning (ML); and processing of complex data types such as text, video data streams, and graphs. Furthermore, there has been significant body of contributions made on event ordering, system scalability, development of EP languages and exploration of use of heterogeneous devices for EP, which we investigate in the latter half of this article. Through our study, we found key areas that require significant attention from the EP community, such as Streaming ML, EP system benchmarking, and graph stream processing.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {33},
numpages = {36},
keywords = {complex event processing, Event processing, data stream processing}
}

@inproceedings{10.5555/1898953.1899050,
author = {Zhang, Li and Parashar, Manish},
title = {Enabling Efficient and Flexible Coupling of Parallel Scientific Applications},
year = {2006},
isbn = {1424400546},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Emerging scientific and engineering simulations are presenting challenging requirements for coupling between multiple physics models and associated parallel codes that execute independently and in a distributed manner. Realizing coupled simulations requires an efficient, flexible and scalable coupling framework and simple programming abstractions. This paper presents a coupling framework that addresses these requirements. The framework is based on the Seine geometry-based interaction model. It enables efficient computation of communication schedules, supports low-overheads processor-to-processor data streaming, and provides high-level abstraction for application developers. The design, CCA-based implementation, and experimental evaluation of the Seine based coupling framework are presented.},
booktitle = {Proceedings of the 20th International Conference on Parallel and Distributed Processing},
pages = {117},
numpages = {1},
location = {Rhodes Island, Greece},
series = {IPDPS'06}
}

@inproceedings{10.1145/1142351.1142394,
author = {Zhao, Qi (George) and Ogihara, Mitsunori and Wang, Haixun and Xu, Jun (Jim)},
title = {Finding Global Icebergs over Distributed Data Sets},
year = {2006},
isbn = {1595933182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142351.1142394},
doi = {10.1145/1142351.1142394},
abstract = {Finding icebergs–items whose frequency of occurrence is above a certain threshold–is an important problem with a wide range of applications. Most of the existing work focuses on iceberg queries at a single node. However, in many real-life applications, data sets are distributed across a large number of nodes. Two na\"{\i}ve approaches might be considered. In the first, each node ships its entire data set to a central server, and the central server uses single-node algorithms to find icebergs. But it may incur prohibitive communication overhead. In the second, each node submits local icebergs, and the central server combines local icebergs to find global icebergs. But it may fail because in many important applications, globally frequent items may not be frequent at any node. In this work, we propose two novel schemes that provide accurate and efficient solutions to this problem: a sampling-based scheme and a counting-sketch-based scheme. In particular, the latter scheme incurs a communication cost at least an order of magnitude smaller than the na\"{\i}ve scheme of shipping all data, yet is able to achieve very high accuracy. Through rigorous theoretical and experimental analysis we establish the statistical properties of our proposed algorithms, including their accuracy bounds.},
booktitle = {Proceedings of the Twenty-Fifth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
pages = {298–307},
numpages = {10},
keywords = {statistical inference, data streaming, icebergs},
location = {Chicago, IL, USA},
series = {PODS '06}
}

@article{10.1145/2904080,
author = {Ghaderi, Javad and Shakkottai, Sanjay and Srikant, R.},
title = {Scheduling Storms and Streams in the Cloud},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/2904080},
doi = {10.1145/2904080},
abstract = {Motivated by emerging big streaming data processing paradigms (e.g., Twitter Storm, Streaming MapReduce), we investigate the problem of scheduling graphs over a large cluster of servers. Each graph is a job, where nodes represent compute tasks and edges indicate data flows between these compute tasks. Jobs (graphs) arrive randomly over time and, upon completion, leave the system. When a job arrives, the scheduler needs to partition the graph and distribute it over the servers to satisfy load balancing and cost considerations. Specifically, neighboring compute tasks in the graph that are mapped to different servers incur load on the network; thus a mapping of the jobs among the servers incurs a cost that is proportional to the number of “broken edges.” We propose a low-complexity randomized scheduling algorithm that, without service preemptions, stabilizes the system with graph arrivals/departures; more importantly, it allows a smooth tradeoff between minimizing average partitioning cost and average queue lengths. Interestingly, to avoid service preemptions, our approach does not rely on a Gibbs sampler; instead, we show that the corresponding limiting invariant measure has an interpretation stemming from a loss system.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {aug},
articleno = {14},
numpages = {28},
keywords = {Markov chains, dynamic resource allocation, stability, Graph partitioning}
}

@inproceedings{10.1145/2934495.2934496,
author = {Nikolakopoulos, Yiannis and Papatriantafilou, Marina and Brauer, Peter and Lundqvist, Martin and Gulisano, Vincenzo and Tsigas, Philippas},
title = {Highly Concurrent Stream Synchronization in Many-Core Embedded Systems},
year = {2016},
isbn = {9781450342629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934495.2934496},
doi = {10.1145/2934495.2934496},
abstract = {Embedded many-core architectures are expected to serve as significant components in the infrastructure of upcoming technologies like networks for the Internet of Things (IoT), facing real-time and stream processing challenges. In this work we explore the applicability of ScaleGate, a synchronization object from the massive data stream processing domain, on many-core embedded systems. We propose a new implementation of ScaleGate on the Epiphany architecture, a scalable embedded many-core co-processor, and study communication patterns that appear in the context of a baseband signal processing application. Our experimental evaluation shows significant improvements over standard barrier-based approaches, due to the asynchrony exploited by the use of ScaleGate.},
booktitle = {Proceedings of the Third ACM International Workshop on Many-Core Embedded Systems},
pages = {2–9},
numpages = {8},
keywords = {synchronization, ScaleGate, Epiphany, streaming},
location = {Seoul, Republic of Korea},
series = {MES '16}
}

@inproceedings{10.1145/3401025.3402684,
author = {Gulisano, Vincenzo and Jorde, Daniel and Mayer, Ruben and Najdataei, Hannaneh and Palyvos-Giannas, Dimitris},
title = {The DEBS 2020 Grand Challenge},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3402684},
doi = {10.1145/3401025.3402684},
abstract = {The ACM DEBS 2020 Grand Challenge is the tenth in a series of challenges which seek to provide a common ground and evaluation criteria for a competition aimed at both research and industrial event-based systems. The focus of the ACM DEBS 2020 Grand Challenge is on Non-Intrusive Load Monitoring (NILM). The goal of the challenge is to detect when appliances contributing to an aggregated stream of voltage and current readings from a smart meter are switched on or off. NILM is leveraged in many contexts, ranging from monitoring of energy consumption to home automation. This paper describes the specifics of the data streams provided in the challenge, as well as the benchmarking platform that supports the testing of the solutions submitted by the participants.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {183–186},
numpages = {4},
keywords = {event processing, NILM, data streaming},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/3524860.3539645,
author = {Frischbier, Sebastian and Tahir, Jawad and Doblander, Christoph and Hormann, Arne and Mayer, Ruben and Jacobsen, Hans-Arno},
title = {Detecting Trading Trends in Financial Tick Data: The DEBS 2022 Grand Challenge},
year = {2022},
isbn = {9781450393089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524860.3539645},
doi = {10.1145/3524860.3539645},
abstract = {The DEBS Grand Challenge (GC) is an annual programming competition open to practitioners from both academia and industry. The GC 2022 edition focuses on real-time complex event processing of high-volume tick data provided by Infront Financial Technology GmbH. The goal of the challenge is to efficiently compute specific trend indicators and detect patterns in these indicators like those used by real-life traders to decide on buying or selling in financial markets. The data set Trading Data used for benchmarking contains 289 million tick events from approximately 5500+ financial instruments that had been traded on the three major exchanges Amsterdam (NL), Paris (FR), and Frankfurt am Main (GER) over the course of a full week in 2021. The data set is made publicly available. In addition to correctness and performance, submissions must explicitly focus on reusability and practicability. Hence, participants must address specific nonfunctional requirements and are asked to build upon open-source platforms. This paper describes the required scenario and the data set Trading Data, defines the queries of the problem statement, and explains the enhancements made to the evaluation platform Challenger that handles data distribution, dynamic subscriptions, and remote evaluation of the submissions.},
booktitle = {Proceedings of the 16th ACM International Conference on Distributed and Event-Based Systems},
pages = {132–138},
numpages = {7},
keywords = {trading, event processing, data streaming, technical analysis},
location = {Copenhagen, Denmark},
series = {DEBS '22}
}

@inproceedings{10.1145/1289612.1289615,
author = {Amini, Lisa and Andrade, Henrique and Bhagwan, Ranjita and Eskesen, Frank and King, Richard and Selo, Philippe and Park, Yoonho and Venkatramani, Chitra},
title = {SPC: A Distributed, Scalable Platform for Data Mining},
year = {2006},
isbn = {159593443X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1289612.1289615},
doi = {10.1145/1289612.1289615},
abstract = {The Stream Processing Core (SPC) is distributed stream processing middleware designed to support applications that extract information from a large number of digital data streams. In this paper, we describe the SPC programming model which, to the best of our knowledge, is the first to support stream-mining applications using a subscription-like model for specifying stream connections as well as to provide support for non-relational operators. This enables stream-mining applications to tap into, analyze and track an ever-changing array of data streams which may contain information relevant to the streaming-queries placed on it. We describe the design, implementation, and experimental evaluation of the SPC distributed middleware, which deploys applications on to the running system in an incremental fashion, making stream connections as required. Using micro-benchmarks and a representative large-scale synthetic stream-mining application, we evaluate the performance of the control and data paths of the SPC middleware.},
booktitle = {Proceedings of the 4th International Workshop on Data Mining Standards, Services and Platforms},
pages = {27–37},
numpages = {11},
location = {Philadelphia, Pennsylvania},
series = {DMSSP '06}
}

@inproceedings{10.1145/3423211.3425668,
author = {Jonathan, Albert and Chandra, Abhishek and Weissman, Jon},
title = {WASP: Wide-Area Adaptive Stream Processing},
year = {2020},
isbn = {9781450381536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423211.3425668},
doi = {10.1145/3423211.3425668},
abstract = {Adaptability is critical for stream processing systems to ensure stable, low-latency, and high-throughput processing of long-running queries. Such adaptability is particularly challenging for wide-area stream processing due to the highly dynamic nature of the wide-area environment, which includes unpredictable workload patterns, variable network bandwidth, occurrence of stragglers, and failures. Unfortunately, existing adaptation techniques typically achieve these performance goals by compromising the quality/accuracy of the results, and they are often application-dependent. In this work, we rethink the adaptability property of wide-area stream processing systems and propose a resource-aware adaptation framework, called WASP. WASP adapts queries through a combination of multiple techniques: task re-assignment, operator scaling, and query re-planning, and applies them in a WAN-aware manner. It is able to automatically determine which adaptation action to take depending on the type of queries, dynamics, and optimization goals. We have implemented a WASP prototype on Apache Flink. Experimental evaluation with the YSB benchmark and a real Twitter trace shows that WASP can handle various dynamics without compromising the quality of the results.},
booktitle = {Proceedings of the 21st International Middleware Conference},
pages = {221–235},
numpages = {15},
location = {Delft, Netherlands},
series = {Middleware '20}
}

@inproceedings{10.1145/3132747.3132750,
author = {Venkataraman, Shivaram and Panda, Aurojit and Ousterhout, Kay and Armbrust, Michael and Ghodsi, Ali and Franklin, Michael J. and Recht, Benjamin and Stoica, Ion},
title = {Drizzle: Fast and Adaptable Stream Processing at Scale},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132750},
doi = {10.1145/3132747.3132750},
abstract = {Large scale streaming systems aim to provide high throughput and low latency. They are often used to run mission-critical applications, and must be available 24x7. Thus such systems need to adapt to failures and inherent changes in workloads, with minimal impact on latency and throughput. Unfortunately, existing solutions require operators to choose between achieving low latency during normal operation and incurring minimal impact during adaptation. Continuous operator streaming systems, such as Naiad and Flink, provide low latency during normal execution but incur high overheads during adaptation (e.g., recovery), while micro-batch systems, such as Spark Streaming and FlumeJava, adapt rapidly at the cost of high latency during normal operations.Our key observation is that while streaming workloads require millisecond-level processing, workload and cluster properties change less frequently. Based on this, we develop Drizzle, a system that decouples the processing interval from the coordination interval used for fault tolerance and adaptability. Our experiments on a 128 node EC2 cluster show that on the Yahoo Streaming Benchmark, Drizzle can achieve end-to-end record processing latencies of less than 100ms and can get 2-3x lower latency than Spark. Drizzle also exhibits better adaptability, and can recover from failures 4x faster than Flink while having up to 13x lower latency during recovery.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {374–389},
numpages = {16},
keywords = {Stream Processing, Performance, Reliability},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{10.1145/2931028.2931033,
author = {Roloff, Sascha and P\"{o}ppl, Alexander and Schwarzer, Tobias and Wildermann, Stefan and Bader, Michael and Gla\ss{}, Michael and Hannig, Frank and Teich, J\"{u}rgen},
title = {ActorX10: An Actor Library for X10},
year = {2016},
isbn = {9781450343862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931028.2931033},
doi = {10.1145/2931028.2931033},
abstract = {The APGAS programming model is a powerful computing paradigm for multi-core and massively parallel computer architectures. It allows for the dynamic creation and distribution of thousands of threads amongst hundreds of nodes in a cluster computer within a single application. For programs of such a complexity, appropriate higher level abstractions on computation and communication are necessary for performance analysis and optimization. In this work, we present actorX10, an X10 library of a formally specified actor model based on the APGAS principles. The realized actor model explicitly exposes communication paths and decouples these from the control flow of the concurrently executed application components. Our approach provides the right abstraction for a wide range of applications. Its capabilities and advantages are introduced and demonstrated for two applications from the embedded system and HPC domain, i.e., an object detection chain and a proxy application for the simulation of tsunami events.},
booktitle = {Proceedings of the 6th ACM SIGPLAN Workshop on X10},
pages = {24–29},
numpages = {6},
keywords = {high performance computing, stream processing, Parallel programming, actor-based programming},
location = {Santa Barbara, CA, USA},
series = {X10 2016}
}

@inproceedings{10.1145/3210284.3210297,
author = {Gupta, Harshit and Ramachandran, Umakishore},
title = {FogStore: A Geo-Distributed Key-Value Store Guaranteeing Low Latency for Strongly Consistent Access},
year = {2018},
isbn = {9781450357821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210284.3210297},
doi = {10.1145/3210284.3210297},
abstract = {We design Fogstore, a key-value store for event-based systems, that exploits the concept of relevance to guarantee low-latency access to relevant data with strong consistency guarantees, while providing tolerance from geographically correlated failures. Distributed event-based processing pipelines are envisioned to utilize the resources of densely geo-distributed infrastructures for low-latency responses - enabling real-time applications. Increasing complexity of such applications results in higher dependence on state, which has driven the incorporation of state-management as a core functionality of contemporary stream processing engines a la Apache Flink and Samza. Processing components executing under the same context (like location) often produce information that may be relevant to others, thereby necessitating shared state and an out-of-band globally-accessible data-store. Efficient access to application state is critical for overall performance, thus centralized data-stores are not a viable option due to the high-latency of network traversals. On the other hand, a highly geo-distributed datastore with low-latency implemented with current key-value stores would necessitate degrading client expectation of consistency as per the PACELC theorem. In this paper we exploit the notion of contextual relevance of events (data) in situation-awareness applications - and offer differential consistency guarantees for clients based on their context. We highlight important systems concerns that may arise with a highly geo-distributed system and show how Fogstore's design tackles them. We present, in detail, a prototype implementation of Fogstore's mechanisms on Apache Cassandra and a performance evaluation. Our evaluations show that Fogstore is able to achieve the throughput of eventually consistent configurations while serving data with strong consistency to the contextually relevant clients.},
booktitle = {Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
pages = {148–159},
numpages = {12},
keywords = {context awareness, edge computing, Distributed key-value stores, latency-consistency trade-off},
location = {Hamilton, New Zealand},
series = {DEBS '18}
}

@inproceedings{10.1145/3093742.3093911,
author = {Ritter, Daniel and Dann, Jonas and May, Norman and Rinderle-Ma, Stefanie},
title = {Hardware Accelerated Application Integration Processing: Industry Paper},
year = {2017},
isbn = {9781450350655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093742.3093911},
doi = {10.1145/3093742.3093911},
abstract = {The growing number of (cloud) applications and devices massively increases the communication rate and volume pushing integration systems to their (throughput) limits. While the usage of modern hardware like Field Programmable Gate Arrays (FPGAs) led to low latency when employed for query and event processing, application integration adds yet unexplored processing opportunities. In this industry paper, we explore how to program integration semantics (e. g., message routing and transformation) in form of Enterprise Integration Patterns (EIP) on top of an FPGA, thus complementing the existing research on FPGA data processing. We focus on message routing, re-define the EIP for stream processing and propose modular hardware implementations as templates that are synthesized to circuits. For our real-world "connected car" scenario (i. e., composed patterns), we discuss common and new optimizations especially relevant for hardware integration processes. Our experimental evaluation shows competitive throughput compared to modern general-purpose CPUs and discusses the results.},
booktitle = {Proceedings of the 11th ACM International Conference on Distributed and Event-Based Systems},
pages = {215–226},
numpages = {12},
location = {Barcelona, Spain},
series = {DEBS '17}
}

@inproceedings{10.1145/2335484.2335506,
author = {Hirzel, Martin},
title = {Partition and Compose: Parallel Complex Event Processing},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335506},
doi = {10.1145/2335484.2335506},
abstract = {Complex event processing uses patterns to detect composite events in streams of simple events. Typically, the events are logically partitioned by some key. For instance, the key can be the stock symbol in stock quotes, the author in tweets, the vehicle in transportation, or the patient in health-care. Composite event patterns often become meaningful only after partitioning. For instance, a pattern over stock quotes is typically meaningful over quotes for the same stock symbol. This paper proposes a pattern syntax and translation scheme organized around the notion of partitions. Besides making patterns meaningful, partitioning also benefits performance, since different keys can be processed in parallel. We have implemented partitioned parallel complex event processing as an extension to IBM's System S high-performance streaming platform. Our experiments with several benchmarks from finance and social media demonstrate processing speeds of up to 830,000 events per second, and substantial speedups for expensive patterns parallelized on multi-core machines as well as multi-machine clusters. Partitioning the event stream before detecting composite events makes event processing both more intuitive and parallel.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {191–200},
numpages = {10},
keywords = {composite events, automata, stream processing, regular expressions, pattern matching, parallelism, SPL, CEP},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.1145/3465480.3466930,
author = {Mari\'{c}, Josip and Pripu\v{z}i\'{c}, Kre\v{s}imir and Antoni\'{c}, Martina},
title = {DEBS Grand Challenge: Real-Time Detection of Air Quality Improvement with Apache Flink},
year = {2021},
isbn = {9781450385558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465480.3466930},
doi = {10.1145/3465480.3466930},
abstract = {The topic of the DEBS Grand Challenge 2021 is to develop a solution for detecting areas in which the air quality index (AQI) improved the most when compared to the previous year. The solution must run two given continuous queries in parallel on the incoming sensor data stream which must return the following: 1) a top 50 cities in terms of AQI improvement with their current AQIs and 2) a histogram of the longest streaks of good AQI. The incoming data is accessed through an API which provides streaming sensor measurements in batches. We present our solution based on Apache Flink, a distributed stream processing framework for the cluster. We opted for Flink since its applications can easily be scaled horizontally and vertically by adding computation nodes or increasing available resources, respectively. Flink allows us to divide the given queries into smaller tasks which can be run concurrently on different nodes in order to reduce the overall processing time and thus improve the performance of our solution. In more detail, the following performance intensive tasks are run in parallel on distributed nodes: 1) retrieving measurement batches, 2) assigning a city to each measurement and 3) calculating air quality index per city. We also discuss the main optimizations we have used to improve the performance and present an experimental evaluation of our solution.},
booktitle = {Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
pages = {148–153},
numpages = {6},
keywords = {sensor streams, geospatial data streams, big data, fast data},
location = {Virtual Event, Italy},
series = {DEBS '21}
}

@inproceedings{10.1145/3030207.3030227,
author = {Ravindra, Sajith and Dayarathna, Miyuru and Jayasena, Sanath},
title = {Latency Aware Elastic Switching-Based Stream Processing Over Compressed Data Streams},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030227},
doi = {10.1145/3030207.3030227},
abstract = {Elastic scaling of event stream processing systems has gained significant attention recently due to the prevalence of cloud computing technologies. We investigate on the complexities associated with elastic scaling of an event processing system in a private/public cloud scenario. We develop an Elastic Switching Mechanism (ESM) which reduces the overall average latency of event processing jobs by significant amount considering the cost of operating the system. ESM is augmented with adaptive compressing of upstream data. The ESM conducts one of the two types of switching where either part of the data is sent to the public cloud (data switching) or a selected query is sent to the public cloud (query switching) based on the characteristics of the query. We model the operation of the ESM as the function of two binary switching functions. We show that our elastic switching mechanism with compression is capable of handling out-of-order events more efficiently compared to techniques which does not involve compression. We used two application benchmarks called EmailProcessor and a Social Networking Benchmark (SNB2016) to conduct multiple experiments to evaluate the effectiveness of our approach. In a single query deployment with EmailProcessor benchmark we observed that our elastic switching mechanism provides 1.24 seconds average latency improvement per processed event which is 16.70% improvement compared to private cloud only deployment. When presented the option of scaling EmailProcessor with four public cloud VMs ESM further reduced the average latency by 37.55% compared to the single public cloud VM. In a multi-query deployment with both EmailProcessor and SNB2016 we obtained a reduction of average latency of both the queries by 39.61 seconds which is a decrease of 7% of overall latency. These performance figures indicate that our elastic switching mechanism with compressed data streams can effectively reduce the average elapsed time of stream processing happening in private/public clouds.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {91–102},
numpages = {12},
keywords = {event-based systems, system sizing and capacity planning, compressed event processing, iass, data compression, cloud computing, software performance engineering, elastic data stream processing},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/2076623.2076631,
author = {Cipriani, Nazario and Schiller, Oliver and Mitschang, Bernhard},
title = {M-TOP: Multi-Target Operator Placement of Query Graphs for Data Streams},
year = {2011},
isbn = {9781450306270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2076623.2076631},
doi = {10.1145/2076623.2076631},
abstract = {Nowadays, many applications processes stream-based data, such as financial market analysis, network intrusion detection, or visualization applications. To process stream-based data in an application-independent manner, distributed stream processing systems emerged. They typically translate a query to an operator graph, place the operators to stream processing nodes, and execute them to process the streamed data. The operator placement is crucial in such systems, as it deeply influences query execution. Often, different stream-based applications require dedicated placement of query graphs according to their specific objectives, e.g. bandwidth not less than 500 MBit/s and costs not more that 1 cost unit. This fact constraints operator placement. Existing approaches do not take into account application-specific objectives, thus not reflecting application-specific placement decisions. As objectives might conflict among each other, operator placement is subject to delicate trade-offs, such as bandwidth maximization is more important than cost reduction. Thus, the challenge is to find a solution which considers the application-specific objectives and their trade-offs.We present M-TOP, an QoS-aware multi-target operator placement framework for data stream systems. Particularly, we propose an operator placement strategy considering application-specific targets consisting of objectives, their respective trade-offs specifications, bottleneck conditions, and ranking schemes to compute a suitable placement. We integrated M-TOP into NexusDS, our distributed data stream processing middleware, and provide an experimental evaluation to show the effectiveness of M-TOP.},
booktitle = {Proceedings of the 15th Symposium on International Database Engineering &amp; Applications},
pages = {52–60},
numpages = {9},
location = {Lisboa, Portugal},
series = {IDEAS '11}
}

@inproceedings{10.1145/3267809.3267842,
author = {Jonathan, Albert and Chandra, Abhishek and Weissman, Jon},
title = {Multi-Query Optimization in Wide-Area Streaming Analytics},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3267842},
doi = {10.1145/3267809.3267842},
abstract = {Wide-area data analytics has gained much attention in recent years due to the increasing need for analyzing data that are geographically distributed. Many of such queries often require real-time analysis on data streams that are continuously being generated across multiple locations. Yet, analyzing these geo-distributed data streams in a timely manner is very challenging due to the highly heterogeneous and limited bandwidth availability of the wide-area network (WAN). This paper examines the opportunity of applying multi-query optimization in the context of wide-area streaming analytics, with the goal of utilizing WAN bandwidth efficiently while achieving high throughput and low latency execution. Our approach is based on the insight that many streaming analytics queries often exhibit common executions, whether in consuming a common set of input data or performing the same data processing. In this work, we study different types of sharing opportunities and propose a practical online algorithm that allows streaming analytics queries to share their common executions incrementally. We further address the importance of WAN awareness in applying multi-query optimization. Without WAN awareness, sharing executions in a wide-area environment may lead to performance degradation. We have implemented our WAN-aware multi-query optimization in a prototype implementation based on Apache Flink. Experimental evaluation using Twitter traces on a real wide-area system deployment across geo-distributed EC2 data centers shows that our technique is able to achieve 21% higher throughput while saving WAN bandwidth consumption by 33% compared to a WAN-aware, sharing-agnostic system.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {412–425},
numpages = {14},
keywords = {stream processing systems, Geo-distributed systems, multi-query optimization, execution sharing},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@article{10.1145/3156655.3156661,
author = {Esteves, Sergi and Janssens, Nico and Theeten, Bart and Veiga, Luis},
title = {Empowering Stream Processing through Edge Clouds},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3156655.3156661},
doi = {10.1145/3156655.3156661},
abstract = {CHive is a new streaming analytics platform to run distributed SQL-style queries on edge clouds. However, CHive is currently tightly coupled to a specific stream processing system (SPS), Apache Storm. In this paper we address the decoupling of the CHive query planner and optimizer from the runtime environment, and also extend the latter to support pluggable runtimes through a common API. As runtimes, we currently support Apache Spark and Flink streaming. The fundamental contribution of this paper is to assess the cost of employing interstream parallelism in SPS. Experimental evaluation indicates that we can enable popular SPS to be distributed on edge clouds with stable overhead in terms of throughput},
journal = {SIGMOD Rec.},
month = {oct},
pages = {23–28},
numpages = {6}
}

@inproceedings{10.1145/3350489.3350495,
author = {Prasaad, Guna and Ramalingam, G. and Rajan, Kaushik},
title = {Scaling Ordered Stream Processing on Shared-Memory Multicores},
year = {2019},
isbn = {9781450376600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350489.3350495},
doi = {10.1145/3350489.3350495},
abstract = {Many modern applications require realtime processing of large volumes of high speed data. Such data processing can be specified in the form of a dataflow graph that exposes multiple opportunities for parallelizing its execution, in the form of data, pipeline and task parallelism. This paper focuses on the problem of effectively parallelizing ordered streaming computations by exploiting low overhead, shared memory parallelism on a multicore machine. We propose an adaptive runtime that dynamically maps the exposed parallelism in a streaming computation to that of the machine using scheduling heuristics. Further, we address some key problems in effectively realizing ordered data parallelism. We propose a new approach for parallelizing partitioned stateful operators that can handle load imbalance across partitions effectively and mostly avoid delays due to ordering constraints. We also present a low latency, non-blocking concurrent data structure to order outputs produced by concurrent workers on an operator. Finally, we perform an in-depth empirical evaluation illustrating the trade-offs and effectiveness of our concurrent data-structures and scheduling heuristics using micro-benchmarks and on the TPCx-BB benchmark.},
booktitle = {Proceedings of Real-Time Business Intelligence and Analytics},
articleno = {6},
numpages = {10},
keywords = {multicore, dynamic scheduling, parallelism, shared memory, concurrent data structures, stream processing},
location = {Los Angeles, CA, USA},
series = {BIRTE 2019}
}

@inproceedings{10.1145/1274971.1275004,
author = {Fang, Zhen and Zhang, Lixin and Carter, John B. and Ibrahim, Ali and Parker, Michael A.},
title = {Active Memory Operations},
year = {2007},
isbn = {9781595937681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1274971.1275004},
doi = {10.1145/1274971.1275004},
abstract = {The performance of modern microprocessors is increasingly limited by their inability to hide main memory latency. The problem is worse in large-scale shared memory systems, where remote memory latencies are hundreds, and soon thousands, of processor cycles. To mitigate this problem, we propose the use of Active Memory Operations (AMOs), in which select operations can be sent to and executed on the home memory controller of data. AMOs can eliminate significant number of coherence messages, minimize intranode and internode memory traffic, and create opportunities for parallelism. Our implementation of AMOs is cache-coherent and requires no changes to the processor core or DRAM chips.In this paper we present architectural and programming models for AMOs, and compare its performance to that of several other memory architectures on a variety of scientific and commercial benchmarks. Through simulation we show that AMOs offer dramatic performance improvements for an important set of data-intensive operations, e.g., up to 50X faster barriers, 12X faster spinlocks, 8.5X-15X faster stream/array operations, and 3X faster database queries. Based on a standard cell implementation, we predict that the circuitry required to support AMOs is less than 1% of the typical chip area of a high performance microprocessor.},
booktitle = {Proceedings of the 21st Annual International Conference on Supercomputing},
pages = {232–241},
numpages = {10},
keywords = {thread synchronization, memory performance, stream processing, distributed shared memory, cache coherence, DRAM},
location = {Seattle, Washington},
series = {ICS '07}
}

@inproceedings{10.1145/1326320.1326323,
author = {Pianese, Fabio and Perino, Diego},
title = {Resource and Locality Awareness in an Incentive-Based P2P Live Streaming System},
year = {2007},
isbn = {9781595937896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1326320.1326323},
doi = {10.1145/1326320.1326323},
abstract = {One of the main challenges in P2P live streaming is the efficient allocation of the available resources. This paper presents an experimental evaluation of the effects of a local pairwise incentive mechanism applied to an unstructured mesh-based architecture. We focus on the relationship between resource availability in the system and the average quality of its data distribution paths, both in terms of bandwidth efficiency and awareness to network locality. We show via large scale testbed experiments based on the PULSE live streaming system that the introduction of appropriate incentive-based policies as the main peer selection mechanism can lead to a global content distribution mesh which has properties similar to tree-based structured systems.},
booktitle = {Proceedings of the 2007 Workshop on Peer-to-Peer Streaming and IP-TV},
pages = {317–322},
numpages = {6},
location = {Kyoto, Japan},
series = {P2P-TV '07}
}

@inproceedings{10.1145/3427760.3428338,
author = {Rinaldi, Luca and Torquati, Massimo and Mencagli, Gabriele and Danelutto, Marco},
title = {High-Throughput Stream Processing with Actors},
year = {2020},
isbn = {9781450381857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427760.3428338},
doi = {10.1145/3427760.3428338},
abstract = {The steady growth of data volume produced as continuous streams makes paramount the development of software capable of providing timely results to the users. The Actor Model (AM) offers a high-level of abstraction suited for developing scalable message-passing applications. It allows the application developer to focus on the application logic moving the burden of implementing fast and reliable inter-Actors message-exchange to the implementation framework. In this paper, we focus on evaluating the model in high data rate streaming applications targeting scale-up servers. Our approach leverages Parallel Patterns (PP) abstractions to model streaming computations and introduces optimizations that otherwise could be challenging to implement without violating the Actor Model's semantics. The experimental analysis demonstrates that the new implementation skeletons we propose for our PPs can bring significant performance boosts (more than 2X) in high data rate streaming applications implemented in CAF.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Workshop on Programming Based on Actors, Agents, and Decentralized Control},
pages = {1–10},
numpages = {10},
keywords = {Actor Model, Multi-Cores, Data Stream Processing, Parallel Patterns, Programming Model},
location = {Virtual, USA},
series = {AGERE 2020}
}

@inproceedings{10.1145/3328905.3332506,
author = {Coenen, Manuel and Wagner, Christoph and Echler, Alexander and Frischbier, Sebastian},
title = {Benchmarking Financial Data Feed Systems},
year = {2019},
isbn = {9781450367943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328905.3332506},
doi = {10.1145/3328905.3332506},
abstract = {Data-driven solutions for the investment industry require event-based backend systems to process high-volume financial data feeds with low latency, high throughput, and guaranteed delivery modes.At vwd we process an average of 18 billion incoming event notifications from 500+ data sources for 30 million symbols per day and peak rates of 1+ million notifications per second using custom-built platforms that keep audit logs of every event.We currently assess modern open source event-processing platforms such as Kafka, NATS, Redis, Flink or Storm for the use in our ticker plant to reduce the maintenance effort for cross-cutting concerns and leverage hybrid deployment models. For comparability and repeatability we benchmark candidates with a standardized workload we derived from our real data feeds.We have enhanced an existing light-weight open source benchmarking tool in its processing, logging, and reporting capabilities to cope with our workloads. The resulting tool wrench can simulate workloads or replay snapshots in volume and dynamics like those we process in our ticker plant. We provide the tool as open source.As part of ongoing work we contribute details on (a) our workload and requirements for benchmarking candidate platforms for financial feed processing; (b) the current state of the tool wrench.},
booktitle = {Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems},
pages = {252–253},
numpages = {2},
keywords = {stream-processing, requirements, event bus, publish/subscribe, workload, big data, financial data, Event-processing, benchmarking},
location = {Darmstadt, Germany},
series = {DEBS '19}
}

@inproceedings{10.1145/2525528.2525537,
author = {Burtsev, Anton and Mishrikoti, Nikhil and Eide, Eric and Ricci, Robert},
title = {Weir: A Streaming Language for Performance Analysis},
year = {2013},
isbn = {9781450324601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525528.2525537},
doi = {10.1145/2525528.2525537},
abstract = {For modern software systems, performance analysis can be a challenging task. The software stack can be a complex, multi-layer, multi-component, concurrent, and parallel environment with multiple contexts of execution and multiple sources of performance data. Although much performance data is available, because modern systems incorporate many mature data-collection mechanisms, analysis algorithms suffer from the lack of a unifying programming environment for processing the collected performance data, potentially from multiple sources, in a convenient and script-like manner.This paper presents Weir, a streaming language for systems performance analysis. Weir is based on the insight that performance-analysis algorithms can be naturally expressed as stream-processing pipelines. In Weir, an analysis algorithm is implemented as a graph composed of stages, where each stage operates on a stream of events that represent collected performance measurements. Weir is an imperative streaming language with a syntax designed for the convenient construction of stream pipelines that utilize composable and reusable analysis stages. To demonstrate practical application, this paper presents the authors' experience in using Weir to analyze performance in systems based on the Xen virtualization platform.},
booktitle = {Proceedings of the Seventh Workshop on Programming Languages and Operating Systems},
articleno = {6},
numpages = {6},
location = {Farmington, Pennsylvania},
series = {PLOS '13}
}

@article{10.1145/2626401.2626415,
author = {Burtsev, Anton and Mishrikoti, Nikhil and Eide, Eric and Ricci, Robert},
title = {Weir: A Streaming Language for Performance Analysis},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/2626401.2626415},
doi = {10.1145/2626401.2626415},
abstract = {For modern software systems, performance analysis can be a challenging task. The software stack can be a complex, multi-layer, multi-component, concurrent, and parallel environment with multiple contexts of execution and multiple sources of performance data. Although much performance data is available, because modern systems incorporate many mature data-collection mechanisms, analysis algorithms suffer from the lack of a unifying programming environment for processing the collected performance data, potentially from multiple sources, in a convenient and script-like manner.This paper presents Weir, a streaming language for systems performance analysis. Weir is based on the insight that performanceanalysis algorithms can be naturally expressed as stream-processing pipelines. In Weir, an analysis algorithm is implemented as a graph composed of stages, where each stage operates on a stream of events that represent collected performance measurements. Weir is an imperative streaming language with a syntax designed for the convenient construction of stream pipelines that utilize composable and reusable analysis stages. To demonstrate practical application, this paper presents the authors' experience in using Weir to analyze performance in systems based on the Xen virtualization platform.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {may},
pages = {65–70},
numpages = {6}
}

@inproceedings{10.1145/3274895.3274932,
author = {Mahmood, Ahmed R. and Daghistani, Anas and Aly, Ahmed M. and Tang, Mingjie and Basalamah, Saleh and Prabhakar, Sunil and Aref, Walid G.},
title = {Adaptive Processing of Spatial-Keyword Data over a Distributed Streaming Cluster},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274932},
doi = {10.1145/3274895.3274932},
abstract = {The widespread use of GPS-enabled smartphones along with the popularity of micro-blogging and social networking applications, e.g., Twitter and Facebook, has resulted in the generation of huge streams of geo-tagged textual data. Many applications require real-time processing of these streams. For example, location-based ad-targeting systems enable advertisers to register millions of ads to millions of users based on the users' location and textual profile. Existing streaming systems are either centralized or are not spatial-keyword aware, and hence these systems cannot efficiently support the processing of rapidly arriving spatial-keyword data streams. In this paper, we introduce a two-layered indexing scheme for the distributed processing of spatial-keyword data streams. We realize this indexing scheme in Tornado, a distributed spatial-keyword streaming system. The first layer, termed the routing layer, is used to fairly distribute the workload, and furthermore, co-locate the data objects and the corresponding queries at the same processing units. The routing layer uses the Augmented-Grid, a novel structure that is equipped with an efficient search algorithm for distributing the data objects and queries. The second layer, termed the evaluation layer, resides within each processing unit to reduce the processing overhead. The two-layered index adapts to changes in the workload by applying a cost formula that continuously represents the processing overhead at each processing unit. Extensive experimental evaluation using real Twitter data indicates that Tornado achieves high scalability and more than 2x improvement over the baseline approach in terms of the overall system throughput.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {219–228},
numpages = {10},
keywords = {distributed streaming, spatial-keyword processing},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.1145/1806565.1806593,
author = {Karki, Rabin and Seenivasan, Thangam and Claypool, Mark and Kinicki, Robert},
title = {Performance Analysis of Home Streaming Video Using Orb},
year = {2010},
isbn = {9781450300438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806565.1806593},
doi = {10.1145/1806565.1806593},
abstract = {A new paradigm in video streaming is emerging, that of personal video servers in the home streaming video to remote clients on the Internet. The potential impact of such technologies demands careful study to assess performance and impact. This project studies one such personal video streaming system called Orb in a closed network environment, allowing us to control network bandwidths. Our performance evaluation focuses on Orb's method of bandwidth estimation, video performance and bitrates, and resource usage during transcoding. Analysis shows Orb uses simplistic, but effective, methods of determining available bandwidth, dynamic temporal and spatial scaling, and significant CPU cycles when transcoding. The results should be useful for subsequent comparison with other home streaming technologies and capacity planning for network providers.},
booktitle = {Proceedings of the 20th International Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {111–116},
numpages = {6},
keywords = {streaming, orb, video},
location = {Amsterdam, The Netherlands},
series = {NOSSDAV '10}
}

@inproceedings{10.1145/3093742.3095100,
author = {Akram, Nihla and Siriwardene, Sachini and Jayasinghe, Malith and Dayarathna, Miyuru and Perera, Isuru and Fernando, Seshika and Perera, Srinath and Bandara, Upul and Suhothayan, Sriskandarajah},
title = {Anomaly Detection of Manufacturing Equipment via High Performance RDF Data Stream Processing: Grand Challenge},
year = {2017},
isbn = {9781450350655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093742.3095100},
doi = {10.1145/3093742.3095100},
abstract = {The ACM DEBS Grand Challenge 2017 focuses on anomaly detection of manufacturing equipment. The goal of the challenge is to detect abnormal behavior of a manufacturing machine based on the observations of the stream of measurements provided. The data produced by each sensor is clustered and the state transitions between the observed clusters are modeled as a Markov chain. In this paper we present how we used WSO2 Data Analytics Server (DAS), an open source, comprehensive enterprise data analytics platform, to solve the problem. On the HOBBIT (Holistic Benchmarking of Big Linked Data) platform our solution processed 35 megabytes/second with an end-to-end mean latency of 7.5 ms at an input rate of 1 ms, while the events spent only 1 ms time on average within our grand challenge solution. The paper describes the solution we propose, the experiments' results and presents how we optimized the performance of our solution.},
booktitle = {Proceedings of the 11th ACM International Conference on Distributed and Event-Based Systems},
pages = {280–285},
numpages = {6},
keywords = {Data Stream Processing, Markov Model, Machine Learning, Complex Event Processing, Data Clustering, Software Performance},
location = {Barcelona, Spain},
series = {DEBS '17}
}

@inproceedings{10.1145/3335783.3335784,
author = {Schelter, Sebastian and Celebi, Ufuk and Dunning, Ted},
title = {Efficient Incremental Cooccurrence Analysis for Item-Based Collaborative Filtering},
year = {2019},
isbn = {9781450362160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335783.3335784},
doi = {10.1145/3335783.3335784},
abstract = {Recommender systems are ubiquitous in the modern internet, where they help users find items they might like. A widely deployed recommendation approach is item-based collaborative filtering. This approach relies on analyzing large item cooccurrence matrices that denote how many users interacted with a pair of items. The potentially quadratic number of items to compare poses a scalability bottleneck in analyzing such item cooccurrences. Additionally, this problem intensifies in real world use cases with incrementally growing datasets, especially when the recommendation model is regularly recomputed from scratch. We highlight the connection between the growing cost of item-based recommendation and densification processes in common interaction datasets. Based on our findings, we propose an efficient incremental algorithm for item-based collaborative filtering based on cooccurrence analysis. This approach restricts the number of interactions to consider from 'power users' and 'ubiquitous items' to guarantee a provably constant amount of work per user-item interaction to process. We discuss efficient implementations of our algorithm on a single machine as well as on a distributed stream processing engine, and present an extensive experimental evaluation. Our results confirm the asymptotic benefits of the incremental approach. Furthermore, we find that our implementation is an order of magnitude faster than existing open source recommender libraries on many datasets, and at the same time scales to high dimensional datasets which these existing recommenders fail to process.},
booktitle = {Proceedings of the 31st International Conference on Scientific and Statistical Database Management},
pages = {61–72},
numpages = {12},
location = {Santa Cruz, CA, USA},
series = {SSDBM '19}
}

@article{10.14778/3377369.3377372,
author = {Stehle, Elias and Jacobsen, Hans-Arno},
title = {ParPaRaw: Massively Parallel Parsing of Delimiter-Separated Raw Data},
year = {2020},
issue_date = {January 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3377369.3377372},
doi = {10.14778/3377369.3377372},
abstract = {Parsing is essential for a wide range of use cases, such as stream processing, bulk loading, and in-situ querying of raw data. Yet, the compute-intense step often constitutes a major bottleneck in the data ingestion pipeline, since parsing of inputs that require more involved parsing rules is challenging to parallelise. This work proposes a massively parallel algorithm for parsing delimiter-separated data formats on GPUs. Other than the state-of-the-art, the proposed approach does not require an initial sequential pass over the input to determine a thread's parsing context. That is, how a thread, beginning somewhere in the middle of the input, should interpret a certain symbol (e.g., whether to interpret a comma as a delimiter or as part of a larger string enclosed in double-quotes). Instead of tailoring the approach to a single format, we are able to perform a massively parallel finite state machine (FSM) simulation, which is more flexible and powerful, supporting more expressive parsing rules with general applicability. Achieving a parsing rate of as much as 14.2 GB/s, our experimental evaluation on a GPU with 3 584 cores shows that the presented approach is able to scale to thousands of cores and beyond. With an end-to-end streaming approach, we are able to exploit the full-duplex capabilities of the PCIe bus and hide latency from data transfers. Considering the end-to-end performance, the algorithm parses 4.8 GB in as little as 0.44 seconds, including data transfers.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {616–628},
numpages = {13}
}

@inproceedings{10.1145/2567948.2580051,
author = {Suzumura, Toyotaro and Nishii, Shunsuke and Ganse, Masaru},
title = {Towards Large-Scale Graph Stream Processing Platform},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2580051},
doi = {10.1145/2567948.2580051},
abstract = {In recent years, real-time data mining for large-scale time-evolving graphs is becoming a hot research topic. Most of the prior arts target relatively static graphs and also process them in store-and-process batch processing model. In this paper we propose a method of applying on-the-fly and incremental graph stream computing model to such dynamic graph analysis. To process large-scale graph streams on a cluster of nodes dynamically in a scalable fashion, we propose an incremental large-scale graph processing model called "Incremental GIM-V (Generalized Iterative Matrix-Vector Multiplication)". We also design and implement UNICORN, a system that adopts the proposed incremental processing model on top of IBM InfoSphere Streams. Our performance evaluation demonstrates that our method achieves up to 48% speedup on PageRank with Scale 16 Log-normal Graph (vertexes=65,536, edges=8,364,525) with 4 nodes, 3023% speedup on Random walk with Restart with Kronecker Graph with Scale 18 (vertexes=262,144, edges=8,388,608) with 4 nodes against original GIM-V.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {1321–1326},
numpages = {6},
keywords = {graph, stream},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.1145/1993498.1993559,
author = {D'Elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
title = {Mining Hot Calling Contexts in Small Space},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993559},
doi = {10.1145/1993498.1993559},
abstract = {Calling context trees (CCTs) associate performance metrics with paths through a program's call graph, providing valuable information for program understanding and performance analysis. Although CCTs are typically much smaller than call trees, in real applications they might easily consist of tens of millions of distinct calling contexts: this sheer size makes them difficult to analyze and might hurt execution times due to poor access locality. For performance analysis, accurately collecting information about hot calling contexts may be more useful than constructing an entire CCT that includes millions of uninteresting paths. As we show for a variety of prominent Linux applications, the distribution of calling context frequencies is typically very skewed. In this paper we show how to exploit this property to reduce the CCT size considerably.We introduce a novel run-time data structure, called Hot Calling Context Tree (HCCT), that offers an additional intermediate point in the spectrum of data structures for representing interprocedural control flow. The HCCT is a subtree of the CCT that includes only hot nodes and their ancestors. We show how to compute the HCCT without storing the exact frequency of all calling contexts, by using fast and space-efficient algorithms for mining frequent items in data streams. With this approach, we can distinguish between hot and cold contexts on the fly, while obtaining very accurate frequency counts. We show both theoretically and experimentally that the HCCT achieves a similar precision as the CCT in a much smaller space, roughly proportional to the number of distinct hot contexts: this is typically several orders of magnitude smaller than the total number of calling contexts encountered during a program's execution. Our space-efficient approach can be effectively combined with previous context-sensitive profiling techniques, such as sampling and bursting.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {516–527},
numpages = {12},
keywords = {program instrumentation, performance profiling, data streaming algorithms, dynamic program analysis, frequent items},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@article{10.1145/1993316.1993559,
author = {D'Elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
title = {Mining Hot Calling Contexts in Small Space},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993559},
doi = {10.1145/1993316.1993559},
abstract = {Calling context trees (CCTs) associate performance metrics with paths through a program's call graph, providing valuable information for program understanding and performance analysis. Although CCTs are typically much smaller than call trees, in real applications they might easily consist of tens of millions of distinct calling contexts: this sheer size makes them difficult to analyze and might hurt execution times due to poor access locality. For performance analysis, accurately collecting information about hot calling contexts may be more useful than constructing an entire CCT that includes millions of uninteresting paths. As we show for a variety of prominent Linux applications, the distribution of calling context frequencies is typically very skewed. In this paper we show how to exploit this property to reduce the CCT size considerably.We introduce a novel run-time data structure, called Hot Calling Context Tree (HCCT), that offers an additional intermediate point in the spectrum of data structures for representing interprocedural control flow. The HCCT is a subtree of the CCT that includes only hot nodes and their ancestors. We show how to compute the HCCT without storing the exact frequency of all calling contexts, by using fast and space-efficient algorithms for mining frequent items in data streams. With this approach, we can distinguish between hot and cold contexts on the fly, while obtaining very accurate frequency counts. We show both theoretically and experimentally that the HCCT achieves a similar precision as the CCT in a much smaller space, roughly proportional to the number of distinct hot contexts: this is typically several orders of magnitude smaller than the total number of calling contexts encountered during a program's execution. Our space-efficient approach can be effectively combined with previous context-sensitive profiling techniques, such as sampling and bursting.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {516–527},
numpages = {12},
keywords = {data streaming algorithms, frequent items, program instrumentation, dynamic program analysis, performance profiling}
}

@article{10.14778/3303753.3303758,
author = {Zeuch, Steffen and Monte, Bonaventura Del and Karimov, Jeyhun and Lutz, Clemens and Renz, Manuel and Traub, Jonas and Bre\ss{}, Sebastian and Rabl, Tilmann and Markl, Volker},
title = {Analyzing Efficient Stream Processing on Modern Hardware},
year = {2019},
issue_date = {January 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3303753.3303758},
doi = {10.14778/3303753.3303758},
abstract = {Modern Stream Processing Engines (SPEs) process large data volumes under tight latency constraints. Many SPEs execute processing pipelines using message passing on shared-nothing architectures and apply a partition-based scale-out strategy to handle high-velocity input streams. Furthermore, many state-of-the-art SPEs rely on a Java Virtual Machine to achieve platform independence and speed up system development by abstracting from the underlying hardware.In this paper, we show that taking the underlying hardware into account is essential to exploit modern hardware efficiently. To this end, we conduct an extensive experimental analysis of current SPEs and SPE design alternatives optimized for modern hardware. Our analysis highlights potential bottlenecks and reveals that state-of-the-art SPEs are not capable of fully exploiting current and emerging hardware trends, such as multi-core processors and high-speed networks. Based on our analysis, we describe a set of design changes to the common architecture of SPEs to scale-up on modern hardware. We show that the single-node throughput can be increased by up to two orders of magnitude compared to state-of-the-art SPEs by applying specialized code generation, fusing operators, batch-style parallelization strategies, and optimized windowing. This speedup allows for deploying typical streaming applications on a single or a few nodes instead of large clusters.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {516–530},
numpages = {15}
}

@inproceedings{10.1145/1854273.1854321,
author = {Tournavitis, Georgios and Franke, Bj\"{o}rn},
title = {Semi-Automatic Extraction and Exploitation of Hierarchical Pipeline Parallelism Using Profiling Information},
year = {2010},
isbn = {9781450301787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1854273.1854321},
doi = {10.1145/1854273.1854321},
abstract = {In recent years multi-core computer systems have left the realm of high-performance computing and virtually all of today's desktop computers and embedded computing systems are equipped with several processing cores. Still, no single parallel programming model has found widespread support and parallel programming remains an art for the majority of application programmers. In addition, there exists a plethora of sequential legacy applications for which automatic parallelization is the only hope to benefit from the increased processing power of modern multi-core systems. In the past automatic parallelization largely focused on data parallelism. In this paper we present a novel approach to extracting and exploiting pipeline parallelism from sequential applications. We use profiling to overcome the limitations of static data and control flow analysis enabling more aggressive parallelization. Our approach is orthogonal to existing automatic parallelization approaches and additional data parallelism may be exploited in the individual pipeline stages. The key contribution of this paper is a whole-program representation that supports profiling, parallelism extraction and exploitation. We demonstrate how this enhances conventional pipeline parallelization by incorporating support for multi-level loops and pipeline stage replication in a uniform and automatic way. We have evaluated our methodology on a set of multimedia and stream processing benchmarks and demonstrate speedups of up to 4.7 on a eight-core Intel Xeon machine.},
booktitle = {Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
pages = {377–388},
numpages = {12},
keywords = {streaming applications, parallelization, program dependence graph, pipeline parallelism},
location = {Vienna, Austria},
series = {PACT '10}
}

@inproceedings{10.1145/3335783.3335793,
author = {Mytilinis, Ioannis and Tsoumakos, Dimitrios and Koziris, Nectarios},
title = {Maintaining Wavelet Synopses for Sliding-Window Aggregates},
year = {2019},
isbn = {9781450362160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335783.3335793},
doi = {10.1145/3335783.3335793},
abstract = {The IoT era has brought forth a computing paradigm shift from traditional high-end servers to "edge" devices of limited processing and memory capabilities. These devices, together with sensors, regularly produce very high data volumes nowadays. For many real-time applications, storing and indexing an unbounded stream may not be an option. Thus, it is important that we design algorithms and systems that can both work at the edge of the network and be able to answer queries on distributed, streaming data. Moreover, in many streaming scenarios, fresh data tend to be prioritized. A sliding-window model is an important case of stream processing, where only the most recent elements remain active and the rest are discarded. In this work, we study the problem of maintaining basic aggregate statistics over a sliding-window data stream under the constraint of limited memory. As in IoT scenarios the available memory is typically much less than the window size, queries are answered from compact synopses that are maintained in an online fashion. For the efficient construction of such synopses, in this work, we propose wavelet-based algorithms that provide deterministic guarantees and produce almost exact results. Our algorithms can work on any kind of numerical data and do not have the positive-numbers constraint of techniques such as the exponential histograms. Our experimental evaluation indicates that, in terms of accuracy and space-efficiency, our solution outperforms the exponential histograms and deterministic waves techniques.},
booktitle = {Proceedings of the 31st International Conference on Scientific and Statistical Database Management},
pages = {73–84},
numpages = {12},
location = {Santa Cruz, CA, USA},
series = {SSDBM '19}
}

@inproceedings{10.1145/1233501.1233559,
author = {Gill, Gennette and Hansen, John and Singh, Montek},
title = {Loop Pipelining for High-Throughput Stream Computation Using Self-Timed Rings},
year = {2006},
isbn = {1595933891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1233501.1233559},
doi = {10.1145/1233501.1233559},
abstract = {We present a technique for increasing the throughput of stream processing architectures by removing the bottlenecks caused by loop structures. We implement loops as self-timed pipelined rings that can operate on multiple data sets concurrently. Our contribution includes a transformation algorithm which takes as input a high-level program and gives as output the structure of an optimized pipeline ring. Our technique handles nested loops and is further enhanced by loop unrolling. Simulations run on benchmark examples show a 1.3 to 4.9x speedup without unrolling and a 2.6 to 9.7x speedup with twofold loop unrolling.},
booktitle = {Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design},
pages = {289–296},
numpages = {8},
location = {San Jose, California},
series = {ICCAD '06}
}

@inproceedings{10.1145/2611286.2611298,
author = {Tudoran, Radu and Nano, Olivier and Santos, Ivo and Costan, Alexandru and Soncu, Hakan and Boug\'{e}, Luc and Antoniu, Gabriel},
title = {JetStream: Enabling High Performance Event Streaming across Cloud Data-Centers},
year = {2014},
isbn = {9781450327374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611286.2611298},
doi = {10.1145/2611286.2611298},
abstract = {The easily-accessible computation power offered by cloud infrastructures coupled with the revolution of Big Data are expanding the scale and speed at which data analysis is performed. In their quest for finding the Value in the 3 Vs of Big Data, applications process larger data sets, within and across clouds. Enabling fast data transfers across geographically distributed sites becomes particularly important for applications which manage continuous streams of events in real time. Scientific applications (e.g. the Ocean Observatory Initiative or the ATLAS experiment) as well as commercial ones (e.g. Microsoft's Bing and Office 365 large-scale services) operate on tens of data-centers around the globe and follow similar patterns: they aggregate monitoring data, assess the QoS or run global data mining queries based on inter site event stream processing. In this paper, we propose a set of strategies for efficient transfers of events between cloud data-centers and we introduce JetStream: a prototype implementing these strategies as a high performance batch-based streaming middleware. JetStream is able to self-adapt to the streaming conditions by modeling and monitoring a set of context parameters. It further aggregates the available bandwidth by enabling multi-route streaming across cloud sites. The prototype was validated on tens of nodes from US and Europe data-centers of the Windows Azure cloud using synthetic benchmarks and with application code from the context of the Alice experiment at CERN. The results show an increase in transfer rate of 250 times over individual event streaming. Besides, introducing an adaptive transfer strategy brings an additional 25% gain. Finally, the transfer rate can further be tripled thanks to the use of multi-route streaming.},
booktitle = {Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems},
pages = {23–34},
numpages = {12},
keywords = {multi data-centers, cloud computing, event streaming, high performance data management},
location = {Mumbai, India},
series = {DEBS '14}
}

@inproceedings{10.5555/2602339.2602353,
author = {Lai, Farley and Hasan, Syed Shabih and Laugesen, Austin and Chipara, Octav},
title = {CSense: A Stream-Processing Toolkit for Robust and High-Rate Mobile Sensing Applications},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {This paper presents CSense - a stream-processing toolkit for developing robust and high-rate mobile sensing application in Java. CSense addresses the needs of these systems by providing a new programming model that supports flexible application configuration, a high-level concurrency model, memory management, and compiler analyses and optimizations. Our compiler includes a novel flow analysis that optimizes the exchange of data across components from an application-wide perspective. A mobile sensing application benchmark indicates that flow analysis may reduce CPU utilization by as much as 45%. Static analysis is used to detect a range of programming errors including application composition errors, improper use of memory management, and data races. We identify that memory management and concurrency limit the scalability of stream processing systems. We incorporate memory pools, frame conversion optimizations, and custom synchronization primitives to develop a scalable run-time. CSense is evaluated on Galaxy Nexus phones running Android. Empirical results indicate that our run-time achieves 19 times higher steam processing rate compared to a realistic baseline implementation. We demonstrate the versatility of CSense by developing three mobile sensing applications.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {119–130},
numpages = {12},
keywords = {dataflow computing, embedded software, runtime environment},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3491086.3492470,
author = {Raza, Murtaza and Tahir, Jawad and Doblander, Christoph and Mayer, Ruben and Jacobsen, Hans-Arno},
title = {Benchmarking Apache Kafka under Network Faults},
year = {2021},
isbn = {9781450391542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491086.3492470},
doi = {10.1145/3491086.3492470},
abstract = {Network faults are often transient and hence hard to detect and difficult to resolve. Our study conducts an analysis of Kafka's network fault tolerance capabilities, one of the widely used distributed stream processing system (DSPS). Across different Kafka configurations, we observed that Kafka is fault-tolerant towards network faults to some degree, and we report observations of its shortcomings. We also define a network fault-tolerance benchmark on which other DSPSs can be evaluated.},
booktitle = {Proceedings of the 22nd International Middleware Conference: Demos and Posters},
pages = {5–7},
numpages = {3},
keywords = {chaos testing, Kafka, monitoring, Kafka streams},
location = {Virtual Event, Canada},
series = {Middleware '21}
}

@inproceedings{10.5555/1516744.1516923,
author = {Bouillet, Eric and Dube, Parijat and George, David and Liu, Zhen and Pendarakis, Dimitrios and Zhang, Li},
title = {Distributed Multi-Layered Workload Synthesis for Testing Stream Processing Systems},
year = {2008},
isbn = {9781424427086},
publisher = {Winter Simulation Conference},
abstract = {Testing and benchmarking of stream processing systems requires workload representative of real world scenarios with myriad of users, interacting through different applications over different modalities with different underlying protocols. The workload should have realistic volumetric and contextual statistics at different levels: user level, application level, packet level etc. Further realistic workload is inherently distributed in nature. We present a scalable framework for synthesis of distributed workload based on identifying different layers of workload corresponding to different time-scales. The architecture is extensible and modular, promotes reuse of libraries at different layers and offers the flexibility to add additional plug-ins at different layers without sacrificing the efficiency.},
booktitle = {Proceedings of the 40th Conference on Winter Simulation},
pages = {1003–1011},
numpages = {9},
location = {Miami, Florida},
series = {WSC '08}
}

@article{10.14778/3476311.3476384,
author = {Gomes, Ana Sofia and Oliveirinha, Jo\~{a}o and Cardoso, Pedro and Bizarro, Pedro},
title = {Railgun: Managing Large Streaming Windows under MAD Requirements},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476384},
doi = {10.14778/3476311.3476384},
abstract = {Some mission critical systems, e.g., fraud detection, require accurate, real-time metrics over long time sliding windows on applications that demand high throughput and low latencies. As these applications need to run "forever" and cope with large, spiky data loads, they further require to be run in a distributed setting. We are unaware of any streaming system that provides all those properties. Instead, existing systems take large simplifications, such as implementing sliding windows as a fixed set of overlapping windows, jeopardizing metric accuracy (violating regulatory rules) or latency (breaching service agreements). In this paper, we propose Railgun, a fault-tolerant, elastic, and distributed streaming system supporting real-time sliding windows for scenarios requiring high loads and millisecond-level latencies. We benchmarked an initial prototype of Railgun using real data, showing significant lower latency than Flink and low memory usage independent of window size. Further, we show that Railgun scales nearly linearly, respecting our msec-level latencies at high percentiles (&lt;250ms @ 99.9%) even under a load of 1 million events per second.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {3069–3082},
numpages = {14}
}

@article{10.1145/3152116,
author = {Wu, Jiyan and Cheng, Bo and Yang, Yuan and Wang, Ming and Chen, Junliang},
title = {Delay-Aware Quality Optimization in Cloud-Assisted Video Streaming System},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3152116},
doi = {10.1145/3152116},
abstract = {Cloud-assisted video streaming has emerged as a new paradigm to optimize multimedia content distribution over the Internet. This article investigates the problem of streaming cloud-assisted real-time video to multiple destinations (e.g., cloud video conferencing, multi-player cloud gaming, etc.) over lossy communication networks. The user diversity and network dynamics result in the delay differences among multiple destinations. This research proposes <underline>D</underline>ifferentiated cloud-<underline>A</underline>ssisted <underline>VI</underline>deo <underline>S</underline>treaming (DAVIS) framework, which proactively leverages such delay differences in video coding and transmission optimization. First, we analytically formulate the optimization problem of joint coding and transmission to maximize received video quality. Second, we develop a quality optimization framework that integrates the video representation selection and FEC (Forward Error Correction) packet interleaving. The proposed DAVIS is able to effectively perform differentiated quality optimization for multiple destinations by taking advantage of the delay differences in cloud-assisted video streaming system. We conduct the performance evaluation through extensive experiments with the Amazon EC2 instances and Exata emulation platform. Evaluation results show that DAVIS outperforms the reference cloud-assisted streaming solutions in video quality and delay performance.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {dec},
articleno = {4},
numpages = {25},
keywords = {Cloud-assisted video streaming, quality optimization, differentiated transmission, delay-awareness, burst loss}
}

@inproceedings{10.1109/CCGRID.2017.105,
author = {Imai, Shigeru and Patterson, Stacy and Varela, Carlos A.},
title = {Maximum Sustainable Throughput Prediction for Data Stream Processing over Public Clouds},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.105},
doi = {10.1109/CCGRID.2017.105},
abstract = {In cloud-based stream processing services, the maximum sustainable throughput (MST) is defined as the maximum throughput that a system composed of a fixed number of virtual machines (VMs) can ingest indefinitely. If the incoming data rate exceeds the system's MST, unprocessed data accumulates, eventually making the system inoperable. Thus, it is important for the service provider to keep the MST always larger than the incoming data rate by dynamically changing the number of VMs used by the system. In this paper, we identify a common data processing environment used by modern data stream processing systems, and we propose MST prediction models for this environment. We train the models using linear regression with samples obtained from a few VMs and predict MST for a larger number of VMs. To minimize the time and cost for model training, we statistically determine a set of training samples using Intel's Storm benchmarks with representative resource usage patterns. Using typical use-case benchmarks on Amazon's EC2 public cloud, our experiments show that, training with up to 8 VMs, we can predict MST for streaming applications with less than 4% average prediction error for 12 VMs, 9% for 16 VMs, and 32% for 24 VMs. Further, we evaluate our prediction models with simulation-based elastic VM scheduling on a realistic workload. These simulation results show that with 10% over-provisioning, our proposed models' cost efficiency is on par with the cost of an optimal scaling policy without incurring any service level agreement violations.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {504–513},
numpages = {10},
keywords = {performance prediction, cloud computing, auto-scaling, resource management},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/1247480.1247536,
author = {Luo, Gang and Tang, Chunqiang and Yu, Philip S.},
title = {Resource-Adaptive Real-Time New Event Detection},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247536},
doi = {10.1145/1247480.1247536},
abstract = {In a document streaming environment, online detection of the first documents that mention previously unseen events is an open challenge. For this online new event detection (ONED) task, existing studies usually assume that enough resources are always available and focus entirely on detection accuracy without considering efficiency. Moreover, none of the existing work addresses the issue of providing an effective and friendly user interface. As a result, there is a significant gap between the existing systems and a system that can be used in practice. In this paper, we propose an ONED framework with the following prominent features. First, a combination of indexing and compression methods is used to improve the document processing rate by orders of magnitude without sacrificing much detection accuracy. Second, when resources are tight, a resource-adaptive computation method is used to maximize the benefit that can be gained from the limited resources. Third, when the new event arrival rate is beyond the processing capability of the consumer of the ONED system, new events are further filtered and prioritized before they are presented to the consumer. Fourth, implicit citation relationships are created among all the documents and used to compute the importance of document sources. This importance information can guide the selection of document sources. We implemented a prototype of our framework on top of IBM's Stream Processing Core middleware. We also evaluated the effectiveness of our techniques on the standard TDT5 benchmark. To the best of our knowledge, this is the first implementation of a real application in a large-scale stream processing system.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {497–508},
numpages = {12},
keywords = {document streaming, online new event detection},
location = {Beijing, China},
series = {SIGMOD '07}
}

