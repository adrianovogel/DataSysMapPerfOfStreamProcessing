
@ARTICLE{Mullen20152553,
author={Mullen, T.R. and Kothe, C.A.E. and Chi, Y.M. and Ojeda, A. and Kerth, T. and Makeig, S. and Jung, T.-P. and Cauwenberghs, G.},
title={Real-time neuroimaging and cognitive monitoring using wearable dry EEG},
journal={IEEE Transactions on Biomedical Engineering},
year={2015},
volume={62},
number={11},
pages={2553-2567},
doi={10.1109/TBME.2015.2481482},
art_number={7274673},
note={cited By 342},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946811116&doi=10.1109%2fTBME.2015.2481482&partnerID=40&md5=60d104a08da0cb91ccce1c4a83f1948c},
abstract={Goal: We present and evaluate a wearable high-density dry-electrode EEG system and an open-source software framework for online neuroimaging and state classification. Methods: The system integrates a 64-channel dry EEG form factor with wireless data streaming for online analysis. A real-time software framework is applied, including adaptive artifact rejection, cortical source localization, multivariate effective connectivity inference, data visualization, and cognitive state classification from connectivity features using a constrained logistic regression approach (ProxConn). We evaluate the system identification methods on simulated 64-channel EEG data. Then, we evaluate system performance, using ProxConn and a benchmark ERP method, in classifying response errors in nine subjects using the dry EEG system. Results: Simulations yielded high accuracy (AUC = 0.97 ± 0.021) for real-time cortical connectivity estimation. Response error classification using cortical effective connectivity [short-time direct-directed transfer function (sdDTF)] was significantly above chance with similar performance (AUC) for cLORETA (0.74 ± 0.09) and LCMV (0.72 ± 0.08) source localization. Cortical ERP-based classification was equivalent to ProxConn for cLORETA (0.74 ± 0.16) but significantly better for LCMV (0.82 ± 0.12). Conclusion: We demonstrated the feasibility for real-time cortical connectivity analysis and cognitive state classification from high-density wearable dry EEG. Significance: This paper is the first validated application of these methods to 64-channel dry EEG. This study addresses a need for robust real-time measurement and interpretation of complex brain activity in the dynamic environment of the wearable setting. Such advances can have broad impact in research, medicine, and brain-computer interfaces. The pipelines are made freely available in the open-source SIFT and BCILAB toolboxes. © 1964-2012 IEEE.},
author_keywords={adaptive systems;  brain-computer interfaces;  connectivity analysis;  dry-contact electrode;  EEG;  neuroimaging;  Wearable sensors},
keywords={Adaptive systems;  Benchmarking;  Brain;  Brain computer interface;  Classification (of information);  Cognitive systems;  Computer programming;  Data visualization;  Electrodes;  Electroencephalography;  Human computer interaction;  Interfaces (computer);  Neuroimaging;  Open source software;  Software engineering;  Wearable sensors;  Wearable technology, Connectivity analysis;  Directed transfer functions;  Dry contact;  Effective connectivities;  Logistic regressions;  Real time measurements;  Real-time software framework;  System identification methods, Open systems, Article;  artifact;  brain computer interface;  cognition;  computer program;  connectome;  dry electrode electroencephalography;  electroencephalography;  evoked visual response;  machine learning;  neuroimaging;  online analysis;  signal noise ratio;  signal processing;  adult;  algorithm;  brain;  cognition;  devices;  electroencephalography;  human;  male;  neuroimaging;  physiology;  procedures;  task performance;  young adult, Adult;  Algorithms;  Brain;  Brain-Computer Interfaces;  Cognition;  Electroencephalography;  Humans;  Male;  Neuroimaging;  Task Performance and Analysis;  Young Adult},
publisher={IEEE Computer Society},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Le-Phuoc2011370,
author={Le-Phuoc, D. and Dao-Tran, M. and Xavier Parreira, J. and Hauswirth, M.},
title={A native and adaptive approach for unified processing of linked streams and linked data},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={7031 LNCS},
number={PART 1},
pages={370-388},
doi={10.1007/978-3-642-25073-6_24},
note={cited By 308},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055063828&doi=10.1007%2f978-3-642-25073-6_24&partnerID=40&md5=d0e47b0bbd957557e4513148838c35e2},
abstract={In this paper we address the problem of scalable, native and adaptive query processing over Linked Stream Data integrated with Linked Data. Linked Stream Data consists of data generated by stream sources, e.g., sensors, enriched with semantic descriptions, following the standards proposed for Linked Data. This enables the integration of stream data with Linked Data collections and facilitates a wide range of novel applications. Currently available systems use a "black box" approach which delegates the processing to other engines such as stream/event processing engines and SPARQL query processors by translating to their provided languages. As the experimental results described in this paper show, the need for query translation and data transformation, as well as the lack of full control over the query execution, pose major drawbacks in terms of efficiency. To remedy these drawbacks, we present CQELS (Continuous Query Evaluation over Linked Streams), a native and adaptive query processor for unified query processing over Linked Stream Data and Linked Data. In contrast to the existing systems, CQELS uses a "white box" approach and implements the required query operators natively to avoid the overhead and limitations of closed system regimes. CQELS provides a flexible query execution framework with the query processor dynamically adapting to the changes in the input data. During query execution, it continuously reorders operators according to some heuristics to achieve improved query execution in terms of delay and complexity. Moreover, external disk access on large Linked Data collections is reduced with the use of data encoding and caching of intermediate query results. To demonstrate the efficiency of our approach, we present extensive experimental performance evaluations in terms of query execution time, under varied query types, dataset sizes, and number of parallel queries. These results show that CQELS outperforms related approaches by orders of magnitude. © 2011 Springer-Verlag.},
author_keywords={dynamic query planning;  Linked Data;  Linked Streams;  query optimisation;  RDF Streams;  stream processing},
keywords={dynamic query planning;  Linked Data;  Linked Streams;  Query optimisation;  RDF Streams;  Stream processing, Metadata;  Query processing;  Semantic Web;  Semantics;  User interfaces, Data acquisition},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fernandez2013725,
author={Fernandez, R.C. and Migliavacca, M. and Kalyvianaki, E. and Pietzuch, P.},
title={Integrating scale out and fault tolerance in stream processing using operator state management},
journal={Proceedings of the ACM SIGMOD International Conference on Management of Data},
year={2013},
pages={725-736},
doi={10.1145/2463676.2465282},
note={cited By 256},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880553720&doi=10.1145%2f2463676.2465282&partnerID=40&md5=4b5cd51c46e56dda752fc8a187e5245e},
abstract={As users of "big data" applications expect fresh results, we witness a new breed of stream processing systems (SPS) that are designed to scale to large numbers of cloud-hosted machines. Such systems face new challenges: (i) to benefit from the "pay-as-you-go" model of cloud computing, they must scale out on demand, acquiring additional virtual machines (VMs) and parallelising operators when the workload increases; (ii) failures are common with deployments on hundreds of VMs - systems must be fault-tolerant with fast recovery times, yet low per-machine overheads. An open question is how to achieve these two goals when stream queries include stateful operators, which must be scaled out and recovered without affecting query results. Our key idea is to expose internal operator state explicitly to the SPS through a set of state management primitives. Based on them, we describe an integrated approach for dynamic scale out and recovery of stateful operators. Externalised operator state is checkpointed periodically by the SPS and backed up to upstream VMs. The SPS identifies individual operator bottlenecks and automatically scales them out by allocating new VMs and partitioning the check-pointed state. At any point, failed operators are recovered by restoring checkpointed state on a new VM and replaying unprocessed tuples. We evaluate this approach with the Linear Road Benchmark on the Amazon EC2 cloud platform and show that it can scale automatically to a load factor of L=350 with 50 VMs, while recovering quickly from failures. Copyright © 2013 ACM.},
author_keywords={Fault tolerance;  Scalability;  Stateful stream processing},
keywords={Cloud platforms;  Fault-tolerant;  Integrated approach;  Query results;  State management;  Stream processing;  Stream processing systems;  Virtual machines, Fault tolerance;  Scalability, Recovery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Baccarelli20179882,
author={Baccarelli, E. and Naranjo, P.G.V. and Scarpiniti, M. and Shojafar, M. and Abawajy, J.H.},
title={Fog of Everything: Energy-Efficient Networked Computing Architectures, Research Challenges, and a Case Study},
journal={IEEE Access},
year={2017},
volume={5},
pages={9882-9910},
doi={10.1109/ACCESS.2017.2702013},
art_number={7921687},
note={cited By 240},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026456235&doi=10.1109%2fACCESS.2017.2702013&partnerID=40&md5=82be521a9f65ec2ce83628742033881c},
abstract={Fog computing (FC) and Internet of Everything (IoE) are two emerging technological paradigms that, to date, have been considered standing-alone. However, because of their complementary features, we expect that their integration can foster a number of computing and network-intensive pervasive applications under the incoming realm of the future Internet. Motivated by this consideration, the goal of this position paper is fivefold. First, we review the technological attributes and platforms proposed in the current literature for the standing-alone FC and IoE paradigms. Second, by leveraging some use cases as illustrative examples, we point out that the integration of the FC and IoE paradigms may give rise to opportunities for new applications in the realms of the IoE, Smart City, Industry 4.0, and Big Data Streaming, while introducing new open issues. Third, we propose a novel technological paradigm, the Fog of Everything (FoE) paradigm, that integrates FC and IoE and then we detail the main building blocks and services of the corresponding technological platform and protocol stack. Fourth, as a proof-of-concept, we present the simulated energy-delay performance of a small-scale FoE prototype, namely, the V-FoE prototype. Afterward, we compare the obtained performance with the corresponding one of a benchmark technological platform, e.g., the V-D2D one. It exploits only device-to-device links to establish inter-thing 'ad hoc' communication. Last, we point out the position of the proposed FoE paradigm over a spectrum of seemingly related recent research projects. © 2017 IEEE.},
author_keywords={Big Data Streaming;  context-aware networking-plus-computing distributed resource management;  Fog of IoE;  future Internet;  Industry 4.0;  Internet of Energy;  Smart City;  virtualized networked computing platforms for IoE},
keywords={Benchmarking;  Big data;  Computer architecture;  Data reduction;  Energy efficiency;  Fog;  Information management;  Smart city, Data streaming;  Distributed resource management;  Energy-delay performance;  Future internet;  Networked computing;  Pervasive applications;  Technological attributes;  Technological platform, Distributed computer systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chintapalli20161789,
author={Chintapalli, S. and Dagit, D. and Evans, B. and Farivar, R. and Graves, T. and Holderbaugh, M. and Liu, Z. and Nusbaum, K. and Patil, K. and Peng, B.J. and Poulosky, P.},
title={Benchmarking streaming computation engines: Storm, flink and spark streaming},
journal={Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016},
year={2016},
pages={1789-1792},
doi={10.1109/IPDPSW.2016.138},
art_number={7530084},
note={cited By 217},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991585535&doi=10.1109%2fIPDPSW.2016.138&partnerID=40&md5=3dcf75c602168afa557bdafaf73e1384},
abstract={Streaming data processing has been gaining attention due to its application into a wide range of scenarios. To serve the booming demands of streaming data processing, many computation engines have been developed. However, there is still a lack of real-world benchmarks that would be helpful when choosing the most appropriate platform for serving real-time streaming needs. In order to address this problem, we developed a streaming benchmark for three representative computation engines: Flink, Storm and Spark Streaming. Instead of testing speed-of-light event processing, we construct a full data pipeline using Kafka and Redis in order to more closely mimic the real-world production scenarios. Based on our experiments, we provide a performance comparison of the three data engines in terms of 99th percentile latency and throughput for various configurations. © 2016 IEEE.},
author_keywords={Benchmark;  Flink;  Low Latency;  Spark;  Storm;  Streaming processing},
keywords={Benchmarking;  Data handling;  Data transfer;  Electric sparks;  Engines;  Storms, Computation engine;  Flink;  Low latency;  Performance comparison;  Real time streaming;  Streaming computations;  Streaming data processing;  Streaming processing, Pipeline processing systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jain2006431,
author={Jain, N. and Amini, L. and Andrade, H. and King, R. and Park, Y. and Selo, P. and Venkatramani, C.},
title={Design, implementation, and evaluation of the linear road bnchmark on the stream processing core},
journal={Proceedings of the ACM SIGMOD International Conference on Management of Data},
year={2006},
pages={431-442},
doi={10.1145/1142473.1142522},
note={cited By 153},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250621863&doi=10.1145%2f1142473.1142522&partnerID=40&md5=a8ef38f2b71d33c80a1a3534a721e780},
abstract={Stream processing applications have recently gained significant attention in the networking and database community. At the core of these applications is a stream processing engine that performs resource allocation and management to support continuous tracking of queries over collections of physically- distributed and rapidly-updating data streams. While numerous stream processing systems exist, there has been little work on understanding the performance characteristics of these applications in a distributed setup. In this paper, we examine the performance bottlenecks of streaming data applications, in particular the Linear Road stream data management benchmark, in achieving good performance in large-scale distributed environments, using the Stream Processing Core (SPC), a stream processing middleware we have developed. First, we present the design and implementation of the Linear Road benchmark on the SPC middleware. SPC has been designed to scale to tens of thousands of processing nodes, while supporting concurrent applications and multiple simultaneous queries. Second, we identify the main performance bottlenecks in the Linear Road application in achieving scalability and low query response latency. Our results show that data locality, buffer capacity, physical allocation of processing elements to infrastructure nodes, and packaging for transporting streamed data are important factors in achieving good application performance. Though we evaluate our system primarily for the Linear Road application, we believe it also provides useful insights into the overall system behavior for supporting other distributed and large-scale continuous streaming data applications. Finally, we examine how SPC can be used and tuned to enable a very efficient implementation of the Linear Road application in a distributed environment. Copyright 2006 ACM.},
author_keywords={Bottleneck analysis;  Distributed stream processing systems;  Linear road;  Performance evaluation},
keywords={Benchmarking;  Database systems;  Query processing;  Resource allocation, Bottleneck analysis;  Performance evaluation;  Stream processing, Data processing},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Peng2015149,
author={Peng, B. and Hosseini, M. and Hong, Z. and Farivar, R. and Campbell, R.},
title={R-storm: Resource-aware scheduling in storm},
journal={Middleware 2015 - Proceedings of the 16th Annual Middleware Conference},
year={2015},
pages={149-161},
doi={10.1145/2814576.2814808},
note={cited By 152},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967211111&doi=10.1145%2f2814576.2814808&partnerID=40&md5=45965b1de1b3e9d84ddc22a64b64fb5c},
abstract={The era of big data has led to the emergence of new systems for real-time distributed stream processing, e.g., Apache Storm is one of the most popular stream processing systems in in- dustry today. However, Storm, like many other stream pro- cessing systems lacks an intelligent scheduling mechanism. The default round-robin scheduling currently deployed in Storm disregards resource demands and availability, and can therefore be inefficient at times. We present R-Storm (Resource-Aware Storm), a system that implements resource- aware scheduling within Storm. R-Storm is designed to in- crease overall throughput by maximizing resource utilization while minimizing network latency. When scheduling tasks, R-Storm can satisfy both soft and hard resource constraints as well as minimizing network distance between components that communicate with each other. We evaluate R-Storm on set of micro-benchmark Storm applications as well as Storm applications used in production at Yahoo! Inc. From our experimental results we conclude that R-Storm achieves 30-47% higher throughput and 69-350% better CPU utiliza- tion than default Storm for the micro-benchmarks. For the Yahoo! Storm applications, R-Storm outperforms default Storm by around 50% based on overall throughput. We also demonstrate that R-Storm performs much better when scheduling multiple Storm applications than default Storm. © 2015 ACM.},
author_keywords={Resource-aware scheduling;  Storm;  Stream},
keywords={Benchmarking;  Big data;  Distributed parameter control systems;  Middleware;  Real time systems;  Routers;  Scheduling;  Scheduling algorithms;  Throughput, Distributed stream processing;  Intelligent scheduling;  Resource Constraint;  Resource utilizations;  Resource-aware scheduling;  Round-robin scheduling;  Stream;  Stream processing systems, Storms},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Eager2001742,
author={Eager, D. and Vernon, M. and Zahorjan, J.},
title={Minimizing bandwidth requirements for on-demand data delivery},
journal={IEEE Transactions on Knowledge and Data Engineering},
year={2001},
volume={13},
number={5},
pages={742-757},
doi={10.1109/69.956098},
note={cited By 147},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035439050&doi=10.1109%2f69.956098&partnerID=40&md5=f0ff7d900a83192d2479ee2e2e227704},
abstract={Two recent techniques for multicast or broadcast delivery of streaming media can provide immediate service to each client request, yet achieve considerable client stream sharing which leads to significant server and network bandwidth savings. This paper considers 1) how well these recently proposed techniques perform relative to each other and 2) whether there are new practical delivery techniques that can achieve better bandwidth savings than the previous techniques over a wide range of client request rates. The principal results are as follows: First, the recent partitioned dynamic skyscraper technique is adapted to provide immediate service to each client request more simply and directly than the original dynamic skyscraper method. Second, at moderate to high client request rates, the dynamic skyscraper method has required server bandwidth that is significantly lower than the recent optimized stream tapping/patching/controlled multicast technique. Third, the minimum required server bandwidth for any delivery technique that provides immediate real-time delivery to clients increases logarithmically (with constant factor equal to one) as a function of the client request arrival rate. Furthermore, it is (theoretically) possible to achieve very close to the minimum required server bandwidth if client receive bandwidth is equal to two times the data streaming rate and client storage capacity is sufficient for buffering data from shared streams. Finally, we propose a new practical delivery technique, called hierarchical multicast stream merging (HMSM), which has a required server bandwidth that is lower than the partitioned dynamic skyscraper and is reasonably close to the minimum achievable required server bandwidth over a wide range of client request rates.},
author_keywords={Multicast;  Performance evaluation;  Scalable protocols;  Streaming media;  Video-on-demand},
keywords={Hierarchical multicast stream merging;  Storage capacity;  Streaming media, Bandwidth;  Channel capacity;  Congestion control (communication);  Data communication systems;  Magnetic disk storage;  Network protocols;  Performance;  Resource allocation;  Storage allocation (computer);  Telecommunication traffic;  Video on demand, Multicasting},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Satyanarayan2016659,
author={Satyanarayan, A. and Russell, R. and Hoffswell, J. and Heer, J.},
title={Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization},
journal={IEEE Transactions on Visualization and Computer Graphics},
year={2016},
volume={22},
number={1},
pages={659-668},
doi={10.1109/TVCG.2015.2467091},
art_number={7192704},
note={cited By 145},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947087865&doi=10.1109%2fTVCG.2015.2467091&partnerID=40&md5=6d97554d113f1859e59cd78b48209dac},
abstract={We present Reactive Vega, a system architecture that provides the first robust and comprehensive treatment of declarative visual and interaction design for data visualization. Starting from a single declarative specification, Reactive Vega constructs a dataflow graph in which input data, scene graph elements, and interaction events are all treated as first-class streaming data sources. To support expressive interactive visualizations that may involve time-varying scalar, relational, or hierarchical data, Reactive Vega's dataflow graph can dynamically re-write itself at runtime by extending or pruning branches in a data-driven fashion. We discuss both compile- and run-time optimizations applied within Reactive Vega, and share the results of benchmark studies that indicate superior interactive performance to both D3 and the original, non-reactive Vega system. © 1995-2012 IEEE.},
author_keywords={Computer architecture;  Data models;  Data visualization;  Encoding;  Indexes;  Runtime;  Visualization},
keywords={Benchmarking;  Computer architecture;  Data flow analysis;  Data structures;  Encoding (symbols);  Flow visualization;  Visualization, Data-flow architectures;  Indexes;  Interaction design;  Interactive performance;  Interactive visualizations;  Runtime optimization;  Runtimes;  System architectures, Data visualization},
publisher={IEEE Computer Society},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Amini200627,
author={Amini, L. and Andrade, H. and Bhagwan, R. and Eskesen, F. and King, R. and Selo, P. and Park, Y. and Venkatramani, C.},
title={SPC: A distributed, scalable platform for data mining},
journal={4th International Workshop on Data Mining Standards, Services and Platforms},
year={2006},
pages={27-37},
doi={10.1145/1289612.1289615},
note={cited By 132},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849108661&doi=10.1145%2f1289612.1289615&partnerID=40&md5=6c770ec54f99f5d468a6cdb43ab895f0},
abstract={The Stream Processing Core (SPC) is distributed stream processing middleware designed to support applications that extract information from a large number of digital data streams. In this paper, we describe the SPC programming model which, to the best of our knowledge, is the first to support stream-mining applications using a subscription-like model for specifying stream connections as well as to provide support for non-relational operators. This enables stream-mining applications to tap into, analyze and track an ever-changing array of data streams which may contain information relevant to the streaming-queries placed on it. We describe the design, implementation, and experimental evaluation of the SPC distributed middleware, which deploys applications on to the running system in an incremental fashion, making stream connections as required. Using micro-benchmarks and a representative large-scale synthetic stream-mining application, we evaluate the performance of the control and data paths of the SPC middleware. Copyright 2006 ACM.},
keywords={Data paths;  Data streams;  Stream Processing Core (SPC);  Subscription-like models, Benchmarking;  Computer programming;  Data acquisition;  Data structures;  Middleware;  Scalability, Data mining},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Green2004752,
author={Green, T.J. and Gupta, A. and Miklau, G. and Onizuka, M. and Suciu, D.},
title={Processing XML streams with deterministic automata and stream indexes},
journal={ACM Transactions on Database Systems},
year={2004},
volume={29},
number={4},
pages={752-788},
doi={10.1145/1042046.1042051},
note={cited By 119},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344273306&doi=10.1145%2f1042046.1042051&partnerID=40&md5=8dea759f5545ca2730f385e1b8e85bdc},
abstract={We consider the problem of evaluating a large number of XPath expressions on a stream of XML packets. We contribute two novel techniques. The first is to use a single Deterministic Finite Automaton (DFA). The contribution here is to show that the DFA can be used effectively for this problem: in our experiments we achieve a constant throughput, independently of the number of XPath expressions. The major issue is the size of the DFA, which, in theory, can be exponential in the number of XPath expressions. We provide a series of theoretical results and experimental evaluations that show that the lazy DFA has a small number of states, for all practical purposes. These results are of general interest in XPath processing, beyond stream processing. The second technique is the Streaming IndeX (SLK), which consists of adding a small amount of binary data to each XML packet that allows the query processor to achieve significant speedups. As an application of these techniques we describe the XML Toolkit (XMLTK), a collection of command-line tools providing highly scalable XML data processing.},
author_keywords={Stream processing;  XML processing},
keywords={Free languages;  Stream index;  Stream processing;  XML processing, Algorithms;  Automata theory;  Data processing;  Finite automata;  Formal languages;  Problem solving, XML},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{He201063,
author={He, B. and Yang, M. and Guo, Z. and Chen, R. and Su, B. and Lin, W. and Zhou, L.},
title={Comet: Batched stream processing for data intensive distributed computing},
journal={Proceedings of the 1st ACM Symposium on Cloud Computing, SoCC '10},
year={2010},
pages={63-74},
doi={10.1145/1807128.1807139},
note={cited By 107},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954920597&doi=10.1145%2f1807128.1807139&partnerID=40&md5=169b998f3d3f122d6e8ac092ad939912},
abstract={Batched stream processing is a new distributed data processing paradigm that models recurring batch computations on incrementally bulk-appended data streams. The model is inspired by our empirical study on a trace from a large-scale production data-processing cluster; it allows a set of effective query optimizations that are not possible in a traditional batch processing model. We have developed a query processing system called Comet that embraces batched stream processing and integrates with DryadLINQ. We used two complementary methods to evaluate the effectiveness of optimizations that Comet enables. First, a prototype system deployed on a 40-node cluster shows an I/O reduction of over 40% using our benchmark. Second, when applied to a real production trace covering over 19 million machine-hours, our simulator shows an estimated I/O saving of over 50%. Copyright 2010 ACM.},
author_keywords={Batched stream processing;  Data-intensive scalable computing;  Query series;  Resource management},
keywords={Batch processing;  Complementary methods;  Data intensive;  Data stream;  Distributed Computing;  Distributed data processing;  Empirical studies;  Large-scale production;  Processing clusters;  Prototype system;  Query optimization;  Query processing system;  Resource management;  Scalable computing;  Stream processing, Distributed computer systems;  Hydraulics;  Natural resources management;  Resource allocation, Batch data processing},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cardellini2016271,
author={Cardellini, V. and Grassi, V. and Presti, F.L. and Nardelli, M.},
title={On QoS-Aware scheduling of data stream applications over fog computing infrastructures},
journal={Proceedings - IEEE Symposium on Computers and Communications},
year={2016},
volume={2016-February},
pages={271-276},
doi={10.1109/ISCC.2015.7405527},
art_number={7405527},
note={cited By 96},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961879980&doi=10.1109%2fISCC.2015.7405527&partnerID=40&md5=5431b873caa0930b8f687cc12625fcdf},
abstract={Fog computing is rapidly changing the distributed computing landscape by extending the Cloud computing paradigm to include wide-spread resources located at the network edges. This diffused infrastructure is well suited for the implementation of data stream processing (DSP) applications, by possibly exploiting local computing resources. Storm is an open source, scalable, and fault-Tolerant DSP system designed for locally distributed clusters. We made it suitable to operate in a geographically distributed and highly variable environment; to this end, we extended Storm with new components that allow to execute a distributed QoS-Aware scheduler and give self-Adaptation capabilities to the system. In this paper we provide a thorough experimental evaluation of the proposed solution using two sets of DSP applications: The former is characterized by a simple topology with different requirements; the latter comprises some well known applications (i.e., Word Count, Log Processing). The results show that the distributed QoS-Aware scheduler outperforms the centralized default one, improving the application performance and enhancing the system with runtime adaptation capabilities. However, complex topologies involving many operators may cause some instability that can decrease the DSP application availability. © 2015 IEEE.},
author_keywords={Cloud computing;  Digital signal processing;  Fasteners;  Monitoring;  Quality of service;  Storms;  Topology},
keywords={Cloud computing;  Data handling;  Digital signal processing;  Fasteners;  Monitoring;  Open systems;  Petroleum reservoir evaluation;  Quality of service;  Scheduling;  Storms;  Topology, Application performance;  Computing infrastructures;  Computing resource;  Data stream processing;  Distributed clusters;  Experimental evaluation;  QoS Aware Scheduling;  Variable environment, Fog computing},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Karimov20181519,
author={Karimov, J. and Rabl, T. and Katsifodimos, A. and Samarev, R. and Heiskanen, H. and Markl, V.},
title={Benchmarking distributed stream data processing systems},
journal={Proceedings - IEEE 34th International Conference on Data Engineering, ICDE 2018},
year={2018},
pages={1519-1530},
doi={10.1109/ICDE.2018.00169},
art_number={8509390},
note={cited By 90},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057125951&doi=10.1109%2fICDE.2018.00169&partnerID=40&md5=a59d920e76d75d26ec0ac993853b69ed},
abstract={The need for scalable and efficient stream analysis has led to the development of many open-source streaming data processing systems (SDPSs) with highly diverging capabilities and performance characteristics. While first initiatives try to compare the systems for simple workloads, there is a clear gap of detailed analyses of the systems' performance characteristics. In this paper, we propose a framework for benchmarking distributed stream processing engines. We use our suite to evaluate the performance of three widely used SDPSs in detail, namely Apache Storm, Apache Spark, and Apache Flink. Our evaluation focuses in particular on measuring the throughput and latency of windowed operations, which are the basic type of operations in stream analytics. For this benchmark, we design workloads based on real-life, industrial use-cases inspired by the online gaming industry. The contribution of our work is threefold. First, we give a definition of latency and throughput for stateful operators. Second, we carefully separate the system under test and driver, in order to correctly represent the open world model of typical stream processing deployments and can, therefore, measure system performance under realistic conditions. Third, we build the first benchmarking framework to define and test the sustainable performance of streaming systems. Our detailed evaluation highlights the individual characteristics and use-cases of each system. © 2018 IEEE.},
author_keywords={Apache Flink;  Apache Spark;  Apache Storm;  Stream benchmark;  Stream data processing},
keywords={Data handling;  Distributed parameter control systems;  Open systems;  Storms, Apache Flink;  Apache storms;  Distributed stream processing;  Individual characteristics;  Performance characteristics;  Stream data processing;  Streaming data processing;  Sustainable performance, Benchmarking},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lohrmann2015399,
author={Lohrmann, B. and Janacik, P. and Kao, O.},
title={Elastic Stream Processing with Latency Guarantees},
journal={Proceedings - International Conference on Distributed Computing Systems},
year={2015},
volume={2015-July},
pages={399-410},
doi={10.1109/ICDCS.2015.48},
art_number={7164926},
note={cited By 88},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944315559&doi=10.1109%2fICDCS.2015.48&partnerID=40&md5=6f6df4f9a0bf04d80033f45f9ebf469d},
abstract={Many Big Data applications in science and industry have arisen, that require large amounts of streamed or event data to be analyzed with low latency. This paper presents a reactive strategy to enforce latency guarantees in data flows running on scalable Stream Processing Engines (SPEs), while minimizing resource consumption. We introduce a model for estimating the latency of a data flow, when the degrees of parallelism of the tasks within are changed. We describe how to continuously measure the necessary performance metrics for the model, and how it can be used to enforce latency guarantees, by determining appropriate scaling actions at runtime. Therefore, it leverages the elasticity inherent to common cloud technology and cluster resource management systems. We have implemented our strategy as part of the Nephele SPE. To showcase the effectiveness of our approach, we provide an experimental evaluation on a large commodity cluster, using both a synthetic workload as well as an application performing real-time sentiment analysis on real-world social media data. © 2015 IEEE.},
author_keywords={Autoscaling;  Big Data;  Elastic Scaling;  Latency Constraint;  Latency Guarantee;  Stream Processing;  Stream Processing Engine;  Streaming},
keywords={Acoustic streaming;  Big data;  Data flow analysis;  Data transfer;  Engines;  Petroleum reservoir evaluation;  Sentiment analysis, Autoscaling;  Elastic Scaling;  Latency constraints;  Latency Guarantee;  Stream processing;  Stream processing engines, Data streams},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Venkataraman2017374,
author={Venkataraman, S. and Armbrust, M. and Panda, A. and Ghodsi, A. and Ousterhout, K. and Franklin, M.J. and Recht, B. and Stoica, I.},
title={Drizzle: Fast and Adaptable Stream Processing at Scale},
journal={SOSP 2017 - Proceedings of the 26th ACM Symposium on Operating Systems Principles},
year={2017},
pages={374-389},
doi={10.1145/3132747.3132750},
note={cited By 81},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041649686&doi=10.1145%2f3132747.3132750&partnerID=40&md5=2646a10b8c9460395c17a5783b49b108},
abstract={Large scale streaming systems aim to provide high throughput and low latency. They are often used to run mission-critical applications, and must be available 24x7. Thus such systems need to adapt to failures and inherent changes in workloads, with minimal impact on latency and throughput. Unfortunately, existing solutions require operators to choose between achieving low latency during normal operation and incurring minimal impact during adaptation. Continuous operator streaming systems, such as Naiad and Flink, provide low latency during normal execution but incur high overheads during adaptation (e.g., recovery), while micro-batch systems, such as Spark Streaming and FlumeJava, adapt rapidly at the cost of high latency during normal operations. Our key observation is that while streaming workloads require millisecond-level processing, workload and cluster properties change less frequently. Based on this, we develop Drizzle, a system that decouples the processing interval from the coordination interval used for fault tolerance and adaptability. Our experiments on a 128 node EC2 cluster show that on the Yahoo Streaming Benchmark, Drizzle can achieve end-to-end record processing latencies of less than 100ms and can get 2–3x lower latency than Spark. Drizzle also exhibits better adaptability, and can recover from failures 4x faster than Flink while having up to 13x lower latency during recovery. © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.},
author_keywords={Performance;  Reliability;  Stream Processing},
keywords={Fault tolerance;  Recovery;  Reliability, Cluster property;  Continuous operators;  Large-scale streaming;  Mission critical applications;  Normal operations;  Performance;  Stream processing;  Streaming systems, Fault tolerant computer systems},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mokbel2005343,
author={Mokbel, M.F. and Xiong, X. and Aref, W.G. and Hammad, M.A.},
title={Continuous query processing of spatio-temporal data streams in PLACE},
journal={GeoInformatica},
year={2005},
volume={9},
number={4},
pages={343-365},
doi={10.1007/s10707-005-4576-7},
note={cited By 77},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-28044451960&doi=10.1007%2fs10707-005-4576-7&partnerID=40&md5=58af2757bab1603754ac7709dbaf0bad},
abstract={The tremendous increase in the use of cellular phones, GPS-like devices, and RFIDs results in highly dynamic environments where objects as well as queries are continuously moving. In this paper, we present a continuous query processor designed specifically for highly dynamic environments (e.g., location-aware environments). We implemented the proposed continuous query processor inside the PLACE server (Pervasive Location-Aware Computing Environments); a scalable location-aware database server developed at Purdue University. The PLACE server extends data streaming management systems to support location-aware environments. These environments are characterized by the wide variety of continuous spatio-temporal queries and the unbounded spatio-temporal streams. The proposed continuous query processor includes: (1) New incremental spatio-temporal operators to support a wide variety of continuous spatio-temporal queries, (2) Extended semantics of sliding window queries to deal with spatial sliding windows as well as temporal sliding windows, and (3) A shared-execution framework for scalable execution of a set of concurrent continuous spatio-temporal queries. Experimental evaluation shows promising performance of the continuous query processor of the PLACE server. © 2005 Springer Science + Business Media, Inc.},
author_keywords={Continuous queries;  Data stream management systems;  Location-aware services;  Spatio-temporal databases},
keywords={Client server computer systems;  Query languages;  Semantics;  Servers, Continuous queries;  Data stream management systems;  Location-aware services;  Spatio-temporal databases, Database systems, information system;  spatial data},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kolajo2019,
author={Kolajo, T. and Daramola, O. and Adebiyi, A.},
title={Big data stream analysis: a systematic literature review},
journal={Journal of Big Data},
year={2019},
volume={6},
number={1},
doi={10.1186/s40537-019-0210-7},
art_number={47},
note={cited By 71},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067007452&doi=10.1186%2fs40537-019-0210-7&partnerID=40&md5=4d98e0d12e6c690822446610d2bd34a2},
abstract={Recently, big data streams have become ubiquitous due to the fact that a number of applications generate a huge amount of data at a great velocity. This made it difficult for existing data mining tools, technologies, methods, and techniques to be applied directly on big data streams due to the inherent dynamic characteristics of big data. In this paper, a systematic review of big data streams analysis which employed a rigorous and methodical approach to look at the trends of big data stream tools and technologies as well as methods and techniques employed in analysing big data streams. It provides a global view of big data stream tools and technologies and its comparisons. Three major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and conferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier were explored as data sources. Out of the initial 2295 papers that resulted from the first search string, 47 papers were found to be relevant to our research questions after implementing the inclusion and exclusion criteria. The study found that scalability, privacy and load balancing issues as well as empirical analysis of big data streams and technologies are still open for further research efforts. We also found that although, significant research efforts have been directed to real-time analysis of big data stream not much attention has been given to the preprocessing stage of big data streams. Only a few big data streaming tools and technologies can do all of the batch, streaming, and iterative jobs; there seems to be no big data tool and technology that offers all the key features required for now and standard benchmark dataset for big data streaming analytics has not been widely adopted. In conclusion, it was recommended that research efforts should be geared towards developing scalable frameworks and algorithms that will accommodate data stream computing mode, effective resource allocation strategy and parallelization issues to cope with the ever-growing size and complexity of data. © 2019, The Author(s).},
author_keywords={Big data stream analysis;  Big data streaming tools and technologies;  Stream computing},
publisher={SpringerOpen},
language={English},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Feng2008269,
author={Feng, C. and Li, B.},
title={On large-scale peer-to-peer streaming Systems with Network Coding},
journal={MM'08 - Proceedings of the 2008 ACM International Conference on Multimedia, with co-located Symposium and Workshops},
year={2008},
pages={269-278},
doi={10.1145/1459359.1459396},
note={cited By 71},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350662969&doi=10.1145%2f1459359.1459396&partnerID=40&md5=a1076d2e8b7540bb091e2e929d65c030},
abstract={Live peer-to-peer (P2P) streaming has recently received much research attention, with successful commercial systems showing its viability in the Internet. Nevertheless, existing analytical studies of P2P streaming systems have failed to mathematically investigate and understand their critical properties, especially with a large scale and under extreme dynamics such as a flash crowd scenario. Even more importantly, there exists no prior analytical work that focuses on an entirely new way of designing streaming protocols, with the help of network coding. In this paper, we seek to show an in-depth analytical understanding of fundamental properties of P2P streaming systems, with a particular spotlight on the benefits of network coding. We show that, if network coding is used according to certain design principles, provably good performance can be guaranteed, with respect to high playback qualities, short initial buffering delays, resilience to peer dynamics, as well as minimal bandwidth costs on dedicated streaming servers. Our results are obtained with mathematical rigor, but without sacrificing realistic assumptions of system scale, peer dynamics, and upload capacities. For further insights, streaming systems using network coding are compared with traditional pull-based streaming in large-scale simulations, with a focus on fundamentals, rather than protocol details. The scale of our simulations throughout this paper exceeds 200, 000 peers at times, which is in sharp contrast with existing empirical studies, typically with a few hundred peers involved. Copyright 2008 ACM.},
author_keywords={Network coding;  Peer-to-peer streaming;  Performance analysis},
keywords={Buffering delay;  Commercial systems;  Critical properties;  Design Principles;  Empirical studies;  Flash crowd;  Fundamental properties;  Large scale simulations;  Network coding;  P2P streaming;  Peer to peer;  Peer-to-peer streaming;  Performance analysis;  Sharp contrast;  STreaming protocols;  Streaming servers, Computer simulation;  Internet protocols;  Method of moments;  Multimedia systems, Network protocols},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shukla2017,
author={Shukla, A. and Chaturvedi, S. and Simmhan, Y.},
title={RIoTBench: An IoT benchmark for distributed stream processing systems},
journal={Concurrency and Computation: Practice and Experience},
year={2017},
volume={29},
number={21},
doi={10.1002/cpe.4257},
art_number={e4257},
note={cited By 70},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030641647&doi=10.1002%2fcpe.4257&partnerID=40&md5=8b8ab5746cc7df7d0a4ef0a56152e02a},
abstract={The Internet of Things (IoT) is an emerging technology paradigm where millions of sensors and actuators help monitor and manage physical, environmental, and human systems in real time. The inherent closed-loop responsiveness and decision making of IoT applications make them ideal candidates for using low latency and scalable stream processing platforms. Distributed stream processing systems (DSPS) hosted in cloud data centers are becoming the vital engine for real-time data processing and analytics in any IoT software architecture. But the efficacy and performance of contemporary DSPS have not been rigorously studied for IoT applications and data streams. Here, we propose RIoTBench, a real-time IoT benchmark suite, along with performance metrics, to evaluate DSPS for streaming IoT applications. The benchmark includes 27 common IoT tasks classified across various functional categories and implemented as modular microbenchmarks. Further, we define four IoT application benchmarks composed from these tasks based on common patterns of data preprocessing, statistical summarization, and predictive analytics that are intrinsic to the closed-loop IoT decision-making life cycle. These are coupled with four stream workloads sourced from real IoT observations on smart cities and smart health, with peak streams rates that range from 500 to 10 000 messages/second from up to 3 million sensors. We validate the RIoTBench suite for the popular Apache Storm DSPS on the Microsoft Azure public cloud and present empirical observations. This suite can be used by DSPS researchers for performance analysis and resource scheduling, by IoT practitioners to evaluate DSPS platforms, and even reused within IoT solutions. Copyright © 2017 John Wiley & Sons, Ltd.},
author_keywords={benchmark;  big data applications;  dataflows;  distributed stream processing;  Internet of Things;  performance evaluation},
keywords={Benchmarking;  Data streams;  Decision making;  Distributed parameter control systems;  Environmental technology;  Predictive analytics;  Real time systems;  Windows operating system, Big data applications;  Data flow;  Distributed stream processing;  Emerging technologies;  Internet of thing (IOT);  performance evaluation;  Real-time data processing;  Sensors and actuators, Internet of things},
publisher={John Wiley and Sons Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lu201469,
author={Lu, R. and Wu, G. and Xie, B. and Hu, J.},
title={Stream bench: Towards benchmarking modern distributed stream computing frameworks},
journal={Proceedings - 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing, UCC 2014},
year={2014},
pages={69-78},
doi={10.1109/UCC.2014.15},
art_number={7027482},
note={cited By 68},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946688481&doi=10.1109%2fUCC.2014.15&partnerID=40&md5=5967e445bcc63ffc44e5e3a254d6c668},
abstract={While big data is becoming ubiquitous, interest in handling data stream at scale is also gaining popularity, which leads to the sprout of many distributed stream computing systems. However, complexity of stream computing and diversity of workloads expose great challenges to benchmark these systems. Due to lack of standard criteria, evaluations and comparisons of these systems tend to be difficult. This paper takes an early step towards benchmarking modern distributed stream computing frameworks. After identifying the challenges and requirements in the field, we raise our benchmark definition Stream Bench regarding the requirements. Stream Bench proposes a message system functioning as a mediator between stream data generation and consumption. It also covers 7 benchmark programs that intend to address typical stream computing scenarios and core operations. Not only does it care about performance of systems under different data scales, but also takes fault tolerance ability and durability into account, which drives to incorporate four workload suites targeting at these various aspects of systems. Finally, we illustrate the feasibility of Stream Bench by applying it to two popular frameworks, Apache Storm and Apache Spark Streaming. We draw comparisons from various perspectives between the two platforms with workload suites of Stream Bench. In addition, we also demonstrate performance improvement of Storm's latest version with the benchmark. © 2014 IEEE.},
author_keywords={Benchmark;  Big data;  Distributed stream computing},
keywords={Benchmarking;  Big data;  Data handling;  Digital storage;  Fault tolerance;  Fault tolerant computer systems;  Storms;  Ubiquitous computing, Benchmark definition;  Benchmark programs;  Data stream;  Message systems;  Performance of systems;  Stream computing;  Stream data, Distributed computer systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang20162114,
author={Zhang, Y. and Cao, T. and Li, S. and Tian, X. and Yuan, L. and Jia, H. and Vasilakos, A.V.},
title={Parallel Processing Systems for Big Data: A Survey},
journal={Proceedings of the IEEE},
year={2016},
volume={104},
number={11},
pages={2114-2136},
doi={10.1109/JPROC.2016.2591592},
art_number={7547948},
note={cited By 65},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983084815&doi=10.1109%2fJPROC.2016.2591592&partnerID=40&md5=578f143f95e51749aa8780133a3c006c},
abstract={The volume, variety, and velocity properties of big data and the valuable information it contains have motivated the investigation of many new parallel data processing systems in addition to the approaches using traditional database management systems (DBMSs). MapReduce pioneered this paradigm change and rapidly became the primary big data processing system for its simplicity, scalability, and fine-grain fault tolerance. However, compared with DBMSs, MapReduce also arouses controversy in processing efficiency, low-level abstraction, and rigid dataflow. Inspired by MapReduce, nowadays the big data systems are blooming. Some of them follow MapReduce's idea, but with more flexible models for general-purpose usage. Some absorb the advantages of DBMSs with higher abstraction. There are also specific systems for certain applications, such as machine learning and stream data processing. To explore new research opportunities and assist users in selecting suitable processing systems for specific applications, this survey paper will give a high-level overview of the existing parallel data processing systems categorized by the data input as batch processing, stream processing, graph processing, and machine learning processing and introduce representative projects in each category. As the pioneer, the original MapReduce system, as well as its active variants and extensions on dataflow, data access, parameter tuning, communication, and energy optimizations will be discussed at first. System benchmarks and open issues for big data processing will also be studied in this survey. © 1963-2012 IEEE.},
author_keywords={Big data;  machine learning;  MapReduce;  parallel processing;  SQL;  survey},
keywords={Abstracting;  Big data;  Data handling;  Fault tolerance;  Information management;  Learning systems;  Machine learning;  Parallel processing systems;  Surveying;  Surveys, Data processing systems;  Energy optimization;  Map-reduce;  Parallel data processing;  Parallel processing;  Processing systems;  Research opportunities;  Stream data processing, Batch data processing},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Review},
source={Scopus},
}

@ARTICLE{Dayarathna2018,
author={Dayarathna, M. and Perera, S.},
title={Recent advancements in event processing},
journal={ACM Computing Surveys},
year={2018},
volume={51},
number={2},
doi={10.1145/3170432},
art_number={33},
note={cited By 63},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042603476&doi=10.1145%2f3170432&partnerID=40&md5=a14ab7658a342d1b700ee67fa2c6051a},
abstract={Event processing (EP) is a data processing technology that conducts online processing of event information. In this survey, we summarize the latest cutting-edge work done on EP from both industrial and academic research community viewpoints. We divide the entire field of EP into three subareas: EP system architectures, EP use cases, and EP open research topics. Then we deep dive into the details of each subsection. We investigate the system architecture characteristics of novel EP platforms, such as Apache Storm, Apache Spark, and Apache Flink. We found significant advancements made on novel application areas, such as the Internet of Things; streaming machine learning (ML); and processing of complex data types such as text, video data streams, and graphs. Furthermore, there has been significant body of contributions made on event ordering, system scalability, development of EP languages and exploration of use of heterogeneous devices for EP, which we investigate in the latter half of this article. Through our study, we found key areas that require significant attention from the EP community, such as Streaming ML, EP system benchmarking, and graph stream processing. © 2018 ACM.},
author_keywords={Complex event processing;  Data stream processing;  Event processing},
keywords={Data handling;  Industrial research;  Learning systems;  Video streaming, Complex event processing;  Data processing technologies;  Data stream processing;  Event Processing;  Heterogeneous devices;  System architectures;  System scalability;  Video data streams, Computer architecture},
publisher={Association for Computing Machinery},
language={English},
document_type={Review},
source={Scopus},
}

@ARTICLE{Chen2013,
author={Chen, Y. and Zhang, B. and Liu, Y. and Zhu, W.},
title={Measurement and modeling of video watching time in a large-scale internet video-on-demand system},
journal={IEEE Transactions on Multimedia},
year={2013},
volume={15},
number={8},
pages={2087-2098A},
doi={10.1109/TMM.2013.2280123},
art_number={6587820},
note={cited By 60},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888321659&doi=10.1109%2fTMM.2013.2280123&partnerID=40&md5=b4f0039bdbf879c2ec103fa6b621fda1},
abstract={Video watching time is a crucial measure for studying user watching behavior in online Internet video-on-demand (VoD) systems. It is important for system planning, user engagement understanding, and system quality evaluation. However, due to the limited access of user data in large-scale streaming systems, a systematic measurement, analysis, and modeling of video watching time is still missing. In this paper, we measure PPLive, one of the most popular commercial Internet VoD systems in China, over a three week period. We collect accurate user watching data of more than 100 million streaming sessions of more than 100 thousand distinct videos. Based on the measurement data, we characterize the distribution of watching time of different types of videos and reveal a number of interesting characteristics regarding the relation between video watching time and various video-related features (including video type, duration, and popularity). We further build a suite of mathematical models for characterizing these relationships. Extensive performance evaluation shows the high accuracy of these models as compared with commonly used data-mining based models. Our measurement and modeling results bring forth important insights for simulation, design, deployment, and evaluation of Internet VoD systems. © 2013 IEEE.},
author_keywords={Consumer behavior;  Measurement;  Modeling;  Streaming media;  Videos},
keywords={Commercial internet;  Large-scale streaming;  Measurement data;  Streaming media;  Streaming sessions;  Video-on-demand system;  Videoon-demand systems (VoD);  Videos, Behavioral research;  Consumer behavior;  Internet;  Mathematical models;  Measurements;  Models;  Online systems;  Video on demand, Quality control},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Schlinker2007,
author={Schlinker, R.H. and Liljenberg, S.A. and Polak, D.R. and Post, K.A. and Chipman, C.T. and Stern, A.M.},
title={Supersonic jet noise source characteristics & propagation: Engine and model scale},
journal={13th AIAA/CEAS Aeroacoustics Conference (28th AIAA Aeroacoustics Conference)},
year={2007},
note={cited By 59},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884736650&partnerID=40&md5=66517fc45b4e12f55bbf0e399a39d307},
abstract={This study benchmarks the source characteristics and propagation of noise on a full scale supersonic exhaust stream engine. The intent is to provide a better understanding of supersonic jet noise generation mechanisms and high intensity propagation for both commercial and military engines. Although high exhaust stream Mach number studies are available to researchers, these are usually limited in temperature and velocity capability by today's model scale test facilities. A series of full scale engine noise measurements were, therefore, conducted using ground plane microphones in the geometric near field below the engine, far field microphones extending to over 300 engine diameters, and, a 30 microphone phased array. This diagnostic configuration provided: a) engine noise directivity and OASPL dependence on exhaust conditions, b) acoustic spectra and propagation decay in the peak directivity angle, c) acoustic source distribution characteristics in the jet axial direction, and d) impulsive acoustic signature dependence on exhaust conditions and variation with propagation distance. The data was acquired over a supersonic Mach number range that varied by factor of 2 and also included variations in nozzle temperature ratio beyond existing laboratory studies. © 2007 by Robert Hans Schlinker.},
keywords={Acoustic signature;  Directivity angle;  Exhaust conditions;  Laboratory studies;  Military engines;  Nozzle temperature;  Propagation distances;  Source characteristics, Acoustic noise measurement;  Aeroacoustics;  Engines;  Mach number;  Microphones;  Supersonic aerodynamics, Acoustic noise},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Luckow20151201,
author={Luckow, A. and Kennedy, K. and Manhardt, F. and Djerekarov, E. and Vorster, B. and Apon, A.},
title={Automotive big data: Applications, workloads and infrastructures},
journal={Proceedings - 2015 IEEE International Conference on Big Data, IEEE Big Data 2015},
year={2015},
pages={1201-1210},
doi={10.1109/BigData.2015.7363874},
art_number={7363874},
note={cited By 57},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963740079&doi=10.1109%2fBigData.2015.7363874&partnerID=40&md5=c6d66c216ef332667f37ec75fcfff4d0},
abstract={Data is increasingly affecting the automotive industry, from vehicle development, to manufacturing and service processes, to online services centered around the connected vehicle. Connected, mobile and Internet of Things devices and machines generate immense amounts of sensor data. The ability to process and analyze this data to extract insights and knowledge that enable intelligent services, new ways to understand business problems, improvements of processes and decisions, is a critical capability. Hadoop is a scalable platform for compute and storage and emerged as de-facto standard for Big Data processing at Internet companies and in the scientific community. However, there is a lack of understanding of how and for what use cases these new Hadoop capabilities can be efficiently used to augment automotive applications and systems. This paper surveys use cases and applications for deploying Hadoop in the automotive industry. Over the years a rich ecosystem emerged around Hadoop comprising tools for parallel, in-memory and stream processing (most notable MapReduce and Spark), SQL and NOSQL engines (Hive, HBase), and machine learning (Mahout, MLlib). It is critical to develop an understanding of automotive applications and their characteristics and requirements for data discovery, integration, exploration and analytics. We then map these requirements to a confined technical architecture consisting of core Hadoop services and libraries for data ingest, processing and analytics. The objective of this paper is to address questions, such as: What applications and datasets are suitable for Hadoop? How can a diverse set of frameworks and tools be managed on multi-tenant Hadoop cluster? How do these tools integrate with existing relational data management systems? How can enterprise security requirements be addressed? What are the performance characteristics of these tools for real-world automotive applications? To address the last question, we utilize a standard benchmark (TPCx-HS), and two application benchmarks (SQL and machine learning) that operate on a dataset of multiple Terabytes and billions of rows. © 2015 IEEE.},
keywords={Artificial intelligence;  Automobiles;  Automotive industry;  Benchmarking;  Data handling;  Digital storage;  Information management;  Internet;  Learning systems, Automotive applications;  De facto standard;  Enterprise security;  Intelligent Services;  Performance characteristics;  Scientific community;  Technical architecture;  Vehicle development, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lin2015811,
author={Lin, Q. and Ooi, B.C. and Wang, Z. and Yu, C.},
title={Scalable distributed stream join processing},
journal={Proceedings of the ACM SIGMOD International Conference on Management of Data},
year={2015},
volume={2015-May},
pages={811-825},
doi={10.1145/2723372.2746485},
note={cited By 57},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957037809&doi=10.1145%2f2723372.2746485&partnerID=40&md5=334031c5fb39fb9a26fad76b561a8c20},
abstract={Efficient and scalable stream joins play an important role in performing real-time analytics for many cloud applications. However, like in conventional database processing, online theta-joins over data streams are computationally expensive and moreover, being memory-based processing, they impose high memory requirement on the system. In this paper, we propose a novel stream join model, called join-biclique, which organizes a large cluster as a complete bipartite graph. Join-biclique has several strengths over state-of-the-art techniques, including memory-efficiency, elasticity and scalability. These features are essential for building efficient and scalable streaming systems. Based on join-biclique, we develop a scalable distributed stream join system, BiStream, over a large-scale commodity cluster. Specifically, BiStream is designed to support efficient full-history joins, windowbased joins and online data aggregation. BiStream also supports adaptive resource management to dynamically scale out and down the system according to its application workloads. We provide both theoretical cost analysis and extensive experimental evaluations to evaluate the efficiency, elasticity and scalability of BiStream. Copyright © 2015 ACM.},
keywords={Efficiency;  Scalability, Adaptive Resource Management;  Commodity clusters;  Complete bipartite graphs;  Conventional database;  Experimental evaluation;  Memory requirements;  Real-time analytics;  State-of-the-art techniques, Elasticity},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Picconi2008289,
author={Picconi, F. and Massoulié, L.},
title={Is there a future for mesh-based live video streaming?},
journal={Proceedings - P2P'08, 8th International Conference on Peer-to-Peer Computing},
year={2008},
pages={289-298},
doi={10.1109/P2P.2008.18},
art_number={4627291},
note={cited By 57},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149109269&doi=10.1109%2fP2P.2008.18&partnerID=40&md5=bde9109fa4653887ad06b7b4b71cd845},
abstract={Peer-to-peer live streaming systems allow a bandwidth-constrained source to broadcast a video feed to a large number of users. In addition, a design with high link utilization can achieve high stream rates, supporting high-quality video. Until now, only tree-based designs have been shown to achieve close-to-optimal rates in real-life conditions, leaving the question open as to the attainable efficiency of completely unstructured mesh-based approaches. In this paper we answer that question by showing that a carefully-designed mesh-based system can achieve close-to-optimal stream rates. Specifically, we implement and evaluate a design based on a mesh-based algorithm called DP/LU. Contrary to tree-based designs, DP/LU uses an unstructured overlay, which is easier to construct and is highly resistant to churn. In addition, we introduce mechanisms for overlay rewiring and source scheduling that lead to significant performance improvements. Our experimental evaluation shows that our design achieves 95% of the maximum achievable stream rate in a static environment, and 90% under high churn. This demonstrates that mesh-based designs are an excellent choice for scalable and robust high-quality peer-to-peer live streaming. © 2008 IEEE.},
keywords={Bandwidth;  Broadcasting;  Communication channels (information theory);  Mesh generation;  Rivers;  Video streaming;  Videotex, Constrained sources;  Experimental evaluations;  Link utilizations;  Live streaming;  Live video streaming;  Optimal rates;  Peer to peers;  Performance improvements;  Quality videos;  Source scheduling;  Static environments;  To broadcasts;  Unstructured meshes;  Unstructured overlays;  Video feeds, Design},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lopez2016,
author={Lopez, M.A. and Lobato, A.G.P. and Duarte, O.C.M.B.},
title={A performance comparison of open-source stream processing platforms},
journal={2016 IEEE Global Communications Conference, GLOBECOM 2016 - Proceedings},
year={2016},
doi={10.1109/GLOCOM.2016.7841533},
art_number={7841533},
note={cited By 53},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015420127&doi=10.1109%2fGLOCOM.2016.7841533&partnerID=40&md5=3e9985b03da63e010d12c93f717541e4},
abstract={Distributed stream processing platforms is a new class of real-time monitoring systems that analyze and extracts knowledge from large continuous streams of data. This type of systems is crucial for providing high throughput and low latency required by Big Data or Internet of Things monitoring applications. This paper describes and analyzes three main open-source distributed stream- processing platforms: Storm Flink, and Spark Streaming. We analyze the system architectures and we compare their main features. We carry out two experiments concerning anomaly detection on network traffic to evaluate the throughput efficiency and the resilience to node failures. Results show that the performance of native stream processing systems, Storm and Flink, is up to 15 times higher than the micro-batch processing system, Spark Streaming. On the other hand, Spark Streaming is more robust to node failures and provides recovery without losses. © 2016 IEEE.},
keywords={Batch data processing;  Continuous time systems;  Distributed parameter control systems;  Real time systems;  Storms, Batch processing system;  Distributed stream processing;  Monitoring applications;  Performance comparison;  Real time monitoring system;  Stream processing systems;  System architectures;  Throughput efficiency, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cetintemel20141633,
author={Cetintemel, U. and Du, J. and Kraska, T. and Madden, S. and Maier, D. and Meehan, J. and Pavlo, A. and Stonebraker, M. and Sutherland, E. and Tatbul, N. and Tufte, K. and Wang, H. and Zdonik, S.},
title={S-Store: A streaming NewSQL system for big velocity applications},
journal={Proceedings of the VLDB Endowment},
year={2014},
volume={7},
number={13},
pages={1633-1636},
doi={10.14778/2733004.2733048},
note={cited By 53},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905829327&doi=10.14778%2f2733004.2733048&partnerID=40&md5=807bbaf92a79fdec9799f2ed29b5fc5f},
abstract={First-generation streaming systems did not pay much attention to state management via ACID transactions (e.g., [3, 4]). S-Store is a data management system that combines OLTP transactions with stream processing. To create S-Store, we begin with H-Store, a main-memory transaction processing engine, and add primitives to support streaming. This includes triggers and transaction workflows to implement push-based processing, windows to provide a way to bound the computation, and tables with hidden state to implement scoping for proper isolation. This demo explores the benefits of this approach by showing how a naïve implementation of our benchmarks using only H-Store can yield incorrect results. We also show that by exploiting push-based semantics and our implementation of triggers, we can achieve significant improvement in transaction throughput. We demo two modern applications: (i) leaderboard maintenance for a version of "American Idol", and (ii) a city-scale bicycle rental scenario. © 2014 VLDB Endowment 2150-8097/14/08.},
keywords={Data streams;  Semantics, ACID Transactions;  Data management system;  Main memory transaction processing;  Modern applications;  State management;  Stream processing;  Streaming systems;  Transaction throughput, Information management},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xu201622,
author={Xu, L. and Peng, B. and Gupta, I.},
title={Stela: Enabling stream processing systems to scale-in and scale-out on-demand},
journal={Proceedings - 2016 IEEE International Conference on Cloud Engineering, IC2E 2016: Co-located with the 1st IEEE International Conference on Internet-of-Things Design and Implementation, IoTDI 2016},
year={2016},
pages={22-31},
doi={10.1109/IC2E.2016.38},
art_number={7484160},
note={cited By 51},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978062188&doi=10.1109%2fIC2E.2016.38&partnerID=40&md5=1aecb0745c21e481fbc515cf96ecd84d},
abstract={The era of big data has led to the emergence of new real-time distributed stream processing engines like Apache Storm. We present Stela (STream processing ELAsticity), a stream processing system that supports scale-out and scale-in operations in an on-demand manner, i.e., when the user requests such a scaling operation. Stela meets two goals: 1) it optimizes post-scaling throughput, and 2) it minimizes interruption to the ongoing computation while the scaling operation is being carried out. We have integrated Stela into Apache Storm. We present experimental results using micro-benchmark Storm applications, as well as production applications from industry (Yahoo! Inc. and IBM). Our experiments show that compared to Apache Storm's default scheduler, Stela's scale-out operation achieves throughput that is 21-120% higher, and interruption time that is significantly smaller. Stela's scale-in operation chooses the right set of servers to remove and achieves 2X-5X higher throughput than Storm's default strategy. © 2016 IEEE.},
author_keywords={Distributed Systems;  Elasticity;  Scalability;  Stream Processing},
keywords={Benchmarking;  Distributed parameter control systems;  Elasticity;  Scalability;  Storms;  Throughput, Distributed stream processing;  Distributed systems;  Micro-benchmark;  On demands;  Real time;  Stream processing;  Stream processing systems, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brenna2009,
author={Brenna, L. and Gehrke, J. and Hong, M. and Johansen, D.},
title={Distributed event stream processing with non-deterministic finite automata},
journal={Proceedings of the 3rd ACM International Conference on Distributed Event-Based Systems, DEBS 2009},
year={2009},
volume={2009-January},
doi={10.1145/1619258.1619263},
art_number={3},
note={cited By 48},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139848278&doi=10.1145%2f1619258.1619263&partnerID=40&md5=c19258886486c8f08ada33dfaea47f70},
abstract={Efficient matching of incoming events to persistent queries is fundamental to event pattern matching, complex event processing, and publish/subscribe systems. Recent processing engines based on non-deterministic finite automata (NFAs) have demonstrated scalability in the number of queries that can be efficiently executed on a single machine. However, existing NFA based systems are limited to processing events on a single machine. Consequently, their event processing capacity cannot be increased by adding more machines. In this paper, we present an experimental evaluation of different methods for distributing an event processing system that is based on NFAs across multiple machines in a cluster. Our results show that careful input stream partitioning gives close to linear performance scaleup for CPU bound workloads. © 2009 ACM.},
author_keywords={continuous queries;  event streams;  NFA;  publish-subscribe},
keywords={Finite automata;  Message passing;  Pipeline processing systems, Continuous queries;  Distributed events;  Event pattern;  Event Processing;  Event stream processing;  Event streams;  Matchings;  Nondeterministic finite automaton;  Publish/subscribe;  Single- machines, Pattern matching},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cardellini201711,
author={Cardellini, V. and Grassi, V. and Presti, F.L. and Nardelli, M.},
title={Optimal operator replication and placement for distributed stream processing systems},
journal={Performance Evaluation Review},
year={2017},
volume={44},
number={4},
pages={11-22},
doi={10.1145/3092819.3092823},
art_number={3092823},
note={cited By 47},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019831968&doi=10.1145%2f3092819.3092823&partnerID=40&md5=4d0815803767bbc43e7517d8daca458d},
abstract={Exploiting on-The-fly computation, Data Stream Processing (DSP) applications are widely used to process unbounded streams of data and extract valuable information in a near real-Time fashion. As such, they enable the development of new intelligent and pervasive services that can improve our everyday life. To keep up with the high volume of daily produced data, the operators that compose a DSP application can be replicated and placed on multiple, possibly distributed, computing nodes, so to process the incoming data flow in parallel. Moreover, to better exploit the abundance of diffused computational resources (e.g., Fog computing), recent trends investigate the possibility of decentralizing the DSP application placement. In this paper, we present and evaluate a general formulation of the optimal DSP replication and placement (ODRP) as an integer linear programming problem, which takes into account the heterogeneity of application requirements and infrastructural resources. We integrate ODRP as prototype scheduler in the Apache Storm DSP framework. By leveraging on the DEBS 2015 Grand Challenge as benchmark application, we show the benefits of a joint optimization of operator replication and placement and how ODRP can optimize different QoS metrics, namely response time, internode traffic, cost, availability, and a combination thereof.},
keywords={Benchmarking;  Data mining;  Distributed parameter control systems;  Integer programming;  Parallel flow, Application requirements;  Benchmark applications;  Computational resources;  Data stream processing;  Distributed stream processing;  Integer Linear Programming;  Joint optimization;  On-the-fly computations, Data handling},
publisher={Association for Computing Machinery},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zeitler20111181,
author={Zeitler, E. and Risch, T.},
title={Massive scale-out of expensive continuous queries},
journal={Proceedings of the VLDB Endowment},
year={2011},
volume={4},
number={11},
pages={1181-1188},
note={cited By 46},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863754736&partnerID=40&md5=bb42d192cde98e0c5053ebe603130619},
abstract={Scalable execution of expensive continuous queries over massive data streams requires input streams to be split into parallel substreams. The query operators are continuously executed in parallel over these sub-streams. Stream splitting involves both partitioning and replication of incoming tuples, depending on how the continuous query is parallelized. We provide a stream splitting operator that enables such customized stream splitting. However, it is critical that the stream splitting itself keeps up with input streams of high volume. This is a problem when the stream splitting predicates have some costs. Therefore, to enable customized splitting of high-volume streams, we introduce a parallelized stream splitting operator, called parasplit. We investigate the performance of parasplit using a cost model and experimentally. Based on these results, a heuristic is devised to automatically parallelize the execution of parasplit. We show that the maximum stream rate of parasplit is network bound, and that the parallelization is energy efficient. Finally, the scalability of our approach is experimentally demonstrated on the Linear Road Benchmark, showing an order of magnitude higher stream processing rate over previously published results, allowing at least 512 expressways. © 2011 VLDB Endowment.},
keywords={Continuous queries;  Cost models;  Energy efficient;  Input streams;  Massive data streams;  Parallelizations;  Query operators;  Stream processing;  Sub-streams;  Continuous queries;  Cost modeling;  Energy efficient;  Input streams;  Massive data streams;  Parallelizations;  Query operators;  Stream processing, Energy efficiency;  Energy efficiency, Query processing;  Data streams},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cardellini2018171,
author={Cardellini, V. and Lo Presti, F. and Nardelli, M. and Russo Russo, G.},
title={Decentralized self-adaptation for elastic Data Stream Processing},
journal={Future Generation Computer Systems},
year={2018},
volume={87},
pages={171-185},
doi={10.1016/j.future.2018.05.025},
note={cited By 45},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047359666&doi=10.1016%2fj.future.2018.05.025&partnerID=40&md5=e8bec49dc333406adbf61dc4ccda8576},
abstract={Data Stream Processing (DSP) applications are widely used to develop new pervasive services, which require to seamlessly process huge amounts of data in a near real-time fashion. To keep up with the high volume of daily produced data, these applications need to dynamically scale their execution on multiple computing nodes, so to process the incoming data flow in parallel. In this paper, we present a hierarchical distributed architecture for the autonomous control of elastic DSP applications. It consists of a two-layered hierarchical solution, where a centralized per-application component coordinates the run-time adaptation of subordinated distributed components, which, in turn, locally control the adaptation of single DSP operators. Thanks to its features, the proposed solution can efficiently run in large-scale Fog computing environments. Exploiting this framework, we design several distributed self-adaptation policies, including a popular threshold-based approach and two reinforcement learning solutions. We integrate the hierarchical architecture and the devised self-adaptation policies in Apache Storm, a popular open-source DSP framework. Relying on the DEBS 2015 Grand Challenge as a benchmark application, we show the benefits of the presented self-adaptation policies, and discuss the strengths of reinforcement learning based approaches, which autonomously learn from experience how to optimize the application performance. © 2018 Elsevier B.V.},
author_keywords={Data stream processing;  Hierarchical control;  MAPE;  Reinforcement learning;  Self adaptive},
keywords={Benchmarking;  Embedded systems;  Fog computing;  Parallel flow;  Reinforcement learning, Application performance;  Data stream processing;  Distributed architecture;  Hierarchical architectures;  Hierarchical control;  Mape;  Reinforcement learning solution;  Self-Adaptive, Data handling},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bar2015165,
author={Bar, A. and Finamore, A. and Casas, P. and Golab, L. and Mellia, M.},
title={Large-scale network traffic monitoring with DBStream, a system for rolling big data analysis},
journal={Proceedings - 2014 IEEE International Conference on Big Data, IEEE Big Data 2014},
year={2015},
pages={165-170},
doi={10.1109/BigData.2014.7004227},
art_number={7004227},
note={cited By 45},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921756125&doi=10.1109%2fBigData.2014.7004227&partnerID=40&md5=75409a7b8b3383d5914c84795934c591},
abstract={The complexity of the Internet has rapidly increased, making it more important and challenging to design scalable network monitoring tools. Network monitoring typically requires rolling data analysis, i.e., continuously and incrementally updating (rolling-over) various reports and statistics over highvolume data streams. In this paper, we describe DBStream, which is an SQL-based system that explicitly supports incremental queries for rolling data analysis. We also present a performance comparison of DBStream with a parallel data processing engine (Spark), showing that, in some scenarios, a single DBStream node can outperform a cluster of ten Spark nodes on rolling network monitoring workloads. Although our performance evaluation is based on network monitoring data, our results can be generalized to other big data problems with high volume and velocity. © 2014 IEEE.},
author_keywords={Big Data Analysis;  Data Stream Processing;  Network Data Analysis;  System Performance},
keywords={Complex networks;  Data communication systems;  Data handling;  Electric sparks;  Information analysis;  Monitoring, Data stream processing;  Incrementally Updating;  Large-scale network;  Network data;  Network Monitoring;  Parallel data processing;  Performance comparison;  System Performance, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Paniga2011165,
author={Paniga, S. and Borsani, L. and Redondi, A. and Tagliasacchi, M. and Cesana, M.},
title={Experimental evaluation of a video streaming system for Wireless Multimedia Sensor Networks},
journal={2011 the 10th IFIP Annual Mediterranean Ad Hoc Networking Workshop, Med-Hoc-Net'2011},
year={2011},
pages={165-170},
doi={10.1109/Med-Hoc-Net.2011.5970484},
art_number={5970484},
note={cited By 45},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052115212&doi=10.1109%2fMed-Hoc-Net.2011.5970484&partnerID=40&md5=4f1b81048cbbd0d9054cd1b0551d0c56},
abstract={Wireless Multimedia Sensor Networks (WMSNs) are recently emerging as an extension to traditional scalar wireless sensor networks, with the distinctive feature of supporting the acquisition and delivery of multimedia content such as audio, images and video. In this paper, a complete framework is proposed and developed for streaming video flows in WMSNs. Such framework is designed in a cross-layer fashion with three main building blocks: (i) a hybrid DPCM/DCT encoder; (ii) a congestion control mechanism and (iii) a selective priority automatic request mechanism at the MAC layer. The system has been implemented on the IntelMote2 platform operated by TinyOS and thoroughly evaluated through testbed experiments on multi-hop WMSNs. The source code of the whole system is publicly available to enable reproducible research. © 2011 IEEE.},
keywords={Building blockes;  Congestion control mechanism;  Cross-layer;  Distinctive features;  Experimental evaluation;  MAC layer;  Multihop;  Multimedia contents;  Reproducible research;  Source codes;  Streaming videos;  Whole systems;  Wireless multimedia sensor network, Audio streaming;  Sensors;  Videotex;  Wireless sensor networks, Wireless ad hoc networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zeuch2018516,
author={Zeuch, S. and Monte, B.D. and Karimov, J. and Lutz, C. and Renz, M. and Traub, J. and Breß, S. and Rabl, T. and Markl, V.},
title={Analyzing Efficient Stream Processing on Modern Hardware},
journal={Proceedings of the VLDB Endowment},
year={2018},
volume={12},
number={5},
pages={516-530},
doi={10.14778/3303753.3303758},
note={cited By 44},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082658899&doi=10.14778%2f3303753.3303758&partnerID=40&md5=60eb64e1731997314b767b1b4953c9d0},
abstract={Modern Stream Processing Engines (SPEs) process large data volumes under tight latency constraints. Many SPEs execute processing pipelines using message passing on sharednothing architectures and apply a partition-based scale-out strategy to handle high-velocity input streams. Furthermore, many state-of-the-art SPEs rely on a Java Virtual Machine to achieve platform independence and speed up system development by abstracting from the underlying hardware. In this paper, we show that taking the underlying hardware into account is essential to exploit modern hardware efficiently. To this end, we conduct an extensive experimental analysis of current SPEs and SPE design alternatives optimized for modern hardware. Our analysis highlights potential bottlenecks and reveals that state-of-the-art SPEs are not capable of fully exploiting current and emerging hardware trends, such as multi-core processors and high-speed networks. Based on our analysis, we describe a set of design changes to the common architecture of SPEs to scale-up on modern hardware. We show that the single-node throughput can be increased by up to two orders of magnitude compared to state-of-the-art SPEs by applying specialized code generation, fusing operators, batch-style parallelization strategies, and optimized windowing. This speedup allows for deploying typical streaming applications on a single or a few nodes instead of large clusters. © 2019, VLDB Endowment.},
keywords={Data streams;  HIgh speed networks;  Message passing;  Network architecture;  Pipeline processing systems, Common architecture;  Experimental analysis;  Java virtual machines;  Multi-core processor;  Parallelization strategies;  Platform independence;  Stream processing engines;  Streaming applications, Computer hardware description languages},
publisher={VLDB Endowment},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kumbhare2015105,
author={Kumbhare, A.G. and Simmhan, Y. and Frincu, M. and Prasanna, V.K.},
title={Reactive resource provisioning heuristics for dynamic dataflows on cloud infrastructure},
journal={IEEE Transactions on Cloud Computing},
year={2015},
volume={3},
number={2},
pages={105-118},
doi={10.1109/TCC.2015.2394316},
art_number={7015569},
note={cited By 44},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933042168&doi=10.1109%2fTCC.2015.2394316&partnerID=40&md5=8b5495c8cc208b7e86f7ee7c58ae5a69},
abstract={The need for low latency analysis over high-velocity data streams motivates the need for distributed continuous dataflow systems. Contemporary stream processing systems use simple techniques to scale on elastic cloud resources to handle variable data rates. However, application QoS is also impacted by variability in resource performance exhibited by clouds and hence necessitates autonomic methods of provisioning elastic resources to support such applications on cloud infrastructure. We develop the concept of 'dynamic dataflows' which utilize alternate tasks as additional control over the dataflow's cost and QoS. Further, we formalize an optimization problem to represent deployment and runtime resource provisioning that allows us to balance the application's QoS, value, and the resource cost. We propose two greedy heuristics, centralized and sharded, based on the variable-sized bin packing algorithm and compare against a Genetic Algorithm (GA) based heuristic that gives a near-optimal solution. A large-scale simulation study, using the linear road benchmark and VM performance traces from the AWS public cloud, shows that while GA-based heuristic provides a better quality schedule, the greedy heuristics are more practical, and can intelligently utilize cloud elasticity to mitigate the effect of variability, both in input data rates and cloud resource performance, to meet the QoS of fast data applications. © 2013 IEEE.},
author_keywords={cloud;  Dataflows;  high velocity data;  resource management;  runtime adaptation;  scheduling;  stream processing},
keywords={Benchmarking;  Clouds;  Data flow analysis;  Data streams;  Genetic algorithms;  Scheduling, Data flow;  High velocity;  Resource management;  Runtime adaptation;  Stream processing, Information management},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhao2006298,
author={Zhao, Q. and Ogihara, M. and Wang, H. and Xu, J.},
title={Finding global icebergs over distributed data sets},
journal={Proceedings of the ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
year={2006},
pages={298-307},
doi={10.1145/1142351.1142394},
note={cited By 44},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250634879&doi=10.1145%2f1142351.1142394&partnerID=40&md5=779fd4b393bcffbdeb16e948803ef62a},
abstract={Finding icebergs-items whose frequency of occurrence is above a certain threshold-is an important problem with a wide range of applications. Most of the existing work focuses on iceberg queries at a single node. However, in many real-life applications, data sets are distributed across a large number of nodes. Two naïve approaches might be considered. In the first, each node ships its entire data set to a central server, and the central server uses single-node algorithms to find icebergs. But it may incur prohibitive communication overhead. In the second, each node submits local icebergs, and the central server combines local icebergs to find global icebergs. But it may fail because in many important applications, globally frequent items may not be frequent at any node. In this work, we propose two novel schemes that provide accurate and efficient solutions to this problem: a sampling-based scheme and a counting-sketch-based scheme. In particular, the latter scheme incurs a communication cost at least an order of magnitude smaller than the naïve scheme of shipping all data, yet is able to achieve very high accuracy. Through rigorous theoretical and experimental analysis we establish the statistical properties of our proposed algorithms, including their accuracy bounds. Copyright 2006 ACM.},
author_keywords={Data streaming;  Icebergs;  Statistical inference},
keywords={Communication overhead;  Data streaming;  Icebergs;  Statistical inference, Algorithms;  Query languages;  Sampling;  Servers;  Statistics, Data structures},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Plale2003422,
author={Plale, B. and Schwan, K.},
title={Dynamic querying of streaming data with the dQUOB system},
journal={IEEE Transactions on Parallel and Distributed Systems},
year={2003},
volume={14},
number={4},
pages={422-432},
doi={10.1109/TPDS.2003.1195413},
note={cited By 44},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038633579&doi=10.1109%2fTPDS.2003.1195413&partnerID=40&md5=6c7c7adce4bc45e375b671ca713a64b3},
abstract={Data streaming has established itself as a viable communication abstraction in data-intensive parallel and distributed computations, occurring in applications such as scientific visualization, performance monitoring, and large-scale data transfer. A known problem in large-scale event communication is tailoring the data received at the consumer. It is the general problem of extracting data of interest from a data source, a problem that the database community has successfully addressed with SQL queries, a time tested, user-friendly way for noncomputer scientists to access data. By leveraging the efficiency of query processing provided by relational queries, the dQUOB system provides a conceptual relational data model and SQL query access over streaming data. Queries can be used to extract data, combine streams, and create new streams. The language augments queries with an action to enable more complex data transformations such as Fourier transforms. The dQUOB system has been applied to two large-scale distributed applications: a safety critical autonomous robotics simulation and scientific software visualization for global atmospheric transport modeling. In this paper, we present the dQUOB system and the results of performance evaluation undertaken to assess its applicability in data-intensive wide-area computations, where the benefit of portable data transformation must be evaluated against the cost of continuous query evaluation.},
author_keywords={Data streams;  Data-intensive computations;  Database query processing;  Grid computing;  Publish-subscribe event channels;  Relational data model;  SQL},
keywords={Data transfer;  Database systems;  Fourier transforms;  Query languages;  Robots, Data streaming, Parallel processing systems},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nardelli20191753,
author={Nardelli, M. and Cardellini, V. and Grassi, V. and Lo Presti, F.},
title={Efficient Operator Placement for Distributed Data Stream Processing Applications},
journal={IEEE Transactions on Parallel and Distributed Systems},
year={2019},
volume={30},
number={8},
pages={1753-1767},
doi={10.1109/TPDS.2019.2896115},
art_number={8630099},
note={cited By 43},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061043254&doi=10.1109%2fTPDS.2019.2896115&partnerID=40&md5=feb32607d89bb54097f61f2839b389cd},
abstract={In the last few years, a large number of real-time analytics applications rely on the Data Stream Processing (DSP) so to extract, in a timely manner, valuable information from distributed sources. Moreover, to efficiently handle the increasing amount of data, recent trends exploit the emerging presence of edge/Fog computing resources so to decentralize the execution of DSP applications. Since determining the Optimal DSP Placement (for short, ODP) is an NP-hard problem, we need efficient heuristics that can identify a good application placement on the computing infrastructure in a feasible amount of time, even for large problem instances. In this paper, we present several DSP placement heuristics that consider the heterogeneity of computing and network resources; we divide them in two main groups: model-based and model-free. The former employ different strategies for efficiently solving the ODP model. The latter implement, for the problem at hand, some of the well-known meta-heuristics, namely greedy first-fit, local search, and tabu search. By leveraging on ODP, we conduct a thorough experimental evaluation, aimed to assess the heuristics' efficiency and efficacy under different configurations of infrastructure size, application topology, and optimization objectives. © 2019 IEEE.},
author_keywords={Distributed data stream processing;  geo-distributed systems;  heuristics;  operator placement;  quality of service},
keywords={Computational complexity;  Data handling;  Data mining;  Optimization;  Quality of service;  Search engines;  Storms;  Tabu search, Computational model;  Delays;  Distributed data stream processing;  Distributed systems;  Heuristics;  Operator placements;  Search problem, Distributed computer systems},
publisher={IEEE Computer Society},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zacheilas2015213,
author={Zacheilas, N. and Kalogeraki, V. and Zygouras, N. and Panagiotou, N. and Gunopulos, D.},
title={Elastic complex event processing exploiting prediction},
journal={Proceedings - 2015 IEEE International Conference on Big Data, IEEE Big Data 2015},
year={2015},
pages={213-222},
doi={10.1109/BigData.2015.7363758},
art_number={7363758},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963734206&doi=10.1109%2fBigData.2015.7363758&partnerID=40&md5=92c51c60404ba0214810f4372783ddd7},
abstract={Supporting real-time, cost-effective execution of Complex Event processing applications in the cloud has been an important goal for many scientists in recent years. Distributed Stream Processing Systems (DSPS) have been widely adopted by major computing companies as a powerful approach for large-scale Complex Event processing (CEP). However, determining the appropriate degree of parallelism of the DSPS' components can be particularly challenging as the volume of data streams is becoming increasingly large, the rule set is becoming continuously complex, and the system must be able to handle such large data stream volumes in real-time, taking into consideration changes in the burstiness levels and data characteristics. In this paper we describe our solution to building elastic complex event processing systems on top of our distributed CEP system which combines two commonly used frameworks, Storm and Esper, in order to provide both ease of usage and scalability. Our approach makes the following contributions: (i) we provide a mechanism for predicting the load and latency of the Esper engines in upcoming time windows, and (ii) we propose a novel algorithm for automatically adjusting the number of engines to use in the upcoming windows, taking into account the cost and the performance gains of possible changes. Our detailed experimental evaluation with a real traffic monitoring application that analyzes bus traces from the city of Dublin indicates the benefits in the working of our approach. Our proposal outperforms the current state of the art technique in regards to the amount of tuples that it can process by four orders of magnitude. © 2015 IEEE.},
keywords={Algorithms;  Cost effectiveness;  Data communication systems;  Distributed computer systems;  Distributed parameter control systems;  Engines, Complex event processing;  Complex event processing (CEP);  Data characteristics;  Degree of parallelism;  Distributed stream processing;  Experimental evaluation;  Orders of magnitude;  State-of-the-art techniques, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mueller2013,
author={Mueller, C. and Lederer, S. and Timmerer, C. and Hellwagner, H.},
title={Dynamic adaptive streaming over HTTP/2.0},
journal={Proceedings - IEEE International Conference on Multimedia and Expo},
year={2013},
doi={10.1109/ICME.2013.6607498},
art_number={6607498},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885581272&doi=10.1109%2fICME.2013.6607498&partnerID=40&md5=f9dfeefa20d91fdbf60a940f772bd8e3},
abstract={MPEG Dynamic Adaptive Streaming over HTTP (DASH) is a new streaming standard that has been recently ratified as an international standard (IS). In comparison to other streaming systems, e.g., HTTP progressive download, DASH is able to handle varying bandwidth conditions providing smooth streaming. Furthermore, it enables NAT and Firewall traversal, flexible and scalable deployment as well as reduced infrastructure costs due to the reuse of existing Internet infrastructure components, e.g., proxies, caches, and Content Distribution Networks (CDN). Recently, the Hypertext Transfer Protocol Bis (httpbis) working group of the IETF has officially started the development of HTTP 2.0. Initially three major proposals have been submitted to the IETF i.e., Googles' SPDY, Microsofts' HTTP Speed+Mobility and Network-Friendly HTTP Upgrade, but SPDY has been chosen as working draft for HTTP 2.0. In this paper we implemented MPEG-DASH over HTTP 2.0 (i.e., SPDY), demonstrating its potential benefits and drawbacks. Moreover, several experimental evaluations have been performed that compare HTTP 2.0 with HTTP 1.1 and HTTP 1.0 in the context of DASH. In particular, the protocol overhead, the performance for different round trip times, and DASH with HTTP 2.0 in a lab test scenario has been evaluated in detail. © 2013 IEEE.},
author_keywords={Dynamic Adaptive Streaming over HTTP;  Evaluation;  HTTP 2.0;  MPEG-DASH;  SPDY},
keywords={Content distribution networks;  Dynamic Adaptive Streaming over HTTP;  Evaluation;  Experimental evaluation;  International standards;  Internet infrastructure;  Mpeg dashes;  SPDY, Exhibitions;  Hypertext systems;  Motion Picture Experts Group standards;  Proxy caches, HTTP},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ericson20131024,
author={Ericson, K. and Pallickara, S.},
title={On the performance of high dimensional data clustering and classification algorithms},
journal={Future Generation Computer Systems},
year={2013},
volume={29},
number={4},
pages={1024-1034},
doi={10.1016/j.future.2012.05.026},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863770788&doi=10.1016%2fj.future.2012.05.026&partnerID=40&md5=b93e93576811f56b7330db8ff41195ed},
abstract={There is often a need to perform machine learning tasks on voluminous amounts of data. These tasks have application in fields such as pattern recognition, data mining, bioinformatics, and recommendation systems. Here we evaluate the performance of 4 clustering algorithms and 2 classification algorithms supported by Mahout within two different cloud runtimes, Hadoop and Granules. Our benchmarks use the same Mahout backend code, ensuring a fair comparison. The differences between these implementations stem from how the Hadoop and Granules runtimes (1) support and manage the lifecycle of individual computations, and (2) how they orchestrate exchange of data between different stages of the computational pipeline during successive iterations of the clustering algorithm. We include an analysis of our results for each of these algorithms in a distributed setting, as well as a discussion on measures for failure recovery. © 2012 Elsevier B.V.},
author_keywords={Classification;  Clustering;  Distributed stream processing;  Granules;  Hadoop;  Machine learning;  Mahout},
keywords={Artificial intelligence;  Classification (of information);  Data mining;  Distributed parameter control systems;  Granulation;  Learning systems;  Pattern recognition;  Pattern recognition systems, Clustering;  Distributed stream processing;  Granules;  Hadoop;  Mahout, Clustering algorithms},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kalyvianaki2011840,
author={Kalyvianaki, E. and Wiesemann, W. and Vu, Q.H. and Kuhn, D. and Pietzuch, P.},
title={SQPR: Stream query planning with reuse},
journal={Proceedings - International Conference on Data Engineering},
year={2011},
pages={840-851},
doi={10.1109/ICDE.2011.5767851},
art_number={5767851},
note={cited By 40},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957846011&doi=10.1109%2fICDE.2011.5767851&partnerID=40&md5=c09ce050bdafdb800de2ba2375dc8cf2},
abstract={When users submit new queries to a distributed stream processing system (DSPS), a query planner must allocate physical resources, such as CPU cores, memory and network bandwidth, from a set of hosts to queries. Allocation decisions must provide the correct mix of resources required by queries, while achieving an efficient overall allocation to scale in the number of admitted queries. By exploiting overlap between queries and reusing partial results, a query planner can conserve resources but has to carry out more complex planning decisions. In this paper, we describe SQPR, a query planner that targets DSPSs in data centre environments with heterogeneous resources. SQPR models query admission, allocation and reuse as a single constrained optimisation problem and solves an approximate version to achieve scalability. It prevents individual resources from becoming bottlenecks by re-planning past allocation decisions and supports different allocation objectives. As our experimental evaluation in comparison with a state-of-the-art planner shows SQPR makes efficient resource allocation decisions, even with a high utilisation of resources, with acceptable overheads. © 2011 IEEE.},
keywords={Allocation decision;  Conserve resources;  CPU cores;  Data centres;  Efficient resource allocation;  Experimental evaluation;  Heterogeneous resources;  Network bandwidth;  Optimisations;  Physical resources;  Re-planning;  Stream processing systems, Distributed parameter control systems, Planning},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gu2006824,
author={Gu, X. and Nahrstedt, K.},
title={On composing stream applications in peer-to-peer environments},
journal={IEEE Transactions on Parallel and Distributed Systems},
year={2006},
volume={17},
number={8},
pages={824-837},
doi={10.1109/TPDS.2006.107},
note={cited By 40},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746045272&doi=10.1109%2fTPDS.2006.107&partnerID=40&md5=8364e77813dfefe415c8dc976e3836ba},
abstract={Stream processing has become increasingly important as many emerging applications call for continuous real-time processing over data streams, such as voice-over-IP telephony, security surveillance, and sensor data analysis. In this paper, we propose a composable stream processing system for cooperative peer-to-peer environments. The system can dynamically select and compose stream processing elements located on different peers into user desired applications. We investigate multiple alternative approaches to composing stream applications: 1) global-state-based centralized versus local-state-based distributed algorithms for initially composing stream applications at setup phase. The centralized algorithm performs periodical global state maintenance while the distributed algorithm performs on-demand state collection. 2) Reactive versus proactive failure recovery schemes for maintaining composed stream applications during runtime. The reactive failure recovery algorithm dynamically recomposes a new stream application upon failures while the proactive approach maintains a number of backup compositions for failure recovery. We conduct both theoretical analysis and experimental evaluations to study the properties of different approaches. Our study illustrates the performance and overhead trade-offs among different design alternatives, which can provide important guidance for selecting proper algorithms to compose stream applications in cooperative peer-to-peer environments. © 2006 IEEE.},
author_keywords={Peer-to-peer;  Quality-of-service;  Resource management;  Service composition;  Stream processing},
keywords={Algorithms;  Computer system recovery;  Quality of service;  Resource allocation, Peer-to-peer;  Resource management;  Service composition;  Stream processing, Distributed computer systems},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cardellini2018,
author={Cardellini, V. and Lo Presti, F. and Nardelli, M. and Russo Russo, G.},
title={Optimal operator deployment and replication for elastic distributed data stream processing},
journal={Concurrency Computation},
year={2018},
volume={30},
number={9},
doi={10.1002/cpe.4334},
art_number={e4334},
note={cited By 38},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031110017&doi=10.1002%2fcpe.4334&partnerID=40&md5=40db61df5d575a91c31c23ead8c1fb82},
abstract={Processing data in a timely manner, data stream processing (DSP) applications are receiving an increasing interest for building new pervasive services. Due to the unpredictability of data sources, these applications often operate in dynamic environments; therefore, they require the ability to elastically scale in response to workload variations. In this paper, we deal with a key problem for the effective runtime management of a DSP application in geo-distributed environments: We investigate the placement and replication decisions while considering the application and resource heterogeneity and the migration overhead, so to select the optimal adaptation strategy that can minimize migration costs while satisfying the application quality of service (QoS) requirements. We present elastic DSP replication and placement (EDRP), a unified framework for the QoS-aware initial deployment and runtime elasticity management of DSP applications. In EDRP, the deployment and runtime decisions are driven by the solution of a suitable integer linear programming problem, whose objective function captures the relative importance between QoS goals and reconfiguration costs. We also present the implementation of EDRP and the related mechanisms on Apache Storm. We conduct a thorough experimental evaluation, both numerical and prototype-based, that shows the benefits achieved by EDRP on the application performance. Copyright © 2017 John Wiley & Sons, Ltd.},
author_keywords={Apache Storm;  data stream processing;  deployment;  elasticity;  replication;  stateful migration},
keywords={Elasticity;  Integer programming;  Quality of service;  Storms, Application performance;  Data stream processing;  deployment;  Distributed data stream processing;  Distributed environments;  Integer Linear Programming;  replication;  stateful migration, Data streams},
publisher={John Wiley and Sons Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tournavitis2010377,
author={Tournavitis, G. and Franke, B.},
title={Semi-automatic extraction and exploitation of hierarchical pipeline parallelism using profiling information},
journal={Parallel Architectures and Compilation Techniques - Conference Proceedings, PACT},
year={2010},
pages={377-388},
doi={10.1145/1854273.1854321},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149252926&doi=10.1145%2f1854273.1854321&partnerID=40&md5=83c29cd89c2489ce182f359c3add2bfc},
abstract={In recent years multi-core computer systems have left the realm of high-performance computing and virtually all of today's desktop computers and embedded computing systems are equipped with several processing cores. Still, no single parallel programming model has found widespread support and parallel programming remains an art for the majority of application programmers. In addition, there exists a plethora of sequential legacy applications for which automatic parallelization is the only hope to benefit from the increased processing power of modern multi-core systems. In the past automatic parallelization largely focused on data parallelism. In this paper we present a novel approach to extracting and exploiting pipeline parallelism from sequential applications. We use profiling to overcome the limitations of static data and control flow analysis enabling more aggressive parallelization. Our approach is orthogonal to existing automatic parallelization approaches and additional data parallelism may be exploited in the individual pipeline stages. The key contribution of this paper is a whole-program representation that supports profiling, parallelism extraction and exploitation. We demonstrate how this enhances conventional pipeline parallelization by incorporating support for multi-level loops and pipeline stage replication in a uniform and automatic way. We have evaluated our methodology on a set of multimedia and stream processing benchmarks and demonstrate speedups of up to 4.7 on a eight-core Intel Xeon machine. © 2010 ACM.},
author_keywords={parallelization;  pipeline parallelism;  program dependence graph;  streaming applications},
keywords={Application programs;  Embedded systems;  Extraction;  Legacy systems;  Multicore programming;  Parallel architectures;  Parallel programming;  Personal computers, Automatic Parallelization;  Embedded computing system;  High performance computing;  Parallel programming model;  Parallelizations;  Pipeline parallelisms;  Program dependence graph;  Streaming applications, Pipelines},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shukla201898,
author={Shukla, A. and Simmhan, Y.},
title={Model-driven scheduling for distributed stream processing systems},
journal={Journal of Parallel and Distributed Computing},
year={2018},
volume={117},
pages={98-114},
doi={10.1016/j.jpdc.2018.02.003},
note={cited By 36},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044845984&doi=10.1016%2fj.jpdc.2018.02.003&partnerID=40&md5=d27f11d7522df19d658c50d36a7929e8},
abstract={Distributed Stream Processing Systems (DSPS) are “Fast Data” platforms that allow streaming applications to be composed and executed with low latency on commodity clusters and Clouds. Such applications are composed as a Directed Acyclic Graph (DAG) of tasks, with data parallel execution using concurrent task threads on distributed resource slots. Scheduling such DAGs for DSPS has two parts—allocation of threads and resources for a DAG, and mapping threads to resources. Existing schedulers often address just one of these, make the assumption that performance linearly scales, or use ad hoc empirical tuning at runtime. Instead, we propose model-driven techniques for both mapping and allocation that rely on low-overhead a priori performance modeling of tasks. Our scheduling algorithms are able to offer predictable and low resource needs that is suitable for elastic pay-as-you-go Cloud resources, support a high input rate through high VM utilization, and can be combined with other mapping approaches as well. These are validated for micro and application benchmarks, and compared with contemporary schedulers, for the Apache Storm DSPS. © 2018 Elsevier Inc.},
author_keywords={Big data;  Cloud computing;  Distributed systems;  Performance models;  Scheduling algorithms;  Stream processing},
keywords={Benchmarking;  Big data;  Cloud computing;  Directed graphs;  Distributed parameter control systems;  Mapping;  Scheduling;  Scheduling algorithms, Directed acyclic graph (DAG);  Distributed resources;  Distributed stream processing;  Distributed systems;  Model-driven techniques;  Performance Model;  Stream processing;  Streaming applications, Distributed computer systems},
publisher={Academic Press Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khoshkbarforoushha2017120,
author={Khoshkbarforoushha, A. and Ranjan, R. and Gaire, R. and Abbasnejad, E. and Wang, L. and Zomaya, A.Y.},
title={Distribution Based Workload Modelling of Continuous Queries in Clouds},
journal={IEEE Transactions on Emerging Topics in Computing},
year={2017},
volume={5},
number={1},
pages={120-133},
doi={10.1109/TETC.2016.2597546},
art_number={7529058},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015359525&doi=10.1109%2fTETC.2016.2597546&partnerID=40&md5=7b4928abb45f46ea65c0454f28b93593},
abstract={Resource usage estimation for managing streaming workload in emerging applications domains such as enterprise computing, smart cities, remote healthcare, and astronomy, has emerged as a challenging research problem. Such resource estimation for processing continuous queries over streaming data is challenging due to: (i) uncertain stream arrival patterns, (ii) need to process different mixes of queries, and (iii) varying resource consumption. Existing techniques approximate resource usage for a query as a single point value which may not be sufficient because it is neither expressive enough nor does it capture the aforementioned nature of streaming workload. In this paper, we present a novel approach of using mixture density networks to estimate the whole spectrum of resource usage as probability density functions. We have evaluated our technique using the linear road benchmark and TPC-H in both private and public clouds. The efficiency and applicability of the proposed approach is demonstrated via two novel applications: i) predictable auto-scaling policy setting which highlights the potential of distribution prediction in consistent definition of cloud elasticity rules; and ii) a distribution based admission controller which is able to efficiently admit or reject incoming queries based on probabilistic service level agreements compliance goals. © 2013 IEEE.},
author_keywords={continuous query;  Data stream processing workload;  distribution-based admission controller;  predictable auto-scaling policy;  resource usage estimation},
keywords={Compliance control;  Data handling;  Probability distributions;  Smart city, Admission controllers;  Continuous queries;  Data stream processing;  Emerging applications;  Enterprise computing;  Resource consumption;  Resource usage;  Service Level Agreements, Probability density function},
publisher={IEEE Computer Society},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ed-daoudy2019,
author={Ed-daoudy, A. and Maalmi, K.},
title={A new Internet of Things architecture for real-time prediction of various diseases using machine learning on big data environment},
journal={Journal of Big Data},
year={2019},
volume={6},
number={1},
doi={10.1186/s40537-019-0271-7},
art_number={104},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075625870&doi=10.1186%2fs40537-019-0271-7&partnerID=40&md5=69024b876f74819927d8418284d7709c},
abstract={A number of technologies enabled by Internet of Thing (IoT) have been used for the prevention of various chronic diseases, continuous and real-time tracking system is a particularly important one. Wearable medical devices with sensor, health cloud and mobile applications have continuously generating a huge amount of data which is often called as streaming big data. Due to the higher speed of the data generation, it is difficult to collect, process and analyze such massive data in real-time in order to perform real-time actions in case of emergencies and extracting hidden value. using traditional methods which are limited and time-consuming. Therefore, there is a significant need to real-time big data stream processing to ensure an effective and scalable solution. In order to overcome this issue, this work proposes a new architecture for real-time health status prediction and analytics system using big data technologies. The system focus on applying distributed machine learning model on streaming health data events ingested to Spark streaming through Kafka topics. Firstly, we transform the standard decision tree (DT) (C4.5) algorithm into a parallel, distributed, scalable and fast DT using Spark instead of Hadoop MapReduce which becomes limited for real-time computing. Secondly, this model is applied to streaming data coming from distributed sources of various diseases to predict health status. Based on several input attributes, the system predicts health status, send an alert message to care providers and store the details in a distributed database to perform health data analytics and stream reporting. We measure the performance of Spark DT against traditional machine learning tools including Weka. Finally, performance evaluation parameters such as throughput and execution time are calculated to show the effectiveness of the proposed architecture. The experimental results show that the proposed system is able to effectively process and predict real-time and massive amount of medical data enabled by IoT from distributed and various diseases. © 2019, The Author(s).},
author_keywords={Apache Spark;  Big data;  Distributed machine learning;  Healthcare;  Internet of Things;  Stream processing},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fortuna2010,
author={Fortuna, R. and Leonardi, E. and Mellia, M. and Meo, M. and Traverso, S.},
title={QoE in pull based P2P-TV systems: Overlay topology design tradeoffs},
journal={2010 IEEE 10th International Conference on Peer-to-Peer Computing, P2P 2010 - Proceedings},
year={2010},
doi={10.1109/P2P.2010.5569966},
art_number={5569966},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349237784&doi=10.1109%2fP2P.2010.5569966&partnerID=40&md5=76459a39e3bde370ec1711cf5cdd4093},
abstract={This paper presents a systematic performance analysis of pull P2P video streaming systems for live applications, providing guidelines for the design of the overlay topology and the chunk scheduling algorithm. The contribution of the paper is threefold: 1) we propose a realistic simulative model of the system that represents the effects of access bandwidth heterogeneity, latencies, peculiar characteristics of the video, while still guaranteeing good scalability properties; 2) we propose a new latency/bandwidth-aware overlay topology design strategy that improves application layer performance while reducing the underlying transport network stress; 3) we investigate the impact of chunk scheduling algorithms that explicitly exploit properties of encoded video. Results show that our proposal jointly improves the actual Quality of Experience of users and reduces the cost the transport network has to support. ©2010 IEEE.},
keywords={Application layers;  Bandwidth heterogeneity;  Encoded videos;  Overlay topologies;  Performance analysis;  Quality of experiences;  Simulative models;  Transport networks;  TV systems, Design;  Distributed computer systems;  Quality of service;  Scheduling algorithms;  Telecommunication networks;  Topology;  Videotex, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Luo2007497,
author={Luo, G. and Tang, C. and Yu, P.S.},
title={Resource-adaptive real-time new event detection},
journal={Proceedings of the ACM SIGMOD International Conference on Management of Data},
year={2007},
pages={497-508},
doi={10.1145/1247480.1247536},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35448956904&doi=10.1145%2f1247480.1247536&partnerID=40&md5=6ae640690d6ba566d371fd333ac13704},
abstract={In a document streaming environment, online detection of the first documents that mention previously unseen events is an open challenge. For this online new event detection (ONED) task, existing studies usually assume that enough resources are always available and focus entirely on detection accuracy without considering efficiency. Moreover, none of the existing work addresses the issue of providing an effective and friendly user interface. As a result, there is a significant gap between the existing systems and a system that can be used in practice. In this paper, we propose an ONED framework with the following prominent features. First, a combination of indexing and compression methods is used to improve the document processing rate by orders of magnitude without sacrificing much detection accuracy. Second, when resources are tight, a resource-adaptive computation method is used to maximize the benefit that can be gained from the limited resources. Third, when the new event arrival rate is beyond the processing capability of the consumer of the ONED system, new events are further filtered and prioritized before they are presented to the consumer. Fourth, implicit citation relationships are created among all the documents and used to compute the importance of document sources. This importance information can guide the selection of document sources. We implemented a prototype of our framework on top of IBM's Stream Processing Core middleware. We also evaluated the effectiveness of our techniques on the standard TDT5 benchmark. To the best of our knowledge, this is the first implementation of a real application in a large-scale stream processing system. Copyright 2007 ACM.},
author_keywords={Document streaming;  Online new event detection},
keywords={Document streaming;  Online new event detection (ONED);  Resource-adaptive real-time, Benchmarking;  Computational methods;  Data processing;  Information retrieval systems;  Middleware;  Online systems, Real time systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pechanek2000348,
author={Pechanek, G.G. and Vassiliadis, S.},
title={The ManArray™ embedded processor architecture},
journal={Conference Proceedings of the EUROMICRO},
year={2000},
volume={1},
pages={348-355},
doi={10.1109/EURMIC.2000.874652},
art_number={874652},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889241806&doi=10.1109%2fEURMIC.2000.874652&partnerID=40&md5=14b7be53f2d7d158080d86608b9084dc},
abstract={The BOPS(R) ManArray™ architecture is presented as a scalable DAP platform for the embedded processor domain. In this domain, ManArray-based processors use a single architecture definition, that supports multiple configurations of processing elements (PEs) from low end single PE to large arrays of PEs, and single tool set. The ManArray (selectable) parallelism architecture mixes control oriented operations, VLIWs, packed data operations, and distributed array processing in a cohesive, independently selectable manner. In addition, scalable conditional execution and single-cycle communications across a high connectivity, low cost network are integrated in the architecture. This allows another level of selectivity that enhances the application of the parallel resources that enhances the application of the parallel resources to high performance algorithms. Coupled with the array DSP is a scalable DMA engine that runs in the background and provides programmer-selectable data-distribution patterns and a high-bandwidth data-streaming interface to system peripherals and global memory. This paper introduces the embedded scalable ManArray architecture and a number of benchmarks. For example, a standard ASIC flow DSP/coprocessor core, the BOPS2040, can process a distributed 256-point complex FFT in 425 cycles and an 8 times; 8 2D IDCT that meets IEEE standards in 34 cycles. © 2000 IEEE.},
keywords={Conditional execution;  Distributed arrays;  Embedded processor architecture;  Embedded processors;  High connectivity;  High performance algorithms;  Multiple configurations;  Processing elements, Complex networks;  Network architecture;  Parallel processing systems, Computer architecture},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Eskandari2016,
author={Eskandari, L. and Huang, Z. and Eyers, D.},
title={P-scheduler: Adaptive hierarchical scheduling in Apache Storm},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={01-05-February-2016},
doi={10.1145/2843043.2843056},
art_number={a26},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962509965&doi=10.1145%2f2843043.2843056&partnerID=40&md5=cc594086de4f3774250e75b53c02b6dc},
abstract={With ever-accelerating data creation rates in Big Data applications, there is a need for efficient stream processing engines. Apache Storm has been of interest in both academia and industry because of its real-time, distributed, scalable and reliable framework for stream processing. In this paper, we propose an adaptive hierarchical scheduler for the Storm framework, to allocate the resources more efficiently and improve performance. In our method, we consider the data transfer rate and traffic pattern between Storm's tasks and assign highly-communicating task pairs to the same computing node by dynamically employing two phases of graph partitioning. We also calculate the number of required computing nodes in the cluster based on the overall load of the application and use this information to reduce inter-node traffic. Our performance evaluation shows a significant improvement compared to the default scheduler provided by Storm, which evenly distributes the tasks across the cluster, ignoring the communication patterns between them. © 2016 ACM.},
author_keywords={Apache Storm;  Big data;  Graph partitioning;  Scheduling;  Stream processing},
keywords={Cluster computing;  Data transfer;  Data transfer rates;  Graph theory;  Scheduling;  Storms, Adaptive hierarchical scheduling;  Big data applications;  Communicating tasks;  Communication pattern;  Graph Partitioning;  Improve performance;  Stream processing;  Stream processing engines, Big data},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cerviño2012295,
author={Cerviño, J. and Kalyvianaki, E. and Salvachúa, J. and Pietzuch, P.},
title={Adaptive provisioning of stream processing systems in the cloud},
journal={Proceedings - 2012 IEEE 28th International Conference on Data Engineering Workshops, ICDEW 2012},
year={2012},
pages={295-301},
doi={10.1109/ICDEW.2012.40},
art_number={6313696},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869033558&doi=10.1109%2fICDEW.2012.40&partnerID=40&md5=db7113a4e42e449ce593d45069cad553},
abstract={With the advent of data-intensive applications that generate large volumes of real-time data, distributed stream processing systems (DSPS) become increasingly important in domains such as social networking and web analytics. In practice, DSPSs must handle highly variable workloads caused by unpredictable changes in stream rates. Cloud computing offers an elastic infrastructure that DSPSs can use to obtain resources on-demand, but an open problem is to decide on the correct resource allocation when deploying DSPSs in the cloud. This paper proposes an adaptive approach for provisioning virtual machines (VMs) for the use of a DSPS in the cloud. We initially perform a set of benchmarks across performance metrics such as network latency and jitter to explore the feasibility of cloud-based DSPS deployments. Based on these results, we propose an algorithm for VM provisioning for DSPSs that reacts to changes in the stream workload. Through a prototype implementation on Amazon EC2, we show that our approach can achieve low-latency stream processing when VMs are not overloaded, while adjusting resources dynamically with workload changes. © 2012 IEEE.},
keywords={Adaptive approach;  Amazon ec2;  Data-intensive application;  Low-latency;  Network latencies;  Performance metrics;  Prototype implementations;  Real-time data;  Stream processing;  Stream processing systems;  Unpredictable changes;  Virtual machines, Distributed parameter control systems, Technical presentations},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cheng2013291,
author={Cheng, C. and Jiang, T. and Zhang, Q.},
title={TESLA-based homomorphic MAC for Authentication in p2p system for live streaming with network coding},
journal={IEEE Journal on Selected Areas in Communications},
year={2013},
volume={31},
number={9},
pages={291-298},
doi={10.1109/JSAC.2013.SUP.0513026},
art_number={6560074},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883364939&doi=10.1109%2fJSAC.2013.SUP.0513026&partnerID=40&md5=eed614418bf1fb0237f1be651091aad8},
abstract={Recently, the peer-to-peer (P2P) live streaming system has benefited from the advent of network coding. However, it was demonstrated that malicious nodes could significantly reduce the network throughput by launching pollution attacks or entropy attacks. In this paper, we propose an efficient symmetric-key based authentication scheme for P2P live streaming system with network coding, to provide in-network detection against pollution attacks and entropy attacks simultaneously. Since the nature of P2P live streaming requires that the detection scheme has high computation efficiency and small communication overheads, we firstly propose a homomorphic message authentication code (MAC), called as PMAC, which has small key size and low computation overhead. Then, the proposed PMAC and the delayed key disclosure technique are employed to make sure that the peers could not only detect the corrupted blocks, but also upload blocks in accordance with random linear network coding. Furthermore, the performance evaluation demonstrates that the proposed scheme has both low communication and computation overheads. © 1983-2012 IEEE.},
author_keywords={homomorphic message authentication code;  P2P living streaming;  pollution attack;  random linear network coding},
keywords={Authentication scheme;  Communication overheads;  Computation overheads;  High computation efficiency;  Message authentication codes;  Network throughput;  Pollution attack;  Random Linear Network Coding, AC motors;  Authentication;  Communication;  Distributed computer systems;  Entropy;  Linear networks;  Network coding;  Pollution detection;  Video streaming, Peer to peer networks},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chen2011509,
author={Chen, Q. and Hsu, M. and Zeller, H.},
title={Experience in Continuous analytics as a Service (CaaaS)},
journal={ACM International Conference Proceeding Series},
year={2011},
pages={509-514},
doi={10.1145/1951365.1951426},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953868814&doi=10.1145%2f1951365.1951426&partnerID=40&md5=e2590ec217ab4c638c20385e044434a1},
abstract={Mobile applications, such as those on WebOS, increasingly depend on continuous analytics results of real-time events, for monitoring oil & gas production, watching traffic status and detecting accident, etc, which has given rise to the need of providing Continuous analytics as a Service (CaaaS). While representing a paradigm shift in cloud computing, CaaaS poses several challenges in scalability, latency, time-window semantics, transaction control and result-set staging. A data stream is infinite thus can only be analyzed in granules. We propose a continuous query model over both static relations and dynamic streaming data, which allows a long-standing SQL query instance to run cycle by cycle, each cycle for a chunk of data from the data stream, using a cut-and-rewind mechanism. We further support the cycle-based transaction model with cycle-based isolation and visibility, for delivering analytics results to the clients continuously while the query is running. To have the continuously generated analytics results staged efficiently, we developed the table-ring and label switching mechanism characterized by staging data through metadata manipulation without physical data moving and copying. To scale-out analytics computation, we support both parallel database based and network distributed Map-Reduce based infrastructure with multiple cooperating engines. We have built the proposed infrastructure by extending the PostgreSQL engine. We tested the throughput and latency of this service based on a well-known stream processing benchmark; the results show that the proposed approach is highly competitive. Our experiments indicate that the database technology can be extended and applied to real-time continuous analytics service provisioning.},
author_keywords={Cloud service;  Continuous query;  Stream analytics},
keywords={Cloud services;  Continuous queries;  Data stream;  Database technology;  Gas productions;  Label switching;  Map-reduce;  Mobile applications;  Paradigm shifts;  Parallel Database;  Physical data;  PostgreSQL;  Service provisioning;  Service-based;  SQL query;  Stream analytics;  Stream processing;  Streaming data;  Time event;  Time windows;  Transaction model, Cloud computing;  Data communication systems;  Distributed database systems;  Hydraulics;  Metadata;  Semantics;  Wireless sensor networks, Technology},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Thomos2015893,
author={Thomos, N. and Kurdoglu, E. and Frossard, P. and Van Der Schaar, M.},
title={Adaptive prioritized random linear coding and scheduling for layered data delivery from multiple servers},
journal={IEEE Transactions on Multimedia},
year={2015},
volume={17},
number={6},
pages={893-906},
doi={10.1109/TMM.2015.2425228},
art_number={7091934},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929315113&doi=10.1109%2fTMM.2015.2425228&partnerID=40&md5=dfd3f01e549ae67fe471b6c550330883},
abstract={In this paper, we deal with the problem of jointly determining the optimal coding strategy and the scheduling decisions when receivers obtain layered data from multiple servers. The layered data is encoded by means of prioritized random linear coding (PRLC) in order to be resilient to channel loss while respecting the unequal levels of importance in the data, and data blocks are transmitted simultaneously in order to reduce decoding delays and improve the delivery performance. We formulate the optimal coding and scheduling decisions problem in our novel framework with the help of Markov decision processes (MDP), which are effective tools for modeling adapting streaming systems. Reinforcement learning approaches are then proposed to derive reduced computational complexity solutions to the adaptive coding and scheduling problems. The novel reinforcement learning approaches and the MDP solution are examined in an illustrative example for scalable video transmission. Our methods offer large performance gains over competing methods that deliver the data blocks sequentially. The experimental evaluation also shows that our novel algorithms offer continuous playback and guarantee small quality variations which is not the case for baseline solutions. Finally, our work highlights the advantages of reinforcement learning algorithms to forecast the temporal evolution of data demands and to decide the optimal coding and scheduling decisions. © 2015 IEEE.},
author_keywords={Layered data;  Markov decision processes (MDP);  prioritized random linear codes (PRLC);  Q-learning;  rateless codes;  virtual experience},
keywords={Algorithms;  Codes (symbols);  Image communication systems;  Learning algorithms;  Markov processes;  Reinforcement learning;  Scalable video coding, Layered data;  Markov Decision Processes;  Q-learning;  Random linear codes;  Rateless codes;  virtual experience, Scheduling},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hazra2012,
author={Hazra, J. and Reddi, R.K. and Das, K. and Seetharam, D.P. and Sinha, A.K.},
title={Power grid transient stability prediction using wide area synchrophasor measurements},
journal={IEEE PES Innovative Smart Grid Technologies Conference Europe},
year={2012},
doi={10.1109/ISGTEurope.2012.6465752},
art_number={6465752},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874701422&doi=10.1109%2fISGTEurope.2012.6465752&partnerID=40&md5=237b8027b41c83dcd925d3506588c366},
abstract={Electric power systems are prone to various kinds of transient disturbances which exist only for a fraction of second and often trigger cascading failures. Hence it is important to detect and prevent them from spreading in time. Conventionally these events are prevented by deploying costly special protection systems (SPS). Unfortunately, in many cases SPSs mis-operate as they could not predict the stability well ahead and are designed to operate based on past experiences and extensive off-line simulations. This paper proposes an online transient stability prediction scheme based on live synchrophasor data. The novelty of the proposed method is that it accurately predicts the transient stability based on only few (10 to 12) sample fault data without solving computationally extensive electromechanical dynamics. Synchrophasor data from geographically distributed Phasor Measurement Units (PMUs) are collected, synchronized, aggregated (if required) and analyzed on a stream computing platform to predict the trajectories of the generators which are then used to predict the transient stability of the grid. Performance of the proposed scheme is evaluated on the benchmark systems and evaluation results are presented in this paper. © 2012 IEEE.},
author_keywords={PMU;  Stream computing;  Synchrophasors;  Transient Stability},
keywords={Benchmark system;  Cascading failures;  Distributed phasor measurement units;  Electro mechanical dynamics;  Evaluation results;  Fault data;  Off-line simulations;  PMU;  Power grids;  Special protection system;  Stream computing;  Synchro-phasors;  Synchrophasor datum;  Synchrophasor measurements;  Transient disturbances;  Transient stability prediction;  Wide area, Benchmarking;  Electric equipment protection;  Forecasting;  Smart power grids;  Stability;  Transients, Phasor measurement units},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bourennane2012147,
author={Bourennane, S. and Fossati, C.},
title={Comparison of shape descriptors for hand posture recognition in video},
journal={Signal, Image and Video Processing},
year={2012},
volume={6},
number={1},
pages={147-157},
doi={10.1007/s11760-010-0176-6},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857443977&doi=10.1007%2fs11760-010-0176-6&partnerID=40&md5=fe382745f643f7700a5dbc6772af6341},
abstract={Hand posture recognition remains a challenging task for in-line systems working directly in the video stream. In this work, we compare several shape descriptors, with the objective of finding a good compromise between accuracy of recognition and computation load for a real-time application. Experiments are run on two families of contour-based Fourier descriptors and two sets of region-based moments, all of them are invariant to translation, rotation and scale changes of hands. These methods are independent of the camera view point. Systematic tests are performed on the Triesch benchmark database and on our own large database, which includes more realistic conditions. Temporal filtering and a method for unknown posture detection are considered to improve posture recognition results in case of video stream processing. © 2010 Springer-Verlag London Limited.},
author_keywords={Hand gesture recognition;  Pattern recognition;  Shape descriptors;  Video stream},
keywords={Benchmark database;  Camera view;  Computation loads;  Fourier descriptors;  Hand posture recognition;  Hand-gesture recognition;  In-line systems;  Large database;  Posture detection;  Posture recognition;  Real-time application;  Realistic conditions;  Region-based;  Shape descriptors;  Systematic test;  Temporal filtering;  Video streams, Pattern recognition;  Video streaming, Gesture recognition},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Carra20071667,
author={Carra, D. and Lo Cigno, R. and Biersack, E.W.},
title={Graph based analysis of mesh overlay streaming systems},
journal={IEEE Journal on Selected Areas in Communications},
year={2007},
volume={25},
number={9},
pages={1667-1677},
doi={10.1109/JSAC.2007.071206},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-36749035474&doi=10.1109%2fJSAC.2007.071206&partnerID=40&md5=729a83f44e433f78b3130fc228fed41c},
abstract={This paper studies fundamental properties of stream-based content distribution services. We assume the presence of an overlay network (such as those built by P2P systems) with limited degree of connectivity, and we develop a mathematical model that captures the essential features of overlay-based streaming protocols and systems. The methodology is based on stochastic graph theory, and models the streaming system as a stochastic process, whose characteristics are related to the streaming protocol. The model captures the elementary properties of the streaming system such as the number of active connections, the different play-out delay of nodes, and the probability of not receiving the stream due to node failures/misbehavior. Besides the static properties, the model is able to capture the transient behavior of the distribution graphs, i.e., the evolution of the structure over time, for instance in the initial phase of the distribution process. Contributions of this paper include a detailed definition of the methodology, its comparison with other analytical approaches and with simulative results, and a discussion of the additional insights enabled by this methodology. Results show that mesh based architectures are able to provide bounds on the receiving delay and maintain rate fluctuations due to system dynamics very low. Additionally, given the tight relationship between the stochastic process and the properties of the distribution protocol, this methodology gives basic guidelines for the design of such protocols and systems. © 2007 IEEE.},
author_keywords={Graph-based protocols;  Modeling;  Peer-to-peer;  Performance analysis;  Simulation;  Stochastic processes;  Streaming},
keywords={Graph based protocols;  Overlay networks;  Performance analysis;  Streaming protocols, Computer simulation;  Distributed computer systems;  Graph theory;  Mathematical models;  Network protocols;  Random processes, Information services},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Riabov200631,
author={Riabov, A. and Liu, Z.},
title={Scalable planning for distributed stream processing systems},
journal={ICAPS 2006 - Proceedings, Sixteenth International Conference on Automated Planning and Scheduling},
year={2006},
volume={2006},
pages={31-40},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746059382&partnerID=40&md5=d46d7f1426ede80f1e4a97fb6df7fe74},
abstract={Recently the problem of automatic composition of workflows has been receiving increasing interest. Initial investigation has shown that designing a practical and scalable composition algorithm for this problem is hard. A very general computational model of a workflow (e.g., BPEL) can be Turing-complete, which precludes fully automatic analysis of compositions. However, in many applications, workflow model can be simplified. We consider a model known as the Stream Processing Planning Language (SPPL), applicable in stream processing and other related domains. SPPL replaces the notion of concurrency by timeless functional computation. In addition, SPPL defines workflow metrics of resource consumption and quality of service. Experiments have shown earlier that even a naïve SPPL planning algorithm significantly outperforms existing metric PDDL planners on stream processing workflow composition problems. In this paper we describe an efficient and scalable algorithm for finding high-quality approximate solutions for large instances of SPPL problems. We demonstrate the scalability of the algorithm on synthetic benchmarks that are derived from practical problems. We also give an example of SPPL model for practical problems. Copyright © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.},
keywords={Adaptive algorithms;  Automation;  Computer aided design;  Distributed computer systems;  Problem solving;  Turing machines, Composition algorithm;  Computational model;  Stream Processing Planning Language (SPPL);  Workflow models, Industrial applications},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{VanDongen20201845,
author={Van Dongen, G. and Van Den Poel, D.},
title={Evaluation of Stream Processing Frameworks},
journal={IEEE Transactions on Parallel and Distributed Systems},
year={2020},
volume={31},
number={8},
pages={1845-1858},
doi={10.1109/TPDS.2020.2978480},
art_number={9025240},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082884278&doi=10.1109%2fTPDS.2020.2978480&partnerID=40&md5=26b022034b6e88f7451b2a6f5952b215},
abstract={The increasing need for real-time insights in data sparked the development of multiple stream processing frameworks. Several benchmarking studies were conducted in an effort to form guidelines for identifying the most appropriate framework for a use case. In this article, we extend this research and present the results gathered. In addition to Spark Streaming and Flink, we also include the emerging frameworks Structured Streaming and Kafka Streams. We define four workloads with custom parameter tuning. Each of these is optimized for a certain metric or for measuring performance under specific scenarios such as bursty workloads. We analyze the relationship between latency, throughput, and resource consumption and we measure the performance impact of adding different common operations to the pipeline. To ensure correct latency measurements, we use a single Kafka broker. Our results show that the latency disadvantages of using a micro-batch system are most apparent for stateless operations. With more complex pipelines, customized implementations can give event-driven frameworks a large latency advantage. Due to its micro-batch architecture, Structured Streaming can handle very high throughput at the cost of high latency. Under tight latency SLAs, Flink sustains the highest throughput. Additionally, Flink shows the least performance degradation when confronted with periodic bursts of data. When a burst of data needs to be processed right after startup, however, micro-batch systems catch up faster while event-driven systems output the first events sooner. © 1990-2012 IEEE.},
author_keywords={apache flink;  apache kafka;  Apache spark;  benchmarking;  big data;  distributed computing;  kafka streams;  stream processing frameworks;  structured streaming},
keywords={Benchmarking;  Big data;  Distributed computer systems;  Pipelines, apache flink;  apache kafka;  kafka streams;  Latency measurements;  Measuring performance;  Performance degradation;  Resource consumption;  Stream processing, Data streams},
publisher={IEEE Computer Society},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gupta2018148,
author={Gupta, H. and Ramachandran, U.},
title={FogStore: A geo-distributed key-value store guaranteeing low latency for strongly consistent access},
journal={DEBS 2018 - Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
year={2018},
pages={148-159},
doi={10.1145/3210284.3210297},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050559351&doi=10.1145%2f3210284.3210297&partnerID=40&md5=e9a99b70f9dce6a20bacebd1f9e03de9},
abstract={We design Fogstore, a key-value store for event-based systems, that exploits the concept of relevance to guarantee low-latency access to relevant data with strong consistency guarantees, while providing tolerance from geographically correlated failures. Distributed event-based processing pipelines are envisioned to utilize the resources of densely geo-distributed infrastructures for low-latency responses - enabling real-time applications. Increasing complexity of such applications results in higher dependence on state, which has driven the incorporation of state-management as a core functionality of contemporary stream processing engines a la Apache Flink and Samza. Processing components executing under the same context (like location) often produce information that may be relevant to others, thereby necessitating shared state and an out-of-band globally-accessible data-store. Efficient access to application state is critical for overall performance, thus centralized data-stores are not a viable option due to the high-latency of network traversals. On the other hand, a highly geo-distributed datastore with low-latency implemented with current key-value stores would necessitate degrading client expectation of consistency as per the PACELC theorem. In this paper we exploit the notion of contextual relevance of events (data) in situation-awareness applications - and offer differential consistency guarantees for clients based on their context. We highlight important systems concerns that may arise with a highly geo-distributed system and show how Fogstore's design tackles them. We present, in detail, a prototype implementation of Fogstore's mechanisms on Apache Cassandra and a performance evaluation. Our evaluations show that Fogstore is able to achieve the throughput of eventually consistent configurations while serving data with strong consistency to the contextually relevant clients. © 2018 Copyright held by the owner/author(s).},
author_keywords={Context awareness;  Distributed key-value stores;  Edge computing;  Latency-consistency trade-off},
keywords={Economic and social effects;  Edge computing;  Software architecture, Context- awareness;  Distributed infrastructure;  Distributed key-value stores;  Geographically correlated failures;  Performance evaluations;  Prototype implementations;  Stream processing engines;  Trade off, Pipeline processing systems},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2017659,
author={Zhang, S. and He, B. and Dahlmeier, D. and Zhou, A.C. and Heinze, T.},
title={Revisiting the design of data stream processing systems on multi-core processors},
journal={Proceedings - International Conference on Data Engineering},
year={2017},
pages={659-670},
doi={10.1109/ICDE.2017.119},
art_number={7930015},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021235248&doi=10.1109%2fICDE.2017.119&partnerID=40&md5=1ebe2cf3ddc8ce34624138b04f812ef2},
abstract={Driven by the rapidly increasing demand for handling real-Time data streams, many data stream processing (DSP) systems have been proposed. Regardless of the different architectures of those DSP systems, they are mostly aiming at scaling out using a cluster of commodity machines and built around a number of key design aspects: A) pipelined processing with message passing, b) on-demand data parallelism, and c) JVM based implementation. However, there lacks a study on those key design aspects on modern scale-up architectures, where more CPU cores are being put on the same die, and the onchip cache hierarchies are getting larger, deeper, and complex. Multiple sockets bring non-uniform memory access (NUMA) effort. In this paper, we revisit the aforementioned design aspects on a modern scale-up server. Specifically, we use a series of applications as micro benchmark to conduct detailed profiling studies on Apache Storm and Flink. From the profiling results, we observe two major performance issues: A) the massively parallel execution model causes serious front-end stalls, which are a major performance bottleneck issue on a single CPU socket, b) the lack of NUMA-Aware mechanism causes major drawback on the scalability of DSP systems on multi-socket architectures. Addressing these issues should allow DSP systems to exploit modern scale-up architectures, which also benefits scaling out environments. We present our initial efforts on resolving the above-mentioned performance issues, which have shown up to 3.2x and 3.1x improvement on the performance of Storm and Flink, respectively. © 2017 IEEE.},
keywords={Benchmarking;  Data communication systems;  Data handling;  Integrated circuit design;  Memory architecture;  Message passing;  Pipeline processing systems;  Real time systems;  Storms, Data stream processing;  Massively parallels;  Multi-core processor;  Non uniform memory access;  Performance bottlenecks;  Performance issues;  Pipelined processing;  Real-time data streams, Computer architecture},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tang20132344,
author={Tang, Y. and Gedik, B.},
title={Autopipelining for data stream processing},
journal={IEEE Transactions on Parallel and Distributed Systems},
year={2013},
volume={24},
number={12},
pages={2344-2354},
doi={10.1109/TPDS.2012.333},
art_number={6381402},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887883611&doi=10.1109%2fTPDS.2012.333&partnerID=40&md5=9ceb65d32607174a20af25c1ab236794},
abstract={Stream processing applications use online analytics to ingest high-rate data sources, process them on-the-fly, and generate live results in a timely manner. The data flow graph representation of these applications facilitates the specification of stream computing tasks with ease, and also lends itself to possible runtime exploitation of parallelization on multicore processors. While the data flow graphs naturally contain a rich set of parallelization opportunities, exploiting them is challenging due to the combinatorial number of possible configurations. Furthermore, the best configuration is dynamic in nature; it can differ across multiple runs of the application, and even during different phases of the same run. In this paper, we propose an autopipelining solution that can take advantage of multicore processors to improve throughput of streaming applications, in an effective and transparent way. The solution is effective in the sense that it provides good utilization of resources by dynamically finding and exploiting sources of pipeline parallelism in streaming applications. It is transparent in the sense that it does not require any hints from the application developers. As a part of our solution, we describe a light-weight runtime profiling scheme to learn resource usage of operators comprising the application, an optimization algorithm to locate best places in the data flow graph to explore additional parallelism, and an adaptive control scheme to find the right level of parallelism. We have implemented our solution in an industrial-strength stream processing system. Our experimental evaluation based on microbenchmarks, synthetic workloads, as well as real-world applications confirms that our design is effective in optimizing the throughput of stream processing applications without requiring any changes to the application code. © 1990-2012 IEEE.},
author_keywords={Autopipelining;  Parallelization;  Stream processing},
keywords={Adaptive control schemes;  Autopipelining;  Experimental evaluation;  Optimization algorithms;  Parallelizations;  Stream processing;  Stream processing systems;  Utilization of resources, Algorithms;  Data flow analysis;  Data flow graphs;  Graphic methods, Parallel processing systems},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kofler200863,
author={Kofler, I. and Prangl, M. and Kuschnig, R. and Hellwagner, H.},
title={An H.264/SVC-based adaptation proxy on a WiFi router},
journal={Proceedings of the International Workshop on Network and Operating System Support for Digital Audio and Video},
year={2008},
pages={63-68},
doi={10.1145/1496046.1496061},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-63749098890&doi=10.1145%2f1496046.1496061&partnerID=40&md5=1889f81b82d5275537401a78a6e78a4b},
abstract={Recent advances in video coding technology like the scalable extension of the MPEG-4 AVC/H.264 video coding standard pave the way for computationally cheap adaptation of video content. In this paper we present our work on a lightweight RTSP/RTP proxy that enables in-network stream processing. Based on an off-the-shelf wireless router that runs a Linux-based firmware we demonstrate that the video adaptation can be performed on-the-fly directly on a network device. The paper covers design and implementation details of the proxy as well as a discussion about the actual adaptation of the SVC stream. Based on experimental evaluations we show that our approach can handle a reasonable number of concurrent sessions for a typical home deployment scenario. Furthermore, the paper covers possible applications in which adaptation on the network device can be beneficial. © 2008 ACM.},
author_keywords={H.264;  In-network adaptation;  Multimedia adaptation;  RTP;  RTSP;  scalable video coding},
keywords={H.264;  In-network adaptation;  Multimedia adaptation;  RTP;  RTSP;  scalable video coding, Audio systems;  Computer operating systems;  Electric network topology;  Embedded systems;  Firmware;  Image coding;  Internet protocols;  Motion Picture Experts Group standards;  Rapid thermal annealing;  Routers;  Visual communication;  Wireless networks, Computer graphics},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jonathan2018412,
author={Jonathan, A. and Chandra, A. and Weissman, J.},
title={Multi-query optimization in wide-area streaming analytics},
journal={SoCC 2018 - Proceedings of the 2018 ACM Symposium on Cloud Computing},
year={2018},
pages={412-425},
doi={10.1145/3267809.3267842},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059012622&doi=10.1145%2f3267809.3267842&partnerID=40&md5=91ce0deb2a1abc54a5f3b146348d3581},
abstract={Wide-area data analytics has gained much attention in recent years due to the increasing need for analyzing data that are geographically distributed. Many of such queries often require real-time analysis on data streams that are continuously being generated across multiple locations. Yet, analyzing these geo-distributed data streams in a timely manner is very challenging due to the highly heterogeneous and limited bandwidth availability of the wide-area network (WAN). This paper examines the opportunity of applying multi-query optimization in the context of wide-area streaming analytics, with the goal of utilizing WAN bandwidth efficiently while achieving high throughput and low latency execution. Our approach is based on the insight that many streaming analytics queries often exhibit common executions, whether in consuming a common set of input data or performing the same data processing. In this work, we study different types of sharing opportunities and propose a practical online algorithm that allows streaming analytics queries to share their common executions incrementally. We further address the importance of WAN awareness in applying multi-query optimization. Without WAN awareness, sharing executions in a wide-area environment may lead to performance degradation. We have implemented our WAN-aware multi-query optimization in a prototype implementation based on Apache Flink. Experimental evaluation using Twitter traces on a real wide-area system deployment across geo-distributed EC2 data centers shows that our technique is able to achieve 21% higher throughput while saving WAN bandwidth consumption by 33% compared to a WAN-aware, sharing-agnostic system. © 2018 Association for Computing Machinery.},
author_keywords={Execution sharing;  Geo-distributed systems;  Multi-query optimization;  Stream processing systems},
keywords={Bandwidth;  Cloud computing;  Data handling;  Data mining, Distributed data streams;  Distributed systems;  Execution sharing;  Experimental evaluation;  Multiquery optimization;  Performance degradation;  Prototype implementations;  Stream processing systems, Wide area networks},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zacheilas2017112,
author={Zacheilas, N. and Kalogeraki, V. and Nikolakopoulos, Y. and Gulisano, V. and Papatriantafilou, M. and Tsigas, P.},
title={Maximizing determinism in stream processing under latency constraints},
journal={DEBS 2017 - Proceedings of the 11th ACM International Conference on Distributed Event-Based Systems},
year={2017},
pages={112-123},
doi={10.1145/3093742.3093921},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023164307&doi=10.1145%2f3093742.3093921&partnerID=40&md5=b83b44c3db713af1404fa8d6e10c7b3c},
abstract={The problem of coping with the demands of determinism and meeting latency constraints is challenging in distributed data stream processing systems that have to process high volume data streams that arrive from different unsynchronized input sources. In order to deterministically process the streaming data, they need mechanisms that synchronize the order in which tuples are processed by the operators. On the other hand, achieving real-time response in such a system requires careful tradeoff between determinism and low latency performance. We build on a recently proposed approach to handle data exchange and synchronization in stream processing, namely ScaleGate, which comes with guarantees for determinism and an efficient lock-free implementation, enabling high scalability. Considering the challenge and trade-offs implied by real-time constraints, we propose a system which comprises (a) a novel data structure called Slack-ScaleGate (SSG), along with its algorithmic implementation; SSG enables us to guarantee the deterministic processing of tuples as long as they are able to meet their latency constraints, and (b) a method to dynamically tune the maximum amount of time that a tuple can wait in the SSG data-structure, relaxing the determinism guarantees when needed, in order to satisfy the latency constraints. Our detailed experimental evaluation using a traffic monitoring application deployed in the city of Dublin, illustrates the working and benefits of our approach. © 2017 ACM.},
author_keywords={Complex event processing;  Deterministic processing;  Stream processing},
keywords={Data communication systems;  Electronic data interchange;  Software architecture, Complex event processing;  Distributed data stream processing;  Experimental evaluation;  Latency constraints;  Real time constraints;  Real time response;  Stream processing;  Traffic monitoring, Data handling},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2013351,
author={Liu, F. and Li, B. and Li, B. and Jin, H.},
title={Peer-assisted on-demand streaming: Characterizing demands and optimizing supplies},
journal={IEEE Transactions on Computers},
year={2013},
volume={62},
number={2},
pages={351-361},
doi={10.1109/TC.2011.222},
art_number={6081856},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871654070&doi=10.1109%2fTC.2011.222&partnerID=40&md5=62c980ad5e3cd0244150904c8a1c19de},
abstract={Nowadays, there has been significant deployment of peer-assisted on-demand streaming services over the Internet. Two of the most unique and salient features in a peer-assisted on-demand streaming system are the differentiation in the demand (or request) and the prefetching capability with caching. In this paper, we develop a theoretical framework based on queuing models, in order to 1) justify the superiority of service prioritization based on a taxonomy of requests, and 2) understand the fundamental principles behind optimal prefetching and caching designs in peer-assisted on-demand streaming systems. The focus is to instruct how limited uploading bandwidth resources and peer caching capacities can be utilized most efficiently to achieve better system performance. To achieve these objectives, we first use priority queuing analysis to prove how service quality and user experience can be statistically guaranteed, by prioritizing requests in the order of significance, including urgent playback (e.g., random seeks or initial startup), normal playback, and prefetching. We then proceed to construct a fine-grained stochastic supply-demand model to investigate peer caching and prefetching as a global optimization problem. This not only provides insights in understanding the fundamental characterization of demand, but also offers guidelines toward optimal prefetching and caching strategies in peer-assisted on-demand streaming systems. © 1968-2012 IEEE.},
author_keywords={On-demand video streaming;  peer-to-peer;  performance evaluation;  queuing model},
keywords={Bandwidth resource;  Caching and prefetching;  Caching strategy;  Fundamental principles;  Global optimization problems;  On-demand streaming;  On-demand video streaming;  Peer to peer;  Performance evaluation;  Prefetching;  Prioritization;  Priority queuing;  Queuing models;  Salient features;  Service Quality;  Supply-demand;  Theoretical framework;  User experience, Optimization;  Queueing networks;  Stochastic models;  Video streaming, Distributed computer systems},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nguyen2011,
author={Nguyen, H. and Tan, Y. and Gu, X.},
title={PAL: Propagation-aware Anomaly Localization for cloud hosted distributed applications},
journal={Managing Large-Scale Systems via the Analysis of System Logs and the Application of Machine Learning Techniques, SLAML'11},
year={2011},
doi={10.1145/2038633.2038634},
art_number={1},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81055138266&doi=10.1145%2f2038633.2038634&partnerID=40&md5=53b0c76c219236f2c5cb6be4a7f19d3b},
abstract={Distributed applications running inside cloud are prone to performance anomalies due to various reasons such as insufficient resource allocations, unexpected workload increases, or software bugs. However, those applications often consist of multiple interacting components where one component anomaly may cause its dependent components to exhibit anomalous behavior as well. It is challenging to identify the faulty components among numerous distributed application components. In this paper, we present a Propagation-aware Anomaly Localization (PAL) system that can pinpoint the source faulty components in distributed applications by extracting anomaly propagation patterns. PAL provides a robust critical change point discovery algorithm to accurately capture the onset of anomaly symptoms at different application components. We then derive the propagation pattern by sorting all critical change points in chronological order. PAL is completely application-agnostic and non-intrusive, which only relies on system-level metrics. We have implemented PAL on top of the Xen platform and tested it on a production cloud computing infrastructure using the RUBiS online auction benchmark application and the IBM System S data streaming processing application with a range of common software bugs. Our experimental results show that PAL can pinpoint faulty components in distributed applications with high accuracy and low overhead. © 2011 ACM.},
author_keywords={Anomaly propagation;  Cloud computing;  Fault Localization},
keywords={Anomalous behavior;  Anomaly propagation;  Application components;  Benchmark applications;  Change-points;  Chronological order;  Common software;  Computing infrastructures;  Data streaming;  Discovery algorithm;  Distributed applications;  Fault localization;  Low overhead;  Non-intrusive;  Online auctions;  Performance anomaly;  Propagation pattern;  Software bug;  System levels, Cloud computing;  Data processing;  Data reduction;  Large scale systems;  Learning algorithms;  Learning systems;  Program debugging;  Software testing, Benchmarking},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Qian2016592,
author={Qian, S. and Wu, G. and Huang, J. and Das, T.},
title={Benchmarking modern distributed streaming platforms},
journal={Proceedings of the IEEE International Conference on Industrial Technology},
year={2016},
volume={2016-May},
pages={592-598},
doi={10.1109/ICIT.2016.7474816},
art_number={7474816},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974624908&doi=10.1109%2fICIT.2016.7474816&partnerID=40&md5=232f43e4770b94ecf4250af918ba4acc},
abstract={The prevalence of big data technology has generated increasing demands in large-scale streaming data processing. However, for certain tasks it is still challenging to appropriately select a platform due to the diversity of choices and the complexity of configurations. This paper focuses on benchmarking some principal streaming platforms. We achieve our goals on StreamBench, a streaming benchmark tool based on which we introduce proper modifications and extensions. We then accomplish performance comparisons among different big data platforms, including Apache Spark, Apache Storm and Apache Samza. In terms of performance criteria, we consider both computational capability and fault-tolerance ability. Finally, we give a summary on some key knobs for performance tuning as well as on hardware utilization. © 2016 IEEE.},
author_keywords={benchmark;  big data;  distributed streaming computing;  spark streaming;  storm},
keywords={Benchmarking;  Data handling;  Fault tolerance;  Storms, Computational capability;  Data technologies;  Distributed streaming;  Hardware utilization;  Large-scale streaming;  Performance comparison;  Performance criterion;  Performance tuning, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tudoran201423,
author={Tudoran, R. and Nano, O. and Santos, I. and Costan, A. and Soncu, H. and Bougé, L. and Antoniu, G.},
title={JetStream: Enabling high performance event streaming across cloud data-centers},
journal={DEBS 2014 - Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems},
year={2014},
pages={23-34},
doi={10.1145/2611286.2611298},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903161711&doi=10.1145%2f2611286.2611298&partnerID=40&md5=55fc44ba661d8dd6140e5235ad38ab97},
abstract={The easily-accessible computation power offered by cloud infrastructures coupled with the revolution of Big Data are expanding the scale and speed at which data analysis is performed. In their quest for finding the Value in the 3 Vs of Big Data, applications process larger data sets, within and across clouds. Enabling fast data transfers across geographically distributed sites becomes particularly important for applications which manage continuous streams of events in real time. Scientific applications (e.g. the Ocean Observatory Initiative or the ATLAS experiment) as well as commercial ones (e.g. Microsoft's Bing and Office 365 large-scale services) operate on tens of data-centers around the globe and follow similar patterns: they aggregate monitoring data, assess the QoS or run global data mining queries based on inter site event stream processing. In this paper, we propose a set of strategies for efficient transfers of events between cloud data-centers and we introduce JetStream: a prototype implementing these strategies as a high performance batch-based streaming middleware. JetStream is able to self-adapt to the streaming conditions by modeling and monitoring a set of context parameters. It further aggregates the available bandwidth by enabling multi-route streaming across cloud sites. The prototype was validated on tens of nodes from US and Europe data-centers of the Windows Azure cloud using synthetic benchmarks and with application code from the context of the Alice experiment at CERN. The results show an increase in transfer rate of 250 times over individual event streaming. Besides, introducing an adaptive transfer strategy brings an additional 25% gain. Finally, the transfer rate can further be tripled thanks to the use of multi-route streaming. © 2014 ACM.},
author_keywords={cloud computing;  event streaming;  high performance data management;  multi data-centers},
keywords={Aggregates;  Benchmarking;  Cloud computing;  Data transfer;  Experiments;  Information management;  Middleware;  Software architecture;  Windows operating system, Available bandwidth;  Cloud infrastructures;  Data centers;  Event stream processing;  Ocean observatories;  Performance data;  Scientific applications;  Transfer strategies, Big data},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zinn2010447,
author={Zinn, D. and Bowers, S. and Köhler, S. and Ludäscher, B.},
title={Parallelizing XML data-streaming workflows via MapReduce},
journal={Journal of Computer and System Sciences},
year={2010},
volume={76},
number={6},
pages={447-463},
doi={10.1016/j.jcss.2009.11.006},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955309365&doi=10.1016%2fj.jcss.2009.11.006&partnerID=40&md5=52ca4f73c892ed15f45d033e34b08473},
abstract={In prior work it has been shown that the design of scientific workflows can benefit from a collection-oriented modeling paradigm which views scientific workflows as pipelines of XML stream processors. In this paper, we present approaches for exploiting data parallelism in XML processing pipelines through novel compilation strategies to the MapReduce framework. Pipelines in our approach consist of sequences of processing steps that receive XML-structured data and produce, often through calls to "black-box" (scientific) functions, modified (i.e., updated) XML structures. Our main contributions are (i) the development of a set of strategies for compiling scientific workflows, modeled as XML processing pipelines, into parallel MapReduce networks, and (ii) a discussion of their advantages and trade-offs, based on a thorough experimental evaluation of the various translation strategies. Our evaluation uses the Hadoop MapReduce system as an implementation platform. Our results show that execution times of XML workflow pipelines can be significantly reduced using our compilation strategies. These efficiency gains, together with the benefits of MapReduce (e.g., fault tolerance) make our approach ideal for executing large-scale, compute-intensive XML-based scientific workflows. © 2009 Elsevier Inc. All right reserved.},
author_keywords={Collection-Oriented Modeling and Design (COMAD);  Data stream processing;  Grouping;  MapReduce;  Parallelization;  Static analysis;  Virtual Data Assembly Line (VDAL);  XML processing pipelines},
keywords={Data streams;  Economic and social effects;  Fault tolerance;  Pipelines;  Static analysis;  XML, Assembly line;  Data stream processing;  Grouping;  Map-reduce;  Parallelizations;  XML processing, Pipeline processing systems},
publisher={Academic Press Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fang2007232,
author={Fang, Z. and Zhang, L. and Carter, J.B. and Ibrahim, A. and Parker, M.A.},
title={Active memory operations},
journal={Proceedings of the International Conference on Supercomputing},
year={2007},
pages={232-241},
doi={10.1145/1274971.1275004},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548057221&doi=10.1145%2f1274971.1275004&partnerID=40&md5=2153c240c7cc178065183a2adddad885},
abstract={The performance of modern microprocessors is increasingly limited by their inability to hide main memory latency. The problem is worse in large-scale shared memory systems, where remote memory latencies are hundreds, and soon thousands, of processor cycles. To mitigate this problem, we propose the use of Active Memory Operations (AMOs), in which select operations can be sent to and executed on the home memory controller of data. AMOs can eliminate significant number of coherence messages, minimize intranode and internode memory traffic, and create opportunities for parallelism. Our implementation of AMOs is cache-coherent and requires no changes to the processor core or DRAM chips. In this paper we present architectural and programming models for AMOs, and compare its performance to that of several other memory architectures on a variety of scientific and commercial benchmarks. Through simulation we show that AMOs offer dramatic performance improvements for an important set of data-intensive operations, e.g., up to 50X faster barriers, 12X faster spinlocks, 8.5X-15X faster stream/array operations, and 3X faster database queries. Based on a standard cell implementation, we predict that the circuitry required to support AMOs is less than 1% of the typical chip area of a high performance microprocessor. Copyright 2007 ACM.},
author_keywords={Cache coherence;  Distributed shared memory;  DRAM;  Memory performance;  Stream processing;  Thread synchronization},
keywords={Cache memory;  Dynamic random access storage;  Optimization;  Problem solving;  Storage allocation (computer);  Synchronization, Active Memory Operations (AMO);  Memory performance;  Stream processing;  Thread synchronization, Microprocessor chips},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2011245,
author={Zhang, Y. and Mueller, F.},
title={GStream: A general-purpose data streaming framework on GPU clusters},
journal={Proceedings of the International Conference on Parallel Processing},
year={2011},
pages={245-254},
doi={10.1109/ICPP.2011.22},
art_number={6047193},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155214412&doi=10.1109%2fICPP.2011.22&partnerID=40&md5=66b55f04f10933913a5b3e249355c8c9},
abstract={Emerging accelerating architectures, such as GPUs, have proved successful in providing significant performance gains to various application domains. However, their viability to operate on general streaming data is still ambiguous. In this paper, we propose GStream, a general-purpose, scalable data streaming framework on GPUs. The contributions of GStream are as follows: (1) We provide powerful, yet concise language abstractions suitable to describe conventional algorithms as streaming problems. (2)We project these abstractions onto GPUs to fully exploit their inherent massive dataparallelism. (3) We demonstrate the viability of streaming on accelerators. Experiments show that the proposed framework provides flexibility, programmability and performance gains for various benchmarks from a collection of domains, including but not limited to data streaming, data parallel problems and numerical codes. © 2011 IEEE.},
keywords={Application domains;  Conventional algorithms;  Data parallel;  Data parallelism;  Data streaming;  Gpu clusters;  Numerical code;  Performance Gain;  Programmability;  Streaming data, Abstracting;  Program processors, Data reduction},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mansour20081366,
author={Mansour, H. and Krishnamurthy, V. and Nasiopoulos, P.},
title={Channel aware multiuser scalable video streaming over lossy under-provisioned channels: Modeling and analysis},
journal={IEEE Transactions on Multimedia},
year={2008},
volume={10},
number={7},
pages={1366-1381},
doi={10.1109/TMM.2008.2004915},
art_number={4668501},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56849131085&doi=10.1109%2fTMM.2008.2004915&partnerID=40&md5=e0bff494b1ed78aec5073196b2f7e571},
abstract={In this paper, we analyze the performance of media-aware multiuser video streaming strategies in capacity limited wireless channels suffering from latency problems and packet losses. Wireless video streaming applications are characterized by their bandwidth-intensity, delay-sensitivity, and loss-tolerance. Our main contributions include i) a rate-minimized unequal erasure protection (UXP) scheme, ii) an analytical expression for packet delay and play-out deadline of UXP protected scalable video, iii) a loss-distortion model for hierarchical predictive video coders with picture copy concealment, iv) an analysis of the performance and complexity of delay-aware, capacity-aware, and optimized UXP streaming scenarios, and v) we show that the use of unequal error protection causes a rate-constrained optimization problem to be nonconvex. Performance evaluations using a 3GPP network simulator show that, for different channel capacities and packet loss rates, delay-aware nonstationary rate-allocation streaming policies deliver significant gains which range between 1.65 dB to 2 dB in average Y-PSNR of the received video streams over delay-unaware strategies. These gains come at a cost of increased offline computation which is performed prior to the start of the streaming session or in batches during transmission and therefore, do not affect the run-time performance of the streaming system. © 2008 IEEE.},
author_keywords={Loss-distortion modeling;  Scalable video coding (SVC);  Streaming delay analysis;  Unequal erasure protection (UXP);  Wireless video streaming},
keywords={Acoustic streaming;  Channel capacity;  Constrained optimization;  Constraint theory;  Image coding;  Packet loss;  Packet networks;  Programming theory;  Videotex;  Visual communication;  Wireless networks, Analytical expressions;  Channel aware;  Constrained optimization problems;  Distortion models;  Latency problems;  Modeling and analysis;  Multiuser;  Network Simulators;  Non stationaries;  Nonconvex;  Offline computations;  Packet delays;  Packet loss rates;  Performance evaluations;  Scalable video coding (SVC);  Scalable video streaming;  Scalable videos;  Streaming delay analysis;  Streaming sessions;  Streaming systems;  Time performances;  Unequal erasure protection (UXP);  Unequal Error protections;  Video coders;  Wireless channels;  Wireless video streaming, Video streaming},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ferdman2007105,
author={Ferdman, M. and Falsafi, B.},
title={Last-touch correlated data streaming},
journal={ISPASS 2007: IEEE International Symposium on Performance Analysis of Systems and Software},
year={2007},
pages={105-115},
doi={10.1109/ISPASS.2007.363741},
art_number={4211027},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949027123&doi=10.1109%2fISPASS.2007.363741&partnerID=40&md5=0281946120c7e6c349ac2ca0b6f04cb1},
abstract={Recent research advocates address-correlating predictors to identify cache block addresses for prefetch. Unfortunately, address-correlating predictors require correlation data storage proportional in size to a program's active memory footprint. As a result, current proposals for this class of predictor are either limited in coverage due to constrained on-chip storage requirements or limited in prediction lookaheaddue to long off-chip correlation data lookup. In this paper, we propose Last-Touch Correlated Data Streaming (LT-cords), a practical address-correlating predictor. The key idea of LT-cords is to record correlation data off chip in the order they will be used and stream them into a practicallysized on-chip table shortly before they are needed, thereby obviating the need for scalable on-chip tables and enabling low-latency lookup. We use cycle-accurate simulation of an 8-way out-of-order superscalar processor to show that: (1) LT-cords with 214KB of on-chip storage can achieve the same coverage as a last-touch predictor with unlimited storage, without sacrificing predictor lookahead, and (2) LT-cords improves performance by 60% on average and 385% at best in the benchmarks studied. © 2007 IEEE.},
keywords={Chip storage;  Data streaming;  Last-Touch Correlated Data Streaming (LT-cords), Block codes;  Cache memory;  Data reduction;  Data storage equipment;  Microprocessor chips;  Table lookup, Correlation theory},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kazanskiy2017806,
author={Kazanskiy, N. and Protsenko, V. and Serafimovich, P.},
title={Performance analysis of real-time face detection system based on stream data mining frameworks},
journal={Procedia Engineering},
year={2017},
volume={201},
pages={806-816},
doi={10.1016/j.proeng.2017.09.602},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033447906&doi=10.1016%2fj.proeng.2017.09.602&partnerID=40&md5=e4b1bca814f51e0442c7c969f986fc17},
abstract={This work describes performance analysis results of two face detection systems based on Apache Storm and IBM InfoSphere Streams frameworks. Profiling was evaluated on image sequences of four different sizes: 100 x 100, 640 x 640, 1920 x 1080, and 4096 x 3112. Face detection was performed by OpenCV cascade classifier. Experiment was held under five CentOS nodes cluster. It was investigated that system based on Apache Storm was able to operate in real-time at 24 frames per second on used hardware configuration. Apache Storm was more scalable and demonstrated advantage in throughput over its counterpart. Experiment helped to reveal configuration parameters of frameworks that played a major role in face detection task on image sequences. © 2017 The Author(s).},
author_keywords={Big Data;  High performance computing;  Image processing;  Performance measurement;  Real-time;  Stream processing},
keywords={Big data;  Classification (of information);  Data mining;  Image processing;  Nanotechnology;  Storms, Configuration parameters;  Hardware configurations;  High performance computing;  Ibm infosphere streams;  Performance measurements;  Real time;  Real-time face detection;  Stream processing, Face recognition},
publisher={Elsevier Ltd},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Weiler2014282,
author={Weiler, A. and Grossniklaus, M. and Scholl, M.H.},
title={Event identification and tracking in social media streaming data},
journal={CEUR Workshop Proceedings},
year={2014},
volume={1133},
pages={282-287},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923683365&partnerID=40&md5=804490e4d8c2b84b3cd9b6753d1c1318},
abstract={In recent years, the growing popularity and active use of social media services on the web have resulted in massive amounts of user-generated data. With these data available, there is also an increasing interest in analyzing it and to extract information from it. Since social media analysis is concerned with investigating current events around the world, there is a strong emphasis on identifying these evens as quickly as possible, ideally in real-time. In order to scale with the rapidly increasing volume of social media data, we propose to explore very simple event identification mechanisms, rather than applying the more complex approaches that have been proposed in the literature. In this paper, we present a first investigation along this motivation. We discuss a simple sliding window model, which uses shifts in the inverse document frequency (IDF) to capture trending terms as well as to track the evolution and the context around events. Further, we present an initial experimental evaluation of the results that we obtained by analyzing real-world data streams from Twitter.},
author_keywords={Event detection;  Social media analytics;  Stream processing},
keywords={Data mining;  Social networking (online), Event detection;  Experimental evaluation;  Inverse Document Frequency;  Sliding window models;  Social media analysis;  Social media analytics;  Social media services;  Stream processing, Media streaming},
publisher={CEUR-WS},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Botan2009934,
author={Botan, I. and Alonso, G. and Fischer, P.M. and Kossmann, D. and Tatbul, N.},
title={Flexible and scalable storage management for data-intensive Stream processing},
journal={Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology, EDBT'09},
year={2009},
pages={934-945},
doi={10.1145/1516360.1516467},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349087620&doi=10.1145%2f1516360.1516467&partnerID=40&md5=4fb4149c14d0f6a28d741ea48703f9e6},
abstract={Data Stream Management Systems (DSMS) operate under strict performance requirements. Key to meeting such requirements is to efficiently handle time-critical tasks such as managing internal states of continuous query operators, traffic on the queues between operators, as well as providing storage support for shared computation and archived data. In this paper, we introduce a general purpose storage management framework for DSMSs that performs these tasks based on a clean, loosely-coupled, and flexible system design that also facilitates performance optimization. An important contribution of the framework is that, in analogy to buffer management techniques in relational database systems, it uses information about the access patterns of streaming applications to tune and customize the performance of the storage manager. In the paper, we first analyze typical application requirements at different granularities in order to identify important tunable parameters and their corresponding values. Based on these parameters, we define a general-purpose storage management interface. Using the interface, a developer can use our SMS (Storage Manager for Streams) to generate a customized storage manager for streaming applications. We explore the performance and potential of SMS through a set of experiments using the Linear Road benchmark. Copyright 2009 ACM.},
keywords={Access patterns;  Archived data;  Buffer management techniques;  Continuous queries;  Critical tasks;  Data stream management systems;  Flexible system;  General purpose;  Internal state;  Performance optimizations;  Performance requirements;  Scalable storage;  Shared computation;  Storage management;  Storage manager;  Stream processing;  Streaming applications;  Tunable parameter;  Typical application, Buffer storage;  Managers, Relational database systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liang2008257,
author={Liang, C. and Huang, X.},
title={Smartcell: A power-efficient reconfigurable architecture for data streaming applications},
journal={IEEE Workshop on Signal Processing Systems, SiPS: Design and Implementation},
year={2008},
pages={257-262},
doi={10.1109/SIPS.2008.4671772},
art_number={4671772},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849112329&doi=10.1109%2fSIPS.2008.4671772&partnerID=40&md5=8a7a14e7dd95205b12b51cacdc077e7f},
abstract={This paper presents SmartCell as a novel power efficient reconfigurable architecture targeted for data streaming applications. We describe the design details of the SmartCell architecture, including processing element, reconfigurable interconnection fabrics, instruction and control process and dynamic configuration scheme. The performance in terms of power efficiency and system throughput is evaluated through a set of benchmark applications, and is compared with ASIC, FPGA and RaPiD reconfigurable architecture. The results show that the SmartCell consumes about 52% and 75% less power than RaPiD and FPGA, respectively. It is demonstrated that SmartCell is a promising reconfigurable, power efficient and scalable computing architecture that can potentially bridge the gap between logic specific ASIC and configurable FPGA for data streaming applications. © 2008 IEEE.},
author_keywords={ASIC;  CGRA;  Data streaming application;  DSP;  FPGA;  Power efficiency;  Reconfigurability},
keywords={Application specific integrated circuits;  Applications;  Architecture;  Data reduction;  Digital signal processors;  Electric power supplies to apparatus;  Field programmable gate arrays (FPGA);  Signal processing;  Technical presentations, ASIC;  CGRA;  Data streaming application;  DSP;  FPGA;  Power efficiency;  Reconfigurability, Benchmarking},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suh2003410,
author={Suh, J. and Kim, E.-G. and Crago, S.P. and Srinivasan, L. and French, M.C.},
title={A performance analysis of PIM, stream processing, and tiled processing on memory-intensive signal processing kernels},
journal={Conference Proceedings - Annual International Symposium on Computer Architecture, ISCA},
year={2003},
pages={410-419},
doi={10.1145/859618.859665},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038345686&doi=10.1145%2f859618.859665&partnerID=40&md5=b70cf2799bec27263183daf012636ac9},
abstract={Trends in microprocessors of increasing die size and clock speed and decreasing feature sizes have fueled rapidly increasing performance. However, the limited improvements in DRAM latency and bandwidth and diminishing returns of increasing superscalar ILP and cache sizes have led to the proposal of new microprocessor architectures that implement processor-in-memory, stream processing, and tiled processing. Each architecture is typically evaluated separately and compared to a baseline architecture. In this paper, we evaluate the performance of processors that implement these architectures on a common set of signal processing kernels. The implementation results are compared with the measured performance of a conventional system based on the PowerPC with Altivec. The results show that these new processors show significant improvements over conventional systems and that each architecture has its own strengths and weaknesses.},
keywords={Buffer storage;  Computer architecture;  Computer simulation;  Performance;  Pipeline processing systems;  Signal processing, Coherent side-lobe canceller;  Memory-intensive signal processing kernels;  Processor-in-memory;  Stream processing;  Tiled processing, Dynamic random access storage},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fernández-Rodríguez201762,
author={Fernández-Rodríguez, J.Y. and Álvarez-García, J.A. and Arias Fisteus, J. and Luaces, M.R. and Corcoba Magaña, V.},
title={Benchmarking real-time vehicle data streaming models for a smart city},
journal={Information Systems},
year={2017},
volume={72},
pages={62-76},
doi={10.1016/j.is.2017.09.002},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030771090&doi=10.1016%2fj.is.2017.09.002&partnerID=40&md5=a8e8561f91eb20efdad76a7297f98266},
abstract={The information systems of smart cities offer project developers, institutions, industry and experts the possibility to handle massive incoming data from diverse information sources in order to produce new information services for citizens. Much of this information has to be processed as it arrives because a real-time response is often needed. Stream processing architectures solve this kind of problems, but sometimes it is not easy to benchmark the load capacity or the efficiency of a proposed architecture. This work presents a real case project in which an infrastructure was needed for gathering information from drivers in a big city, analyzing that information and sending real-time recommendations to improve driving efficiency and safety on roads. The challenge was to support the real-time recommendation service in a city with thousands of simultaneous drivers at the lowest possible cost. In addition, in order to estimate the ability of an infrastructure to handle load, a simulator that emulates the data produced by a given amount of simultaneous drivers was also developed. Experiments with the simulator show how recent stream processing platforms like Apache Kafka could replace custom-made streaming servers in a smart city to achieve a higher scalability and faster responses, together with cost reduction. © 2017 Elsevier Ltd},
author_keywords={Big Data;  Data streaming;  Distributed systems;  Simulator;  Smart city},
keywords={Big data;  Computer architecture;  Cost reduction;  Data reduction;  Efficiency;  Information services;  Simulators, Data streaming;  Distributed systems;  Information sources;  Project developers;  Proposed architectures;  Real time response;  Stream processing;  Streaming servers, Smart city},
publisher={Elsevier Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bumgardner2014219,
author={Bumgardner, V.K.C. and Marek, V.W.},
title={Scalable hybrid stream and hadoop network analysis system},
journal={ICPE 2014 - Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
year={2014},
pages={219-224},
doi={10.1145/2568088.2568103},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899703832&doi=10.1145%2f2568088.2568103&partnerID=40&md5=1f8312a0523dc3f0c7522398be5c67f6},
abstract={Collections of network traces have long been used in network traffic analysis. Flow analysis can be used in network anomaly discovery, intrusion detection and more generally, discovery of actionable events on the network. The data collected during processing may be also used for prediction and avoidance of traffic congestion, network capacity planning, and the development of software-defined networking rules. As network flow rates increase and new network technologies are introduced on existing hardware platforms, many organizations find themselves either technically or financially unable to generate, collect, and/or analyze network flow data. The continued rapid growth of network trace data, requires new methods of scalable data collection and analysis. We report on our deployment of a system designed and implemented at the University of Kentucky that supports analysis of network traffic across the enterprise. Our system addresses problems of scale in existing systems, by using distributed computing methodologies, and is based on a combination of stream and batch processing techniques. In addition to collection, stream processing using Storm is utilized to enrich the data stream with ephemeral environment data. Enriched stream-data is then used for event detection and near real-time flow analysis by an in-line complex event processor. Batch processing is performed by the Hadoop MapReduce framework, from data stored in HBase BigTable storage. In benchmarks on our 10 node cluster, using actual network data, we were able to stream process over 315k flows/sec. In batch analysis were we able to process over 2.6M flows/sec with a storage compression ratio of 6.7:1.},
author_keywords={Complex event processing;  Hadoop;  NetFlow;  SDN;  Stream processing},
keywords={Batch data processing;  Data handling;  Digital storage;  Intrusion detection;  Traffic congestion, Complex event processing;  Hadoop;  NetFlows;  SDN;  Stream processing, Complex networks},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bo2008577,
author={Bo, D. and Kun, D. and Xiaoyi, Z.},
title={A high performance enterprise service bus platform for complex event processing},
journal={Proceedings - 7th International Conference on Grid and Cooperative Computing, GCC 2008},
year={2008},
pages={577-582},
doi={10.1109/GCC.2008.66},
art_number={4662919},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57949113573&doi=10.1109%2fGCC.2008.66&partnerID=40&md5=5eb58b67e8419030c4b3d46e6c82fe43},
abstract={Enterprise Service Bus (ESB) technology combines the Service-Oriented Architecture, which is based on the request/ response model, and the Event-Driven Architecture, which is based on the publish/subscribe model. It can satisfy the demand of loose couple communication and cooperation in the enterprise applications. However, existing ESB based systems can't process the complex events in the real-world applications very well. In this paper, we firstly give a complex event processing model based on the relational algebra, and then propose a complex event processing oriented enterprise service bus platform and a complex event stream processing engine, and give the main algorithms of complex event processing and their performance analysis. Experiments show that our platform performs much better than the existing ESB based systems in complex event processing. © 2008 IEEE.},
author_keywords={Complex event processing;  Enterprise service bus;  Event-driven architecture;  Service-oriented architecture},
keywords={Buses;  Distributed computer systems;  Grid computing;  Information services;  Web services, Complex event processing;  Enterprise applications;  Enterprise service bus;  Event stream processing;  Event-driven architecture;  Model-based;  Performance analyses;  Publish/subscribe;  Relational algebras;  Response models;  Service-oriented architecture, Architecture},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stephen2016348,
author={Stephen, J.J. and Savvides, S. and Sundaram, V. and Ardekani, M.S. and Eugster, P.},
title={STYX: Stream processing with trustworthy cloud-based execution},
journal={Proceedings of the 7th ACM Symposium on Cloud Computing, SoCC 2016},
year={2016},
pages={348-360},
doi={10.1145/2987550.2987574},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995701631&doi=10.1145%2f2987550.2987574&partnerID=40&md5=59055ab423d42a60e9338aa6c3461b18},
abstract={With the advent of the Internet of Things (IoT), billions of devices are expected to continuously collect and process sensitive data (e.g., location, personal health). Due to limited computational capacity available on IoT devices, the current de facto model for building IoT applications is to send the gathered data to the cloud for computation. While private cloud infrastructures for handling large amounts of data streams are expensive to build, using low cost public (untrusted) cloud infrastructures for processing continuous queries including on sensitive data leads to concerns over data confidentiality. This paper presents STYX, a novel programming abstraction and managed runtime system, that ensures confidentiality of IoT applications whilst leveraging the public cloud for continuous query processing. The key idea is to intelligently utilize partially homomorphic encryption to perform as many computationally intensive operations as possible in the untrusted cloud. STYX provides a simple abstraction to the IoT developer to hide the complexities of (1) applying complex cryptographic primitives, (2) reasoning about performance of such primitives, (3) deciding which computations can be executed in an untrusted tier, and (4) optimizing cloud resource usage. An empirical evaluation with benchmarks and case studies shows the feasibility of our approach.},
author_keywords={Confidentiality;  IoT;  Stream data processing},
keywords={Abstracting;  Cloud computing;  Computer programming;  Computer systems programming;  Cryptography;  Data handling;  Internet of things;  Query processing;  Trusted computing, Computational capacity;  Confidentiality;  Continuous query processing;  Cryptographic primitives;  Ho-momorphic encryptions;  Internet of thing (IOT);  Programming abstractions;  Stream data processing, Distributed computer systems},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Casas20161,
author={Casas, P. and D'Alconzo, A. and Zseby, T. and Mellia, M.},
title={Big-DAMA: Big data analytics for network traffic monitoring and analysis},
journal={LANCOMM 2016 - Proceedings of the 2016 ACM SIGCOMM Workshop on Fostering Latin-American Research in Data Communication Networks, Part of SIGCOMM 2016},
year={2016},
pages={1-3},
doi={10.1145/2940116.2940117},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019037019&doi=10.1145%2f2940116.2940117&partnerID=40&md5=bec18c74e71a5a6ce95e8b36918e8ddd},
abstract={The complexity of the Internet has dramatically increased in the last few years, making it more important and challenging to design scalable Network Traffic Monitoring and Analysis (NTMA) applications and tools. Critical NTMA applications such as the detection of anomalies, network attacks and intrusions, require fast mechanisms for online analysis of thousands of events per second, as well as efficient techniques for offline analysis of massive historical data. We are witnessing a major development in Big Data Analysis Frameworks (BDAFs), but the application of BDAFs and scalable analysis techniques to the NTMA domain remains poorly understood and only in-house and difficult to benchmark solutions are conceived. In this position paper we describe the basis of the Big-DAMA research project, which aims at tackling this growing need by benchmarking and developing novel scalable techniques and frameworks capable to analyze both online network traffic data streams and offline massive traffic datasets. © 2016 ACM.},
author_keywords={Big data;  Data mining;  Data stream processing;  Machine learning;  Network traffic monitoring and analysis},
keywords={Benchmarking;  Convolutional codes;  Data communication systems;  Data handling;  Data mining;  Learning systems, Analysis frameworks;  Benchmark solutions;  Data stream processing;  Network traffic monitoring;  Off-line analysis;  On-line analysis;  Scalable analysis;  Scalable networks, Big data},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{DePauw201018,
author={De Pauw, W. and Leţia, M. and Gedik, B. and Andrade, H. and Frenkiel, A. and Pfeifer, M. and Sow, D.},
title={Visual debugging for stream processing applications},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6418 LNCS},
pages={18-35},
doi={10.1007/978-3-642-16612-9_3},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650141213&doi=10.1007%2f978-3-642-16612-9_3&partnerID=40&md5=66c8d1e56d1f432bd466ddc7993b7e2c},
abstract={Stream processing is a new computing paradigm that enables continuous and fast analysis of massive volumes of streaming data. Debugging streaming applications is not trivial, since they are typically distributed across multiple nodes and handle large amounts of data. Traditional debugging techniques like breakpoints often rely on a stop-the-world approach, which may be useful for debugging single node applications, but insufficient for streaming applications. We propose a new visual and analytic environment to support debugging, performance analysis, and troubleshooting for stream processing applications. Our environment provides several visualization methods to study, characterize, and summarize the flow of tuples between stream processing operators. The user can interactively indicate points in the streaming application from where tuples will be traced and visualized as they flow through different operators, without stopping the application. To substantiate our discussion, we also discuss several of these features in the context of a financial engineering application. © 2010 Springer-Verlag.},
author_keywords={debugging;  performance analysis;  streaming applications;  tracing;  Visualization},
keywords={Break-points;  Computing paradigm;  debugging;  Financial engineering;  Flowthrough;  Large amounts of data;  Multiple nodes;  performance analysis;  Stream processing;  Streaming applications;  Streaming data;  tracing;  Troubleshooting;  Visual debugging;  Visualization method;  Computing paradigm;  Financial engineering;  Large amounts of data;  Performance analysis;  Stream processing;  Streaming applications;  Tracing;  Visualization method, Visualization;  Computer debugging;  Flow visualization;  Visualization, Hydraulics;  Program debugging},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Buddhika20161143,
author={Buddhika, T. and Pallickara, S.},
title={NEPTUNE: Real Time Stream Processing for Internet of Things and Sensing Environments},
journal={Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016},
year={2016},
pages={1143-1152},
doi={10.1109/IPDPS.2016.43},
art_number={7516111},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983239037&doi=10.1109%2fIPDPS.2016.43&partnerID=40&md5=ad40dd2a0ed8906d66dd3dec483d4124},
abstract={Improvements in miniaturization and networking capabilities of sensors have contributed to the proliferation of Internet of Things (IoT) and continuous sensing environments. Data streams generated in such settings must keep pace with generation rates and be processed in real time. Challenges in accomplishing this include: high data arrival rates, buffer overflows, context-switches, and object creation overheads. We propose a holistic framework that addresses the CPU, memory, network, and kernel issues involved in stream processing. Our prototype, Neptune, builds on our Granules cloud runtime. The framework maximizes bandwidth utilization in the presence of small messages via the use of buffering and dynamic compactions of packets based on payload entropy. Our use of thread-pools and batched processing reduces context switches and improves effective CPU utilizations. NEPTUNE alleviates memory pressure that can lead to swapping, page faults, and thrashing through efficient reuse of objects. To cope with buffer overflows we rely on flow control and throttling the preceding stages of a processing pipeline. Our benchmarks demonstrate the suitability of the Neptune and we contrast our performance with Apache Storm, the dominant stream-processing framework developed by Twitter. At a single node, we are able to achieve a processing rate of ∼2 million stream packets per-second. In a distributed setup, we achieved a rate of ∼100 million packets per-second. © 2016 IEEE.},
author_keywords={Distributed stream processing;  High-throughput data processing;  Internet-of-Things},
keywords={Benchmarking;  Buffer storage;  Data handling;  Distributed parameter control systems;  Internet;  Internet of things, Band-width utilization;  Distributed setups;  Distributed stream processing;  Dynamic compaction;  High-throughput data;  Holistic frameworks;  Internet of Things (IOT);  Million packets per seconds, Pipeline processing systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo20151161,
author={Guo, T. and Sathe, S. and Aberer, K.},
title={Fast distributed correlation discovery over streaming time-series data},
journal={International Conference on Information and Knowledge Management, Proceedings},
year={2015},
volume={19-23-Oct-2015},
pages={1161-1170},
doi={10.1145/2806416.2806440},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958247744&doi=10.1145%2f2806416.2806440&partnerID=40&md5=264c3776d67d3080a03b3f48f436e5fb},
abstract={The dramatic rise of time-series data in a variety of contexts, such as social networks, mobile sensing, data centre monitoring, etc., has fuelled interest in obtaining real-time insights from such data using distributed stream processing systems. One such extremely valuable insight is the discovery of correlations in real-time from large-scale time-series data. A key challenge in discovering correlations is that the number of time-series pairs that have to be analyzed grows quadratically in the number of time-series, giving rise to a quadratic increase in both computation cost and communication cost between the cluster nodes in a distributed environment. To tackle the challenge, we propose a framework called AEGIS. AEGIS exploits well-established statistical properties to dramatically prune the number of time-series pairs that have to be evaluated for detecting interesting correlations. Our extensive experimental evaluations on real and synthetic datasets establish the efficacy of AEGIS over baselines.},
author_keywords={Approximate algorithm;  Distributed computing;  Stream processing;  Time series analysis},
keywords={Distributed computer systems;  Distributed parameter control systems;  Knowledge management;  Real time systems, Approximate algorithms;  Distributed environments;  Distributed stream processing;  Experimental evaluation;  Large-scale time series;  Statistical properties;  Stream processing;  Synthetic datasets, Time series analysis},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bellavista2014363,
author={Bellavista, P. and Corradi, A. and Reale, A. and Ticca, N.},
title={Priority-based resource scheduling in distributed stream processing systems for big data applications},
journal={Proceedings - 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing, UCC 2014},
year={2014},
pages={363-370},
doi={10.1109/UCC.2014.46},
art_number={7027513},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946692108&doi=10.1109%2fUCC.2014.46&partnerID=40&md5=12a56e5d02db84468e9700245baa17da},
abstract={Distributed Stream Processing Systems (DSPSs) are attracting increasing industrial and academic interest as flexible tools to implement scalable and cost-effective on-line analytics applications over Big Data streams. Often hosted in private/public cloud deployment environments, DSPSs offer data stream processing services that transparently exploit the distributed computing resources made available to them at runtime. Given the volume of data of interest, possible (hard/soft) real-time processing requirements, and the time-variable characteristics of input data streams, it is very important for DSPSs to use smart and innovative scheduling techniques that allocate computing resources properly and avoid static over-provisioning. In this paper, we originally investigate the suitability of exploiting application-level indications about differentiated priorities of different stream processing tasks to enable application-specific DSPS resource scheduling, e.g., Capable of re-shaping processing resources in order to dynamically follow input data peaks of prioritized tasks, with no static over-provisioning. We originally propose a general and simple technique to design and implement priority-based resource scheduling in flow-graph-based DSPSs, by allowing application developers to augment DSPS graphs with priority metadata and by introducing an extensible set of priority schemas to be automatically handled by the extended DSPS. In addition, we show the effectiveness of our approach via its implementation and integration in our Quasit DSPS and through experimental evaluation of this prototype on a real-world stream processing application of Big Data vehicular traffic analysis. © 2014 IEEE.},
author_keywords={Application-level and Application-specific Scheduling;  Big Data;  Cloud Computing Optimization;  Distributed Stream Processing;  Priority-based Resource Scheduling;  Vehicular Traffic Analysis},
keywords={Cloud computing;  Cost effectiveness;  Data communication systems;  Data handling;  Distributed parameter control systems;  Flow graphs;  Graphic methods;  Input output programs;  Scheduling, Application developers;  Application-specific scheduling;  Data stream processing;  Design and implements;  Distributed computing resources;  Distributed stream processing;  Experimental evaluation;  Resource-scheduling, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mahmood2018219,
author={Mahmood, A.R. and Daghistani, A. and Aly, A.M. and Tang, M. and Basalamah, S. and Prabhakar, S. and Aref, W.G.},
title={Adaptive processing of spatial-keyword data over a distributed streaming cluster},
journal={GIS: Proceedings of the ACM International Symposium on Advances in Geographic Information Systems},
year={2018},
pages={219-228},
doi={10.1145/3274895.3274932},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058614525&doi=10.1145%2f3274895.3274932&partnerID=40&md5=dda1b6ecf17ee160ecaba85ecdb504a4},
abstract={The widespread use of GPS-enabled smartphones along with the popularity of micro-blogging and social networking applications, e.g., Twitter and Facebook, has resulted in the generation of huge streams of geo-tagged textual data. Many applications require real-time processing of these streams. For example, location-based ad-targeting systems enable advertisers to register millions of ads to millions of users based on the users’ location and textual proile. Existing streaming systems are either centralized or are not spatial-keyword aware, and hence these systems cannot eiciently support the processing of rapidly arriving spatial-keyword data streams. In this paper, we introduce a two-layered indexing scheme for the distributed processing of spatial-keyword data streams. We realize this indexing scheme in Tornado, a distributed spatial-keyword streaming system. The irst layer, termed the routing layer, is used to fairly distribute the workload, and furthermore, co-locate the data objects and the corresponding queries at the same processing units. The routing layer uses the Augmented-Grid, a novel structure that is equipped with an eicient search algorithm for distributing the data objects and queries. The second layer, termed the evaluation layer, resides within each processing unit to reduce the processing overhead. The two-layered index adapts to changes in the workload by applying a cost formula that continuously represents the processing overhead at each processing unit. Extensive experimental evaluation using real Twitter data indicates that Tornado achieves high scalability and more than 2x improvement over the baseline approach in terms of the overall system throughput. © 2018 held by the owner/author(s). Publication rights licensed to ACM.},
author_keywords={Distributed streaming;  Spatial-keyword processing},
keywords={Data mining;  Geographic information systems;  Image coding;  Indexing (of information);  Information systems;  Information use;  Social networking (online);  Spatial distribution;  Tornadoes, Adaptive processing;  Distributed processing;  Distributed streaming;  Experimental evaluation;  High scalabilities;  Processing overhead;  Realtime processing;  Social networking applications, Indexing (materials working)},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hoffmann20181002,
author={Hoffmann, M. and Lattuada, A. and McSherry, F. and Kalavri, V. and Liagouris, J. and Roscoe, T.},
title={Megaphone: Latencyconscious state migration for distributed streaming dataflows},
journal={Proceedings of the VLDB Endowment},
year={2018},
volume={12},
number={9},
pages={1002-1015},
doi={10.14778/3329772.3329777},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073957073&doi=10.14778%2f3329772.3329777&partnerID=40&md5=be5cfefa57243528a817080fb1fb0a60},
abstract={We design and implement Megaphone, a data migration mechanism for stateful distributed dataflow engines with latency objectives. When compared to existing migration mechanisms, Megaphone has the following differentiating characteristics: (i) migrations can be subdivided to a configurable granularity to avoid latency spikes, and (ii) migrations can be prepared ahead of time to avoid runtime coordination. Megaphone is implemented as a library on an unmodified timely dataflow implementation, and provides an operator interface compatible with its existing APIs. We evaluate Megaphone on established benchmarks with varying amounts of state and observe that compared to naïve approaches Megaphone reduces service latencies during reconfiguration by orders of magnitude without significantly increasing steady-state overhead. © 2018, VLDB Endowment.},
keywords={Design and implements;  Distributed data flow;  Distributed streaming;  Migration mechanisms;  Operator interfaces;  Orders of magnitude;  Service latency;  State migrations, Application programming interfaces (API)},
publisher={VLDB Endowment},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Imai2017504,
author={Imai, S. and Patterson, S. and Varela, C.A.},
title={Maximum Sustainable throughput Prediction for Data Stream Processing over Public Clouds},
journal={Proceedings - 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2017},
year={2017},
pages={504-513},
doi={10.1109/CCGRID.2017.105},
art_number={7973737},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027442774&doi=10.1109%2fCCGRID.2017.105&partnerID=40&md5=461baeb999cdc957170a0dccf23c1399},
abstract={In cloud-based stream processing services, the maximum sustainable throughput (MST) is defined as the maximum throughput that a system composed of a fixed number of virtual machines (VMs) can ingest indefinitely. If the incoming data rate exceeds the system's MST, unprocessed data accumulates, eventually making the system inoperable. Thus, it is important for the service provider to keep the MST always larger than the incoming data rate by dynamically changing the number of VMs used by the system. In this paper, we identify a common data processing environment used by modern data stream processing systems, and we propose MST prediction models for this environment. We train the models using linear regression with samples obtained from a few VMs and predict MST for a larger number of VMs. To minimize the time and cost for model training, we statistically determine a set of training samples using Intel's Storm benchmarks with representative resource usage patterns. Using typical use-case benchmarks on Amazon's EC2 public cloud, our experiments show that, training with up to 8 VMs, we can predict MST for streaming applications with less than 4% average prediction error for 12 VMs, 9% for 16 VMs, and 32% for 24 VMs. Further, we evaluate our prediction models with simulation based elastic VM scheduling on a realistic workload. These simulation results show that with 10% over provisioning, our proposed models' cost efficiency is on par with the cost of an optimal scaling policy without incurring any service level agreement violations. © 2017 IEEE.},
author_keywords={Auto-scaling;  Cloud computing;  Performance prediction;  Resource management},
keywords={Benchmarking;  Cloud computing;  Cluster computing;  Costs;  Data communication systems;  Data handling;  Forecasting;  Grid computing;  Sustainable development;  Throughput, Auto-scaling;  Average prediction error;  Data stream processing;  Performance prediction;  Resource management;  Resource usage patterns;  Service Level Agreements;  Streaming applications, Distributed computer systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Čermák2016919,
author={Čermák, M. and Tovarňák, D. and Laśtovička, M. and Čeleda, P.},
title={A performance benchmark for NetFlow data analysis on distributed stream processing systems},
journal={Proceedings of the NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium},
year={2016},
pages={919-924},
doi={10.1109/NOMS.2016.7502926},
art_number={7502926},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979784158&doi=10.1109%2fNOMS.2016.7502926&partnerID=40&md5=5144a62e72398f052a8446fa402c3aed},
abstract={Modern distributed stream processing systems can potentially be applied to real time network flow processing. However, differences in performance make some systems more suitable than others for being applied to this domain. We propose a novel performance benchmark, which is based on common security analysis algorithms of NetFlow data to determine the suitability of distributed stream processing systems. Three of the most used distributed stream processing systems are bench-marked and the results are compared with NetFlow data processing challenges and requirements. The benchmark results show that each system reached a sufficient data processing speed using a basic deployment scenario with little to no configuration tuning. Our benchmark, unlike any other, enables the performance of small structured messages to be processed on any stream processing system. © 2016 IEEE.},
keywords={Benchmarking;  Distributed parameter control systems;  Real time systems, Deployment scenarios;  Distributed stream processing;  NetFlow data;  Processing speed;  Real time network;  Security analysis;  Stream processing systems;  Structured messages, Data handling},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Najafi2016493,
author={Najafi, M. and Sadoghi, M. and Jacobsen, H.-A.},
title={SplitJoin: A scalable, low-latency stream join architecture with adjustable ordering precision},
journal={Proceedings of the 2016 USENIX Annual Technical Conference, USENIX ATC 2016},
year={2016},
pages={493-505},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023188645&partnerID=40&md5=abab9b6237daf67c3a69687ee3619087},
abstract={There is a rising interest in accelerating stream processing through modern parallel hardware, yet it remains a challenge as how to exploit the available resources to achieve higher throughput without sacrificing latency due to the increased length of processing pipeline and communication path and the need for central coordination. To achieve these objectives, we introduce a novel top-down data flow model for stream join processing (arguably, one of the most resource-intensive operators in stream processing), called SplitJoin, that operates by splitting the join operation into independent storing and processing steps that gracefully scale with respect to the number of cores. Furthermore, SplitJoin eliminates the need for global coordination while preserving the order of input streams by re-thinking how streams are channeled into distributed join computation cores and maintaining the order of output streams by proposing a novel distributed punctuation technique. Throughout our experimental analysis, SplitJoin offered up to 60% improvement in throughput while reducing latency by up to 3.3X compared to state-of-the-art solutions. © 2016 by The USENIX Association. All Rights Reserved.},
keywords={Computer hardware description languages;  Data flow analysis, Communication path;  Data flow modeling;  Experimental analysis;  Global coordination;  Parallel hardware;  Processing steps;  State of the art;  Stream processing, Pipeline processing systems},
publisher={USENIX Association},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dehghanzadeh2015307,
author={Dehghanzadeh, S. and Dell’Aglio, D. and Gao, S. and Della Valle, E. and Mileo, A. and Bernstein, A.},
title={Approximate continuous query answering over streams and dynamic linked data sets},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9114},
pages={307-325},
doi={10.1007/978-3-319-19890-3_20},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937398752&doi=10.1007%2f978-3-319-19890-3_20&partnerID=40&md5=235ad0e355b134a110f8b8020717a174},
abstract={To perform complex tasks, RDF Stream Processing Web applications evaluate continuous queries over streams and quasi-static (background) data. While the former are pushed in the application, the latter are continuously retrieved from the sources. As soon as the background data increase the volume and become distributed over the Web, the cost to retrieve them increases and applications become unresponsive. In this paper, we address the problem of optimizing the evaluation of these queries by leveraging local views on background data. Local views enhance performance, but require maintenance processes, because changes in the background data sources are not automatically reflected in the application. We propose a two-step query-driven maintenance process to maintain the local view: it exploits information from the query (e.g., the sliding window definition and the current window content) to maintain the local view based on user-defined Quality of Service constraints. Experimental evaluation show the effectiveness of the approach. © Springer International Publishing Switzerland 2015.},
keywords={Quality of service;  Query processing;  Social networking (online), Continuous queries;  Data-sources;  Experimental evaluation;  Maintenance process;  Quality of Service constraints;  Sliding Window;  Stream processing;  WEB application, Big data},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mokhtari2014819,
author={Mokhtari, R. and Stumm, M.},
title={BigKernel - High performance CPU-GPU communication pipelining for big data-style applications},
journal={Proceedings of the International Parallel and Distributed Processing Symposium, IPDPS},
year={2014},
pages={819-828},
doi={10.1109/IPDPS.2014.89},
art_number={6877313},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906695225&doi=10.1109%2fIPDPS.2014.89&partnerID=40&md5=3feb2a11068e1ee348b3f0d7cb652521},
abstract={GPUs offer an order of magnitude higher compute power and memory bandwidth than CPUs. GPUs therefore might appear to be well suited to accelerate computations that operate on voluminous data sets in independent ways, e.g., for transformations, filtering, aggregation, partitioning or other 'Big Data' style processing. Yet experience indicates that it is difficult, and often error-prone, to write GPGPU programs which efficiently process data that does not fit in GPU memory, partly because of the intricacies of GPU hardware architecture and programming models, and partly because of the limited bandwidth available between GPUs and CPUs. In this paper, we propose Big Kernel, a scheme that provides pseudo-virtual memory to GPU applications and is implemented using a 4-stage pipeline with automated prefetching to (i) optimize CPU-GPU communication and (ii) optimize GPU memory accesses. Big Kernel simplifies the programming model by allowing programmers to write kernels using arbitrarily large data structures that can be partitioned into segments where each segment is operated on independently, these kernels are transformed into Big Kernel using straight-forward compiler transformations. Our evaluation on six data-intensive benchmarks shows that Big Kernel achieves an average speedup of 1.7 over state-of-the-art double-buffering techniques and an average speedup of 3.0 over corresponding multi-threaded CPU implementations. © 2014 IEEE.},
author_keywords={communication;  CPU;  GPU;  optimization;  stream processing},
keywords={Communication;  Distributed parameter networks;  Optimization;  Program compilers, Compiler transformations;  CPU;  GPU;  Hardware architecture;  Limited bandwidth;  Memory bandwidths;  Programming models;  Stream processing, Big data},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsu2010169,
author={Hsu, C.-H. and Hefeeda, M.},
title={Quality-aware segment transmission scheduling in peer-to-peer streaming systems},
journal={MMSys'10 - Proceedings of the 2010 ACM SIGMM Conference on Multimedia Systems},
year={2010},
pages={169-179},
doi={10.1145/1730836.1730857},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951289876&doi=10.1145%2f1730836.1730857&partnerID=40&md5=d070f660395f722f7a84d8febeb560c0},
abstract={In peer-to-peer (P2P) mesh-based streaming systems, each video sequence is typically divided into segments, which are then streamed from multiple senders to a receiver. The receiver needs to coordinate the senders by specifying a transmission schedule for each of them. We consider the scheduling problem in both live and on-demand P2P streaming systems. We formulate the problem of scheduling segment transmission in order to maximize the perceived video quality of the receiver. We prove that this problem is NP-Complete. We present an integer linear programming (ILP) formulation for this problem, and we optimally solve it using an ILP solver. This optimal solution, however, is computationally expensive and is not suitable for real-time streaming systems. Thus, we propose a polynomial-time approximation algorithm, which yields transmission schedules with analytical guarantees on the worst-case performance. More precisely, we show that the approximation factor is at most 3, compared to the absolutely optimal solution as a benchmark. We implement the proposed approximation and optimal algorithms in a packet-level simulator for P2P streaming systems. We also implement two other scheduling algorithms proposed in the literature and used in popular P2P streaming systems. By simulating large P2P systems and streaming nine real video sequences with diverse visual and motion characteristics, we demonstrate that our proposed approximation algorithm: (i) produces near-optimal perceived video quality, (ii) can run in real time, and (iii) outperforms other algorithms in terms of perceived video quality, smoothness of the rendered videos, and balancing the load across sending peers. For example, our simulation results indicate that the proposed algorithm outperforms heuristic algorithms used in current systems by up to 8 dB in perceived video quality and up to 20% in continuity index. Copyright 2010 ACM.},
author_keywords={Optimization;  Peer-to-peer streaming;  Perceived video quality;  Transmission scheduling;  Video streaming},
keywords={Approximation factor;  Continuity index;  Current system;  Integer Linear Programming;  Motion characteristics;  NP Complete;  On-Demand;  Optimal algorithm;  Optimal solutions;  Other algorithms;  P2P streaming;  P2P system;  Peer to peer;  Peer-to-peer streaming;  Perceived video quality;  Polynomial-time approximation algorithms;  Real time;  Real time streaming;  Real video sequences;  Scheduling problem;  Simulation result;  Transmission scheduling;  Video sequences;  Worst-case performance, Acoustic streaming;  Distributed computer systems;  Heuristic algorithms;  Integer programming;  Linear programming;  Linearization;  Multimedia systems;  Optimal systems;  Optimization;  Peer to peer networks;  Polynomial approximation;  Real time systems;  Scheduling algorithms;  Video recording;  Video streaming;  Videotex, Approximation algorithms},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kontaki2008251,
author={Kontaki, M. and Papadopoulos, A.N. and Manolopoulos, Y.},
title={Continuous trend-based clustering in data streams},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={5182 LNCS},
pages={251-262},
doi={10.1007/978-3-540-85836-2_24},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949142775&doi=10.1007%2f978-3-540-85836-2_24&partnerID=40&md5=8860be8dae926cccdb47d1cb5194b079},
abstract={Trend analysis of time series is an important problem since trend identification enables the prediction of the near future. In streaming time series the problem is more challenging due to the dynamic nature of the data. In this paper, we propose a method to continuously clustering a number of streaming time series based on their trend characteristics. Each streaming time series is transformed to a vector by means of the Piecewise Linear Approximation (PLA) technique. The PLA vector comprises pairs of values (timestamp, trend) denoting the starting time of the trend and the type of the trend (either UP or DOWN) respectively. A distance metric for PLA vectors is introduced. We propose split and merge criteria to continuously update the clustering information. Moreover, the proposed method handles outliers. Performance evaluation results, based on real-life and synthetic data sets, show the efficiency and scalability of the proposed scheme. © 2008 Springer-Verlag Berlin Heidelberg.},
keywords={Administrative data processing;  Cluster analysis;  Flow of solids;  Knowledge based systems;  Piecewise linear techniques;  Vectors, Clustering information;  Data streaming;  Data warehousing;  Distance metric;  Dynamic natures;  International conferences;  Knowledge discovery;  Performance evaluation;  Piecewise linear approximation;  Streaming time series;  Synthetic data sets;  Time stamping;  Time-series;  Trend analysis, Time series analysis},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2008568,
author={Liu, X. and Yin, H. and Lin, C. and Liu, Y. and Chen, Z. and Xiao, X.},
title={Performance analysis and industrial practice of peer-assisted content distribution network for large-scale live video streaming},
journal={Proceedings - International Conference on Advanced Information Networking and Applications, AINA},
year={2008},
pages={568-574},
doi={10.1109/AINA.2008.132},
art_number={4482757},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249122029&doi=10.1109%2fAINA.2008.132&partnerID=40&md5=11c156ab193498d62a70d9798725aad2},
abstract={Recently efficient and scalable live video streaming system over the Internet has become a hot topic. In order to improve the system performance metrics, such as startup delay, source-to-end delay, playback continuity and scalability, many previous works developed two successful cases of Content Distribution Network (CDN) and Peer-to-Peer (P2P) Network for the design of large-scale live video streaming systems, but no single one has yet delivered both the scale and service quality. To combine the advantages of CDN and P2P network has been considered as a feasible orientation for large-scale video stream delivering. In this paper, we propose a peer-assisted content distribution network, i.e. PACDN, which borrows the mesh-based P2P ideas into the traditional CDN to enhance the performance and scalability. The basic features of PACDN include: 1) To meet the real time requirement of live video stream service, i.e. to ensure that the video stream could be continuously and stably delivered from the source to each edge server for offering good QoS to different regions clients, the placement edge servers and source streaming server(s) build a hierarchical multi-tree based and in-hierarchy peer-assisted overlay, which is optimized according to the knowledge of underlying physical topology. This scheme in the design is called "server side peer-assisted". 2) To enhance the system scalability and reduce the deployment cost, clients and edge servers construct a Client/Server based and P2P network assisted overlay with the increasing of viewers, which is called "client side peer-assisted" in this design. We compare the inner performance of PACDN with existing approaches based on comprehensive simulations and analysis. The results show that our proposed design outperforms previous systems in the service quality and scalability. PACDN has been implemented as an Internet live video streaming service and it was successfully deployed for broadcasting many important live programs in China in 2007. The industrial experiences prove that this design is scalable and reliable. We believe that the wide deployment of PACDN and its further development will soon benefit many more Internet users. © 2008 IEEE.},
author_keywords={Content distribution network;  Live video streaming;  Peer-to-peer;  QoS;  Scalability},
keywords={Acoustic streaming;  BASIC (programming language);  Broadcasting;  Client server computer systems;  Distributed parameter networks;  Distribution of goods;  Electric power distribution;  Internet;  Rivers;  Scalability;  Servers;  Video signal processing;  Video streaming;  Videotex, Client/server;  Content distribution network;  Content Distribution Networks;  Edge servers;  Industrial practices;  Information networking;  International conferences;  Internet users;  Live video;  Live video streaming;  P2p networks;  Peer-to-peer;  Peer-to-peer networks;  Performance analyses;  Physical topology;  QoS;  Real-time requirements;  SERVER side;  Service quality;  System performances;  System scalability, Quality of service},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liang2007472,
author={Liang, J. and Gut, X. and Nahrstedt, K.},
title={Self-configuring information management for large-scale service overlays},
journal={Proceedings - IEEE INFOCOM},
year={2007},
pages={472-480},
doi={10.1109/INFCOM.2007.62},
art_number={4215644},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548330012&doi=10.1109%2fINFCOM.2007.62&partnerID=40&md5=477e8f85f193e8555fe24d36edde3680},
abstract={Service overlay networks (SON) provide important infrastructure support for many emerging distributed applications such as web service composition, distributed stream processing, and workflow management. Quality-sensitive distributed applications such as multimedia services and on-line data analysis often desire the SON to provide up-to-date dynamic information about different overlay nodes and overlay links. However, it is a challenging task to provide scalable and efficient information management for large-scale SONs, where both system conditions and application requirements can change over time. In this paper, we present InfoEye, a model-based self-configuring distributed information management system that consists of a set of monitoring sensors deployed on different overlay nodes. InfoEye can dynamically configure the operations of different sensors based on current statistical application query patterns and system attribute distributions. Thus, InfoEye can greatly improve the scalability of SON by answering information queries with minimum monitoring overhead. We have implemented a prototype of InfoEye and evaluated its performance using both extensive simulations and micro-benchmark experiments on PlanetLab. The experimental results show that InfoEye can significantly reduce the information management overhead compared with existing approaches. In addition, InfoEye can quickly reconfigure itself in response to application requirement and system information pattern changes. © 2007 IEEE.},
keywords={Computer simulation;  Information management;  Large scale systems;  Quality of service;  Software prototyping, Large-scale SON;  Up-to-date dynamic information, Telecommunication networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2019705,
author={Zhang, S. and Zhou, A.C. and He, J. and He, B.},
title={BriskStream: Scaling data stream processing on shared-memory multicore architectures},
journal={Proceedings of the ACM SIGMOD International Conference on Management of Data},
year={2019},
pages={705-722},
doi={10.1145/3299869.3300067},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069524668&doi=10.1145%2f3299869.3300067&partnerID=40&md5=3cb606f415e53bda0ce5c1226c4632ad},
abstract={We introduce BriskStream, an in-memory data stream processing system (DSPSs) specifically designed for modern shared-memory multicore architectures. BriskStream's key contribution is an execution plan optimization paradigm, namely RLAS, which takes relative-location (i.e., NUMA distance) of each pair of producer-consumer operators into consideration. We propose a branch and bound based approach with three heuristics to resolve the resulting nontrivial optimization problem. The experimental evaluations demonstrate that BriskStream yields much higher throughput and better scalability than existing DSPSs on multi-core architectures when processing different types of workloads. © 2019 Association for Computing Machinery.},
keywords={Data handling;  Optimization;  Software architecture, Data stream processing;  Execution plans;  Experimental evaluation;  Multicore architectures;  Nontrivial optimization;  Relative location;  Shared memory, Memory architecture},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mossel201743,
author={Mossel, A. and Kroeter, M.},
title={Streaming and Exploration of Dynamically Changing Dense 3D Reconstructions in Immersive Virtual Reality},
journal={Adjunct Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2016},
year={2017},
pages={43-48},
doi={10.1109/ISMAR-Adjunct.2016.0035},
art_number={7836456},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015163176&doi=10.1109%2fISMAR-Adjunct.2016.0035&partnerID=40&md5=8886a58b1db6e198c2fba44e9976d8db},
abstract={We introduce a novel framework that enables large-scale dense 3D scene reconstruction, data streaming over the network and immersive exploration of the reconstructed environment using virtual reality. The system is operated by two remote entities, where one entity - for instance an autonomous aerial vehicle - captures and reconstructs the environment as well as transmits the data to another entity - such as human observer - that can immersivly explore the 3D scene, decoupled from the view of the capturing entity. The performance evaluation revealed the framework's capabilities to perform RGB-D data capturing, dense 3D reconstruction, streaming and dynamic scene updating in real time for indoor environments up to a size of 100m2, using either a state-of-the-art mobile computer or a workstation. Thereby, our work provides a foundation for enabling immersive exploration of remotely captured and incrementally reconstructed dense 3D scenes, which has not shown before and opens up new research aspects in future. © 2016 IEEE.},
author_keywords={and virtual realities;  augmented;  H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;  I.4.8 [Computing Methodologies]: Image Processing and Computer Vision-Scene Analysis},
keywords={Augmented reality;  Computer workstations;  Image processing;  Image reconstruction;  Three dimensional computer graphics, 3D reconstruction;  3D scene reconstruction;  augmented;  H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial;  Image processing and computer vision;  Immersive virtual reality;  Indoor environment;  State of the art, Virtual reality},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{HoseinyFarahabady2016334,
author={Hoseiny Farahabady, M.R. and Dehghani Samani, H.R. and Wang, Y. and Zomaya, A.Y. and Tari, Z.},
title={A QoS-aware controller for Apache Storm},
journal={Proceedings - 2016 IEEE 15th International Symposium on Network Computing and Applications, NCA 2016},
year={2016},
pages={334-342},
doi={10.1109/NCA.2016.7778638},
art_number={7778638},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010432779&doi=10.1109%2fNCA.2016.7778638&partnerID=40&md5=872c2546d230414069633724556796f2},
abstract={Apache Storm has recently emerged as an attractive fault-tolerant open-source distributed data processing platform that has been chosen by many industry leaders to develop real-time applications for processing a huge amount of data in a scalable manner. A key aspect to achieve the best performance in this system lies on the design of an efficient scheduler for component execution, called topology, on the available computing resources. In response to workload fluctuations, we propose an advanced scheduler for Apache Storm that provides improved performance with highly dynamic behavior. While enforcing the required Quality-of-Service (QoS) of individual data streams, the controller allocates computing resources based on decisions that consider the future states of non-controllable disturbance parameters, e.g. arriving rate of tuples or resource utilization in each worker node. The performance evaluation is carried out by comparing the proposed solution with two well-known alternatives, namely the Storm's default scheduler and the best-effort approach (i.e. the heuristic that is based on the first-fit decreasing approximation algorithm). Experimental results clearly show that the proposed controller increases the overall resource utilization by 31% on average compared to the two others solutions, without significant negative impact on the QoS enforcement level. © 2016 IEEE.},
author_keywords={Apache Storm;  Model Predictive Control;  Resource Allocation/Scheduling;  Streaming Data Processing},
keywords={Approximation algorithms;  Controllers;  Data handling;  Model predictive control;  Scheduling;  Storms, Computing resource;  Distributed data processing;  Disturbance parameters;  Dynamic behaviors;  Fault-tolerant;  Real-time application;  Resource utilizations;  Streaming data processing, Quality of service},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suzumura20141321,
author={Suzumura, T. and Nishii, S. and Ganse, M.},
title={Towards large-scale graph stream processing platform},
journal={WWW 2014 Companion - Proceedings of the 23rd International Conference on World Wide Web},
year={2014},
pages={1321-1326},
doi={10.1145/2567948.2580051},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990927903&doi=10.1145%2f2567948.2580051&partnerID=40&md5=20b88f1c55df9f46da08aa32ed27881c},
abstract={In recent years, real-time data mining for large-scale timeevolving graphs is becoming a hot research topic. Most of the prior arts target relatively static graphs and also process them in store-and-process batch processing model. In this paper we propose a method of applying on-the-fly and incremental graph stream computing model to such dynamic graph analysis. To process large-scale graph streams on a cluster of nodes dynamically in a scalable fashion, we propose an incremental large-scale graph processing model called "Incremental GIM-V (Generalized Iterative Matrix-Vector Multiplication)". We also design and implement UNICORN, a system that adopts the proposed incremental processing model on top of IBM InfoSphere Streams. Our performance evaluation demonstrates that our method achieves up to 48% speedup on PageRank with Scale 16 Log-normal Graph (vertexes=65,536, edges=8,364,525) with 4 nodes, 3023% speedup on Random walk with Restart with Kronecker Graph with Scale 18 (vertexes=262,144, edges=8,388,608) with 4 nodes against original GIM-V. © Copyright 2014 by the International World Wide Web Conferences Steering Committee.},
author_keywords={Data stream management system;  Distributed computing;  DSMS;  Graph algorithms;  Page rank;  Random walk with restart},
keywords={Batch data processing;  Data mining;  Database systems;  Distributed computer systems;  Information management;  Iterative methods;  Random processes;  World Wide Web, Data stream management systems;  DSMS;  Graph algorithms;  Page ranks;  Random walk with restart, Graph theory},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Simoncelli201330,
author={Simoncelli, D. and Gringoli, F. and Dusi, M. and Niccolini, S.},
title={Stream-monitoring with blockmon: Convergence of network measurements and data analytics platforms?},
journal={Computer Communication Review},
year={2013},
volume={43},
number={2},
pages={30-35},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002655794&partnerID=40&md5=36df7e240342d0502cd7c1dd1f537c31},
abstract={Recent work in network measurements focuses on scaling the performance of monitoring platforms to 10Gb/s and beyond. Concurrently, IT community focuses on scaling the analysis of big-data over a cluster of nodes. So far, combinations of these approaches have targeted flexibility and usability over real-timeliness of results and efficient allocation of resources. In this paper we show how to meet both objectives with BlockMon, a network monitoring platform originally designed to work on a single node, which we extended to run distributed stream-data analytics tasks. We compare its performance against Storm and Apache S4, the state-ofthe-art open-source stream-processing platforms, by implementing a phone call anomaly detection system and a Twitter trending algorithm: our enhanced BlockMon has a gain in performance of over 2.5x and 23x, respectively. Given the different nature of those applications and the performance of BlockMon as single-node network monitor [1], we expect our results to hold for a broad range of applications, making distributed BlockMon a good candidate for the convergence of network-measurement and IT-analysis platforms.},
author_keywords={Data analysis;  Distributed computing;  Performance analysis},
keywords={Data reduction;  Distributed computer systems;  Open systems, Anomaly detection systems;  Convergence of networks;  Efficient allocations;  Monitoring platform;  Network Monitoring;  Network monitors;  Performance analysis;  Stream processing, Big data},
publisher={Association for Computing Machinery},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chen2011,
author={Chen, Y. and Zhang, B. and Chen, C.},
title={Modeling and performance analysis of P2P live streaming systems under flash crowds},
journal={IEEE International Conference on Communications},
year={2011},
doi={10.1109/icc.2011.5962881},
art_number={5962881},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052176728&doi=10.1109%2ficc.2011.5962881&partnerID=40&md5=7e274be51a57396c84cf7754a482f23e},
abstract={A fundamental problem that a peer-to-peer (P2P) live streaming system faces is how to support flash crowds effectively. A flash crowd occurs when a burst of join requests arrive at a system. When a flash crowd occurs, the sudden arrival of numerous peers may starve the upload capacity of a P2P system, and degrade the quality of service. By theoretical analysis and simulations, we find that a system has limited capacity to handle a flash crowd: It can recover to a new stable state when the size of flash crowd is small or moderate, but collapse when the flash crowd is excessively large. The capacity of a system is independent of initial state of the system while relevant to stable peers' departure rate, which suggests this capacity is an essential property of a P2P live streaming system. In addition, we prove that a P2P live streaming system with admission control has excellent capacity to handle flash crowds: It can recover from flash crowds of excessively large size and a startup peer's waiting time scales logarithmically with the size of flash crowds. Our theoretical model and simulation results provide a promising framework to understand the capacity of a P2P live streaming system for handling flash crowds. © 2011 IEEE.},
author_keywords={admission control;  flash crowd;  fluid model;  p2p streaming},
keywords={Analysis and simulation;  Flash crowd;  fluid model;  Fundamental problem;  Initial state;  Large sizes;  Limited capacity;  Live streaming;  P2P streaming;  P2P system;  Peer to peer;  Performance analysis;  Stable state;  Theoretical models;  Waiting-time, Computer simulation;  Quality of service;  Video streaming, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brito200922,
author={Brito, A. and Fetzer, C. and Felber, P.},
title={Multithreading-enabled active replication for event stream processing operators},
journal={Proceedings of the IEEE Symposium on Reliable Distributed Systems},
year={2009},
pages={22-31},
doi={10.1109/SRDS.2009.37},
art_number={5283513},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-74949095499&doi=10.1109%2fSRDS.2009.37&partnerID=40&md5=704e7ddacd3abd3b18d0af803f344b8b},
abstract={Event Stream Processing (ESP) systems are very popular in monitoring applications. Algorithmic trading, network monitoring and sensor networks are good examples of applications that rely upon ESP systems. As these systems become larger and more widely deployed, they have to answer increasingly stronger requirements that are often difficult to satisfy. Fault-tolerance is a good example of such a nontrivial requirement. Making ESP operators fault-tolerant can add considerable performance overhead to the application. In this paper, we focus on active replication as an approach to provide fault-tolerance to ESP operators. More precisely, we address the performance costs of active replication for operators in distributed ESP applications. We use a speculation mechanism based on Software Transactional Memory (STM) to achieve the following goals: (i) enable replicas to make progress using optimistic delivery; (ii) enable early forwarding of speculative computation results; (iii) enable active replication of multi-threaded operators using transactional executions. Experimental evaluation shows that, using this combination of mechanisms, one can implement highly efficient fault-tolerant ESP operators. © 2009 IEEE.},
keywords={Active replication;  Algorithmic trading;  Event streams;  Experimental evaluation;  Fault-tolerant;  Monitoring applications;  Multi-threading;  Multithreaded;  Network Monitoring;  Performance costs;  Software transactional memory;  Speculative computation, Fault tolerance;  Hydraulics;  Multitasking, Quality assurance},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bracciale20081,
author={Bracciale, L. and Lo Piccolo, F. and Luzzi, D. and Salsano, S. and Bianchi, G. and Blefari-Melazzi, N.},
title={A push-based scheduling algorithm for large scale P2P live streaming},
journal={Proceedings of the 2008 4th International Telecommunication Networking Workshop on QoS in Multiservice IP Networks, IT-NEWS},
year={2008},
pages={1-7},
doi={10.1109/ITNEWS.2008.4488121},
art_number={4488121},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-50149085066&doi=10.1109%2fITNEWS.2008.4488121&partnerID=40&md5=454b51a125fb0778cf98f8e6604fe082},
abstract={In this paper, we present a chunk scheduling algorithm for a mesh-based peer-to-peer live streaming system and we evaluate it by simulations over large-scale networks. Literature papers typically design chunk scheduling algorithms by considering the chunk delivery ratio as performance metric. We propose a push-based algorithm, which not only tries to maximize the chunk delivery ratio but it also takes into account and tries to minimize the delivery delay of chunks at the peer nodes. This is an important requirement, when dealing with real-time multimedia flows. Another important contribution of this paper is the design and implementation of a simulator able to evaluate the performance of large scale P2P networks (tens of thousands peers). The importance of this contribution lies in the fact that existing simulators and performance studies handle at most hundreds or few thousands of peers, while real-life P2P streaming systems aim at distributing contents to several hundreds of thousands, if not millions, of users. The performance evaluation study aims at providing a comprehensive view of what performance can be expected for mesh-based peer-to-peer streaming systems, both in terms of chunk delivery ratio and delay, for a large range of the number of users. The individual effect of a variety of system parameters, and especially number of partner nodes in the mesh, constrained link bandwidth, node heterogeneity, and network size, has been analyzed. Our results show that performances of the proposed push-based solution are already quite effective even with severely bandwidth constrained large scale networks. © 2008 IEEE.},
keywords={Distributed computer systems;  Internet protocols;  Scheduling;  Simulators;  Telecommunication systems;  Video streaming, Bandwidth constrained;  Delivery delays;  Delivery ratio;  IP-networks;  Large range;  Large-scale networks;  Link bandwidths;  Live streaming;  Multi services;  Network sizes;  P2p networks;  P2P streaming;  Peer nodes;  Peer-to-Peer;  Peer-to-peer streaming;  Performance evaluation;  Performance metric;  Real-time multimedia;  System parameters, Scheduling algorithms},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Walulya2018297,
author={Walulya, I. and Palyvos-Giannas, D. and Nikolakopoulos, Y. and Gulisano, V. and Papatriantafilou, M. and Tsigas, P.},
title={Viper: A module for communication-layer determinism and scaling in low-latency stream processing},
journal={Future Generation Computer Systems},
year={2018},
volume={88},
pages={297-308},
doi={10.1016/j.future.2018.05.067},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048415125&doi=10.1016%2fj.future.2018.05.067&partnerID=40&md5=442ec6ee9f9383f28b2d2a7ac96a677e},
abstract={Stream Processing Engines (SPEs) process continuous streams of data and produce results in a real-time fashion, typically through one-at-a-time tuple analysis. In Fog architectures, the limited resources of the edge devices, enabling close-to-the-source scalable analysis, demand for computationally- and energy-efficient SPEs. When looking into the vital SPE processing properties required from applications, determinism, which ensures consistent results independently of the way the analysis is parallelized, has a strong position besides scalability in throughput and low processing latency. SPEs scale in throughput and latency by relying on shared-nothing parallelism, deploying multiple copies of each operator to which tuples are distributed based on its semantics. The coordination of the asynchronous analysis of parallel operators required to enforce determinism is then carried out by additional dedicated sorting operators. To prevent this costly coordination from becoming a bottleneck, we introduce the Viper communication module, which can be integrated in the SPE communication layer and boost the coordination of the parallel threads analyzing the data. Using Apache Storm and data extracted from the Linear Road benchmark and a real-world smart grid system, we show benefits in the throughput, latency and energy efficiency coming from the utilization of the Viper module. © 2018 Elsevier B.V.},
author_keywords={Data parallelism;  Data streaming;  Determinism;  Low-latency;  Shared-nothing and shared-memory parallelism;  Stream processing engines},
keywords={Engines;  Memory architecture;  Semantics;  Smart power grids;  Throughput, Data parallelism;  Data streaming;  Determinism;  Low latency;  Shared memory parallelism;  Stream processing engines, Energy efficiency},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nagasu2017153,
author={Nagasu, K. and Sano, K. and Kono, F. and Nakasato, N.},
title={FPGA-based tsunami simulation: Performance comparison with GPUs, and roofline model for scalability analysis},
journal={Journal of Parallel and Distributed Computing},
year={2017},
volume={106},
pages={153-169},
doi={10.1016/j.jpdc.2016.12.015},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009284694&doi=10.1016%2fj.jpdc.2016.12.015&partnerID=40&md5=ff13309d023d4f9f4e3362f9f247c643},
abstract={MOST (Method Of Splitting Tsunami) is widely used to solve shallow water equations (SWEs) for simulation of tsunami. This paper presents high-performance and power-efficient computation of MOST for practical tsunami simulation with FPGA. The custom hardware for simulation is based on a stream computing architecture for deeply pipelining to increase performance with a limited bandwidth. We design a stream processing element (SPE) of computing kernels combined with stencil buffers. We also introduce an SPE array architecture with spatial and temporal parallelism to further exploit available hardware resources by implementing multiple SPEs with parallel internal pipelines. Our prototype implementation with Arria 10 FPGA demonstrates that the FPGA-based design performs numerically stable tsunami simulation with real ocean-depth data in single precision by introducing non-dimensionalization. We explore the design space of SPE arrays, and find that the design of six cascaded SPEs with a single pipeline achieves the sustained performance of 383 GFlops and the performance per power of 8.41 GFlops/W with a stream bandwidth of only 7.2 GB/s. These numbers are 8.6 and 17.2 times higher than those of NVidia Tesla K20c GPU, and 1.7 and 7.1 times higher than those of AMD Radeon R9 280X GPU, respectively, for the same tsunami simulation in single precision. Moreover, we proposed a roofline model for stream computing with the SPE array in order to investigate factors of performance degradation and possible performance improvement for given FPGAs. With the model, we estimate that an upcoming Stratix 10 GX2800 FPGA can achieve the sustained performance of 8.7 TFlops at most with our SPE array architecture for tsunami simulation. © 2016 Elsevier Inc.},
author_keywords={Custom hardware;  FPGA;  GPU;  Roofline model;  Stream computing;  Tsunami simulation},
keywords={Architecture;  Bandwidth;  Computer architecture;  Computer hardware;  Computer hardware description languages;  Equations of motion;  Field programmable gate arrays (FPGA);  Graphics processing unit;  Pipelines;  Program processors;  Tsunamis, Custom hardwares;  Performance comparison;  Performance degradation;  Prototype implementations;  Roofline models;  Shallow water equation (SWEs);  Stream computing;  Tsunami simulation, Integrated circuit design},
publisher={Academic Press Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Korkhov2007173,
author={Korkhov, V. and Vasyunin, D. and Wibisono, A. and Belloum, A.S.Z. and Inda, M.A. and Roos, M. and Breit, T.M. and Hertzberger, L.O.},
title={VLAM-G: Interactive data driven workflow engine for Grid-enabled resources},
journal={Scientific Programming},
year={2007},
volume={15},
number={3},
pages={173-188},
doi={10.1155/2007/812036},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048947329&doi=10.1155%2f2007%2f812036&partnerID=40&md5=27ea8f4108b19ba7ab7d76ef25bae4bb},
abstract={Grid brings the power of many computers to scientists. However, the development of Grid-enabled applications requires knowledge about Grid infrastructure and low-level API to Grid services. In turn, workflow management systems provide a high-level environment for rapid prototyping of experimental computing systems. Coupling Grid and workflow paradigms is important for the scientific community: it makes the power of the Grid easily available to the end user. The paradigm of data driven workflow execution is one of the ways to enable distributed workflow on the Grid. The work presented in this paper is carried out in the context of the Virtual Laboratory for e-Science project. We present the VLAM-G workflow management system and its core component: the Run-Time System (RTS). The RTS is a dataflow driven workflow engine which utilizes Grid resources, hiding the complexity of the Grid from a scientist. Special attention is paid to the concept of dataflow and direct data streaming between distributed workflow components. We present the architecture and components of the RTS, describe the features of VLAM-G workflow execution, and evaluate the system by performance measurements and a real life use case. © 2007 - IOS Press and the authors. All rights reserved.},
author_keywords={Data driven workflow engine;  Direct data streaming;  Grid},
keywords={Data flow analysis;  Knowledge based systems;  Search engines;  Software architecture;  Software prototyping, Data streaming;  Interactive data;  Workflow management systems, Grid computing},
publisher={Hindawi Limited},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang2006,
author={Zhang, L. and Parashar, M.},
title={Enabling efficient and flexible coupling of parallel scientific applications},
journal={20th International Parallel and Distributed Processing Symposium, IPDPS 2006},
year={2006},
volume={2006},
doi={10.1109/IPDPS.2006.1639354},
art_number={1639354},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847144143&doi=10.1109%2fIPDPS.2006.1639354&partnerID=40&md5=89c0cf8577009ef35321048de66d0144},
abstract={Emerging scientific and engineering simulations are presenting challenging requirements for coupling between multiple physics models and associated parallel codes that execute independently and in a distributed manner. Realizing coupled simulations requires an efficient, flexible and scalable coupling framework and simple programming abstractions. This paper presents a coupling framework that addresses these requirements. The framework is based on the Seine geometry-based interaction model. It enables efficient computation of communication schedules, supports low-overheads processor-to-processor data streaming, and provides high-level abstraction for application developers. The design, CCA-based implementation, and experimental evaluation of the Seine based coupling framework are presented. © 2006 IEEE.},
keywords={Abstracting;  Computation theory;  Computer programming;  Computer simulation;  Data processing;  Mathematical models, Data streaming;  Parallel codes;  Programming abstractions;  Scalable coupling framework, Parallel processing systems},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kreutzer2015417,
author={Kreutzer, M. and Pieper, A. and Hager, G. and Wellein, G. and Alvermann, A. and Fehske, H.},
title={Performance Engineering of the Kernel Polynomal Method on Large-Scale CPU-GPU Systems},
journal={Proceedings - 2015 IEEE 29th International Parallel and Distributed Processing Symposium, IPDPS 2015},
year={2015},
pages={417-426},
doi={10.1109/IPDPS.2015.76},
art_number={7161530},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971385371&doi=10.1109%2fIPDPS.2015.76&partnerID=40&md5=2e2e6b00a0d00fd81deb524b3d990de3},
abstract={The Kernel Polynomial Method (KPM) is a well-established scheme in quantum physics and quantum chemistry to determine the Eigen value density and spectral properties of large sparse matrices. In this work we demonstrate the high optimization potential and feasibility of peta-scale heterogeneous CPU-GPU implementations of the KPM. At the node level we show that it is possible to decouple the sparse matrix problem posed by KPM from main memory bandwidth both on CPU and GPU. To alleviate the effects of scattered data access we combine loosely coupled outer iterations with tightly coupled block sparse matrix multiple vector operations, which enables pure data streaming. All optimizations are guided by a performance analysis and modelling process that indicates how the computational bottlenecks change with each optimization step. Finally we use the optimized node-level KPM with a hybrid-parallel framework to perform large-scale heterogeneous electronic structure calculations for novel topological materials on a pet scale-class Cray XC30 system. © 2015 IEEE.},
author_keywords={Parallel programming;  Performance analysis;  Quantum mechanics;  Sparse matrices},
keywords={Electronic structure;  Hybrid materials;  Parallel programming;  Quantum chemistry;  Quantum theory, Computational bottlenecks;  Electronic structure calculations;  Optimization potential;  Performance analysis;  Performance engineering;  Sparse matrices;  Spectral properties;  Topological materials, Matrix algebra},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dayarathna2013225,
author={Dayarathna, M. and Suzumura, T.},
title={A performance analysis of system S, S4, and Esper via two level benchmarking},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={8054 LNCS},
pages={225-240},
doi={10.1007/978-3-642-40196-1_19},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882741975&doi=10.1007%2f978-3-642-40196-1_19&partnerID=40&md5=bbe6cd96ad8f1bf4044ca57715e2e098},
abstract={Data stream processing systems have become popular due to their effectiveness in applications in large scale data stream processing scenarios. This paper compares and contrasts performance characteristics of three stream processing softwares System S, S4, and Esper. We study about which software aspects shape the characteristics of the workloads handled by these software. We use a micro benchmark and different real world stream applications on System S, S4, and Esper to construct 70 different application scenarios. We use job throughput, CPU, Memory consumption, and network utilization of each application scenario as performance metrics. We observed that S4's architectural aspect which instantiates a Processing Element (PE) for each keyed attribute is less efficient compared to the fixed number of PEs used by System S and Esper. Furthermore, all the Esper benchmarks produced more than 150% increased performance in single node compared to S4 benchmarks. S4 and Esper are more portable compared to System S and could be fine tuned for different application scenarios easily. In future we hope to widen our understanding of performance characteristics of these systems by investigating in to the code level profiling. © 2013 Springer-Verlag.},
author_keywords={benchmarking;  data-intensive computing;  performance analysis;  stream processing;  systems scalability;  workload characterization},
keywords={Application scenario;  Data stream processing;  Data-intensive computing;  Net work utilization;  Performance analysis;  Performance characteristics;  Stream processing;  Workload characterization, Data communication systems, Benchmarking},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Purandare20071633,
author={Purandare, D. and Guha, R.},
title={An alliance based peering scheme for P2P live media streaming},
journal={IEEE Transactions on Multimedia},
year={2007},
volume={9},
number={8},
pages={1633-1644},
doi={10.1109/TMM.2007.907453},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348992075&doi=10.1109%2fTMM.2007.907453&partnerID=40&md5=4f5aaf36321a00090e85c2da4d4f6198},
abstract={While recent measurement studies have shown the effectiveness of P2P network in media streaming, there have been questions raised about the Quality of Service (QoS), reliability of streaming services and sub optimal uplink utilization in particular. P2P streaming systems are inherently less reliable because of churn, internet dynamics, node heterogeneity and randomness in the swarm. We present a new model for P2P media streaming based on clustering of peers, called alliances. We show that alliance formation is a loosely coupled and an effective way to organize the peers. We show that our model maps to a "small-world" network, which form efficient overlay structures and are robust to network perturbations such as churn. We present a comparative simulation based study of our model with CoolStreaming/DONet and present a quantitative performance evaluation. Simulation results are promising and show that our model scales well under varying workloads and conditions, delivers near optimal levels of QoS, and for most cases, performs at par or even better than Cool-Streaming/DONet. © 2007 IEEE.},
author_keywords={Media streaming;  Peer-to-peer;  Quality of service;  Small world network;  Video on demand},
keywords={Media streaming;  Node heterogeneity;  Peer-to-peer;  Small world networks;  Streaming services, Clustering algorithms;  Computer simulation;  Quality of service;  Reliability;  Video on demand;  Video streaming, Distributed computer systems},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Vikash2020207,
author={Vikash and Mishra, L. and Varma, S.},
title={Performance evaluation of real-time stream processing systems for Internet of Things applications},
journal={Future Generation Computer Systems},
year={2020},
volume={113},
pages={207-217},
doi={10.1016/j.future.2020.07.012},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087799195&doi=10.1016%2fj.future.2020.07.012&partnerID=40&md5=753e6ccdb204112b1e769bd28bf7c056},
abstract={In the current scenario, IoT is an ideal and novel technology, which fulfills the needs of most of the commercial, non-commercial, government, and private organizations by its real-time supportive nature and characteristics. However, real-time processing itself a very critical research topic. But, most of the IoT applications are empowered by real-time data processing. Thus, it become a vital part of IoT. In this work, we proposed a four-layer infrastructure for IoT along with stream processing. Further, we use stream processing techniques along with IoT infrastructure for applications and analyze the performance of stream processing techniques for IoT applications. Also, we compare and find the five most suitable distributed stream processing systems for IoT, based on its performance and characteristics. We use two benchmark applications to evaluate the performance of distributed stream processing systems against response time, throughput, jitter, and scalability. Based on that, we suggest the adapted solution for IoT applications. We evaluate the performance with peak stream rates from 100k to 1M along with the various frequencies of benchmark applications. Further, on the basis of results, we conclude that Apache NiFi is the most suitable solution for IoT applications. © 2020 Elsevier B.V.},
author_keywords={Internet of Things;  Middleware;  Pervasive computing;  Real-time processing;  Stream processing;  Wireless sensor networks},
keywords={Benchmarking;  Data handling;  Distributed parameter control systems;  Real time systems, Benchmark applications;  Critical researches;  Distributed stream processing;  Private organizations;  Real-time data processing;  Real-time streams;  Realtime processing;  Suitable solutions, Internet of things},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mortazavi-Dehkordi2020241,
author={Mortazavi-Dehkordi, M. and Zamanifar, K.},
title={Efficient deadline-aware scheduling for the analysis of Big Data streams in public Cloud},
journal={Cluster Computing},
year={2020},
volume={23},
number={1},
pages={241-263},
doi={10.1007/s10586-019-02908-2},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061563953&doi=10.1007%2fs10586-019-02908-2&partnerID=40&md5=e6e9e883957ce9d2416bb5e36e48fd60},
abstract={The emergence of Big Data has had a profound impact on how data are analyzed. Open source distributed stream processing platforms have gained popularity for analyzing streaming Big Data as they provide low latency required for streaming Big Data applications using Cloud resources. However, existing resource schedulers are still lacking the efficiency and deadline meeting that Big Data analytical applications require. Recent works have already considered streaming Big Data characteristics to improve the efficiency and the likelihood of deadline meeting for scheduling in the platforms. Nevertheless, they have not taken into account the specific attributes of analytical application, public Cloud utilization cost and delays caused by performance degradation of leasing public Cloud resources. This study, therefore, presents BCframework, an efficient deadline-aware scheduling framework used by streaming Big Data analysis applications based on public Cloud resources. BCframework proposes a scheduling model which considers public Cloud utilization cost, performance variation, deadline meeting and latency reduction requirements of streaming Big Data analytical applications. Furthermore, it introduces two operator scheduling algorithms based on both a novel partitioning algorithm and an operator replication method. BCframework is highly adaptable to the fluctuation of streaming Big Data and the performance degradation of public Cloud resources. Experiments with the benchmark and real-world queries show that BCframework can significantly reduce the latency and utilization cost and also minimize deadline violations and provisioned virtual machine instances. © 2019, The Author(s).},
author_keywords={Cloud-based stream processing;  Deadline-aware scheduling;  Streaming Big Data analysis query},
keywords={Cost reduction;  Data handling;  Distributed parameter control systems;  Efficiency;  Information analysis;  Scheduling;  Scheduling algorithms, Analytical applications;  Big data applications;  Deadline-aware scheduling;  Distributed stream processing;  Partitioning algorithms;  Performance degradation;  Performance variations;  Stream processing, Big data},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gao2019376,
author={Gao, L. and Tang, M. and Pang, H. and Huang, J. and Sun, L.},
title={Multi-User Cooperative Mobile Video Streaming: Performance Analysis and Online Mechanism Design},
journal={IEEE Transactions on Mobile Computing},
year={2019},
volume={18},
number={2},
pages={376-389},
doi={10.1109/TMC.2018.2834358},
art_number={8356092},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046728192&doi=10.1109%2fTMC.2018.2834358&partnerID=40&md5=bb45ce74a087f71d309f216208c7ae4a},
abstract={Adaptive bitrate streaming enables video users to adapt their playing bitrates to the real-time network conditions, hence achieving the desirable quality-of-experience (QoE). In a multi-user wireless scenario, however, existing single-user based bitrate adaptation methods may fail to provide the desirable QoE, due to lack of consideration of multi-user interactions (such as the multi-user interferences and network congestion). In this work, we propose a novel user cooperation framework based on user-provided networking for multi-user mobile video streaming over wireless cellular networks. The framework enables nearby mobile video users to crowdsource their cellular links and resources for cooperative video streaming. We first analyze the social welfare performance bound of the proposed cooperative streaming system by introducing a virtual time-slotted system. Then, we design a low complexity Lyapunov-based online algorithm, which can be implemented in an online and distributed manner without the complete future and global network information. Numerical results show that the proposed online algorithm achieves an average 97 percent of the theoretical maximum social welfare. We further conduct experiments with real data traces, to compare our proposed online algorithm with the existing online algorithms in the literature. Experiment results show that our algorithm outperforms the existing algorithms in terms of both the achievable bitrate (with an average gain of 20 \sim 30 percent) and social welfare (with an average gain of 10 \sim 50 percent). © 2002-2012 IEEE.},
author_keywords={adaptive bitrate;  mobile crowdsourcing;  Mobile video streaming;  online algorithm},
keywords={Cellular telephone systems;  Distributed computer systems;  Interactive computer systems;  Internet;  Machine design;  Online systems;  Quality of service;  Real time systems;  Smartphones;  Video streaming, Bit rates;  Device-to-Device communications;  Mobile crowdsourcing;  Mobile video streaming;  On-line algorithms;  Quality of experience (QoE);  Streaming media, Media streaming},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{GhaffariSheshjavani20177535,
author={Ghaffari Sheshjavani, A. and Akbari, B.},
title={An adaptive buffer-map exchange mechanism for pull-based peer-to-peer video-on-demand streaming systems},
journal={Multimedia Tools and Applications},
year={2017},
volume={76},
number={5},
pages={7535-7561},
doi={10.1007/s11042-016-3425-z},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960420716&doi=10.1007%2fs11042-016-3425-z&partnerID=40&md5=27839ebc35f5cf8ae1ec87824c5d210f},
abstract={Unlike P2P live video streaming in which all the peers in a channel watch a video with tiny differences in viewing points, in P2P video on demand (VoD) streaming systems, neighbor peers may watch the same video with more different viewing points; therefore, using push-based approach is not efficient for such systems, and the overhead of the pull-based approaches is challenging due to the periodical exchange of buffer-maps among the peers. In pull-based P2P VoD systems, to achieve better quality of experience it is necessary to use large buffers at the peers that results in more buffer-maps exchange overhead. In this paper, we study buffer-map exchange challenging in pull-based P2P VoD streaming systems and propose an adaptive mechanism for decreasing overhead by sending the buffer-maps with regard to the viewing points of the peers. Bandwidth overhead of the proposed mechanism is independent of the used buffer sizes and is less dependent to the buffer-map exchange period. By using this effective mechanism, better quality of service can be achieved through using large buffers at the peers, without increasing in the overhead. Our simulation based performance evaluation shows the efficiency of the proposed mechanism in decreasing the bandwidth overhead of buffer-map exchange in P2P VoD streaming systems. © 2016, Springer Science+Business Media New York.},
author_keywords={Buffer-map exchange;  Peer-to-Peer;  Video streaming;  Video-on-demand},
keywords={Bandwidth;  Distributed computer systems;  Peer to peer networks;  Quality of service;  Video streaming;  Watches, Adaptive mechanism;  Bandwidth overheads;  Effective mechanisms;  Live video streaming;  On-demand streaming;  Peer to peer;  Quality of experience (QoE);  Video on demands (VoD), Video on demand},
publisher={Springer New York LLC},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hazra201143,
author={Hazra, J. and Das, K. and Seetharam, D.P. and Singhee, A.},
title={Stream computing based synchrophasor application for power grids},
journal={HiPCNA-PG'11 - Proceedings of the 1st International Workshop on High Performance Computing, Networking and Analytics for the Power Grid, Co-located with SC'11},
year={2011},
pages={43-49},
doi={10.1145/2096123.2096134},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857293265&doi=10.1145%2f2096123.2096134&partnerID=40&md5=c9d8a06a6747e870ffaf4fa3364f9cae},
abstract={This paper proposes an application of stream computing analytics framework to high speed synchrophasor data for real time monitoring and control of electric grid. High volume streaming synchrophasor data from geographically distributed grid sensors (namely, Phasor Measurement Units) are collected, synchronized, aggregated when required and analyzed using a stream computing platform to estimate the grid stability in real time. This real time stability monitoring scheme will help the grid operators to take preventive or corrective measures ahead of time to mitigate any disturbance before they develop into wide-spread. A protptype of the scheme is demonstrated on a benchmark 3 machines 9 bus system and the IEEE 14 bus test system. © 2011 ACM.},
author_keywords={power grid;  stream computing;  synchrophasor;  voltage stability},
keywords={Bus systems;  Corrective measures;  Electric grids;  Grid operators;  Grid stability;  Phasor measurement unit;  Power grids;  Real time;  Real time monitoring;  Real-time stability monitoring;  Stream computing;  Synchrophasors;  Test systems, Voltage control, Electric power distribution},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Repantis2008346,
author={Repantis, T. and Kalogeraki, V.},
title={Hot-spot prediction and alleviation in distributed stream processing applications},
journal={Proceedings of the International Conference on Dependable Systems and Networks},
year={2008},
pages={346-355},
doi={10.1109/DSN.2008.4630103},
art_number={4630103},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-53349144457&doi=10.1109%2fDSN.2008.4630103&partnerID=40&md5=3507fdf120510ed3e688ec8b1c7e56ee},
abstract={Many emerging distributed applications require the real-time processing of large amounts of data that are being updated continuously. Distributed stream processing systems offer a scalable and efficient means of in-network processing of such data streams. However, the large scale and the distributed nature of such systems, as well as the fluctuation of their load render it difficult to ensure that distributed stream processing applications meet their Quality of Service demands. We describe a decentralized framework for proactively predicting and alleviating hot-spots in distributed stream processing applications in real-time. We base our hot-spot prediction techniques on statistical forecasting methods, while for hot-spot alleviation we employ a non-disruptive component migration protocol. The experimental evaluation of our techniques, implemented in our Synergy distributed stream processing middleware over PlanetLab, using a real stream processing application operating on real streaming data, demonstrates high prediction accuracy and substantial performance benefits. © 2008 IEEE.},
keywords={Data streaming;  Dependable systems;  Distributed applications;  Distributed stream processing;  Experimental evaluations;  Hot spotting;  Hot-spots;  In-network processing;  International conferences;  Large amounts of data;  Migration protocols;  Performance benefits;  Prediction accuracy;  Prediction techniques;  Real streaming;  Real-time processing;  Statistical forecasting;  Stream processing, Ad hoc networks;  Applications;  Computer networks;  Data storage equipment;  Distributed parameter control systems;  Electric load forecasting;  Forecasting;  Middleware;  Network protocols;  Quality of service;  Rivers;  Sensor networks;  Statistical methods, Data processing},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gill2006289,
author={Gill, G. and Hansen, J. and Singh, M.},
title={Loop pipelining for high-throughput stream computation using self-timed rings},
journal={IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD},
year={2006},
pages={289-296},
doi={10.1109/ICCAD.2006.320135},
art_number={4110188},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46149100071&doi=10.1109%2fICCAD.2006.320135&partnerID=40&md5=38d2aa593a6e2ec001b7e8c3b5f0980d},
abstract={We present a technique for increasing the throughput of stream processing architectures by removing the bottlenecks caused by loop structures. We implement loops as self-timed pipelined rings that can operate on multiple data sets concurrently. Our contribution includes a transformation algorithm which takes as input a high-level program and gives as output the structure of an optimized pipeline ring. Our technique handles nested loops and is further enhanced by loop unrolling. Simulations run on benchmark examples show a 1.3 to 4.9x speedup without unrolling and a 2.6 to 9.7x speedup with twofold loop unrolling. Copyright 2006 ACM.},
keywords={Design;  Mathematical transformations;  Pipelines, Computer-aided design;  High-level program;  High-throughput (HT);  international conferences;  loop pipelining;  Loop structures;  Loop unrolling;  Multiple data;  Nested loops;  Stream processing;  transformation algorithms, Throughput},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Enzler200563,
author={Enzler, R. and Plessl, C. and Platzner, M.},
title={System-level performance evaluation of reconfigurable processors},
journal={Microprocessors and Microsystems},
year={2005},
volume={29},
number={2-3},
pages={63-73},
doi={10.1016/j.micpro.2004.06.004},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-14744299741&doi=10.1016%2fj.micpro.2004.06.004&partnerID=40&md5=f7d1681b22f6e36edd46058424017174},
abstract={Reconfigurable architectures that tightly integrate a standard CPU core with a field-programmable hardware structure have recently been receiving increased attention. The design of such a hybrid reconfigurable processor involves a multitude of design decisions regarding the field-programmable structure as well as its system integration with the CPU core. Determining the impact of these design decisions on the overall system performance is a challenging task. In this paper, we first present a framework for the cycle-accurate performance evaluation of hybrid reconfigurable processors on the system level. Then, we discuss a reconfigurable processor for data-streaming applications, which attaches a coarse-grained reconfigurable unit to the coprocessor interface of a standard embedded CPU core. By means of a case study we evaluate the system-level impact of certain design features for the reconfigurable unit, such as multiple contexts, register replication, and hardware context scheduling. The results illustrate that a system-level evaluation framework is of paramount importance for studying the architectural trade-offs and optimizing design parameters for reconfigurable processors. © 2004 Elsevier B.V. All rights reserved.},
author_keywords={Co-simulation;  Field-programmable gate arrays;  Hybrid reconfigurable processors;  Reconfigurable computing;  VHDL},
keywords={Computer hardware;  Computer simulation;  Embedded systems;  Field programmable gate arrays;  High level languages;  Large scale systems;  Mapping;  Scheduling;  Systems analysis, Co-simulation;  Hybrid reconfigurable processors;  Reconfigurable computing;  VHDL, Microprocessor chips},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chaudhari2019237,
author={Chaudhari, A.A. and Mulay, P.},
title={SCSI: Real-Time Data Analysis with Cassandra and Spark},
journal={Studies in Big Data},
year={2019},
volume={43},
pages={237-264},
doi={10.1007/978-981-13-0550-4_11},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090891458&doi=10.1007%2f978-981-13-0550-4_11&partnerID=40&md5=636d696d1fac26c5f658964f9ee66551},
abstract={The open-source framework for stream processing and enormous informationIn-memory handling model executed with the machine learning algorithmsThe data used in subset of non-distributed mode is better than using all data in distributed modeThe Apache Spark platform handles big data sets with immaculate parallel speedup. Abstract The dynamic progress in the nature of pervasive computing datasets has been main motivation for development of the NoSQL model. The devices having capability of executing “Internet of Things” (IoT) concepts are producing massive amount of data in various forms (structured and unstructured). To handle this IoT data with traditional database schemes is impracticable and expensive. The large-scale unstructured data required as the prerequisites for a preparing pipeline, which flawlessly consolidating the NoSQL storage model such as Apache Cassandra and a Big Data processing platform such as Apache Spark. The Apache Spark is the data-intensive computing paradigm, which allows users to write the applications in various high-level programming languages including Java, Scala, R, Python, etc. The Spark Streaming module receives live input data streams and divides that data into batches by using the Map and Reduce operations. This research presents a novel and scalable approaches called "Smart Cassandra Spark Integration (SCSI)” for solving the challenge of integrating NoSQL data stores like Apache Cassandra with Apache Spark to manage distributed systems based on varied platter of amalgamation of current technologies, IT enabled devices, etc., while eliminating complexity and risk. In this chapter, for performance evaluations, SCSI Streaming framework is compared with the file system-based data stores such as Hadoop Streaming framework. SCSI framework proved scalable, efficient, and accurate while computing big streams of IoT data. © 2019, Springer Nature Singapore Pte Ltd.},
author_keywords={Apache Cassandra;  Apache Spark;  Big data;  IoT;  MapReduce},
keywords={Data integration;  Digital storage;  High level languages;  Information management;  Internet of things;  Machine learning;  Metals;  Open source software, Apache cassandrum;  Apache spark;  Cassandras;  Data store;  Handling models;  Internet of thing”;  Map-reduce;  Open source frameworks;  Real time data analysis;  Stream processing, MapReduce},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Hoffmann201895,
author={Hoffmann, M. and Lattuada, A. and Liagouris, J. and Kalavri, V. and Dimitrova, D. and Wicki, S. and Chothia, Z. and Roscoe, T.},
title={Snailtrail: Generalizing critical paths for online analysis of distributed dataflows},
journal={Proceedings of the 15th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2018},
year={2018},
pages={95-110},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076789014&partnerID=40&md5=8896698b89ac896cced57926107424c3},
abstract={We rigorously generalize critical path analysis (CPA) to long-running and streaming computations and present SnailTrail, a system built on Timely Dataflow, which applies our analysis to a range of popular distributed dataflow engines. Our technique uses the novel metric of critical participation, computed on time-based snapshots of execution traces, that provides immediate insights into specific parts of the computation. This allows SnailTrail to work online in real-time, rather than requiring complete offline traces as with traditional CPA. It is thus applicable to scenarios like model training in machine learning, and sensor stream processing. SnailTrail assumes only a highly general model of dataflow computation (which we define) and we show it can be applied to systems as diverse as Spark, Flink, TensorFlow, and Timely Dataflow itself. We further show with examples from all four of these systems that SnailTrail is fast and scalable, and that critical participation can deliver performance analysis and insights not available using prior techniques. © Proceedings of NSDI 2010: 7th USENIX Symposium on Networked Systems Design and Implementation. All rights reserved.},
keywords={Critical path analysis;  Molluscs;  Regression analysis;  Systems analysis, Critical Paths;  Data-flow computation;  Distributed data flow;  Execution trace;  On-line analysis;  Performance analysis;  Stream processing;  Streaming computations, Data flow analysis},
publisher={USENIX Association},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Runsewe2017848,
author={Runsewe, O. and Samaan, N.},
title={Cloud Resource Scaling for Big Data Streaming Applications Using a Layered Multi-dimensional Hidden Markov Model},
journal={Proceedings - 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2017},
year={2017},
pages={848-857},
doi={10.1109/CCGRID.2017.147},
art_number={7973790},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027460385&doi=10.1109%2fCCGRID.2017.147&partnerID=40&md5=7369cb8fa8e195215c93bee3cfb08d81},
abstract={Recent advancements in technology have led to a deluge of data that require real-Time analysis with strict latency constraints. A major challenge, however, is determining the amount of resources required by big data stream processing applications in response to heterogeneous data sources, streaming events, unpredictable data volume and velocity changes. Over-provisioning of resources for peak loads can be wasteful while under-provisioning can have a huge impact on the performance of the streaming applications. The majority of research efforts on resource scaling in the cloud are investigated from the cloud provider's perspective, they focus on web applications and do not consider multiple resource bottlenecks. We aim at analyzing the resource scaling problem from a big data streaming application provider's point of view such that efficient scaling decisions can be made for future resource utilization. This paper proposes a Layered Multi-dimensional Hidden Markov Model (LMD-HMM) for facilitating the management of resource auto-scaling for big data streaming applications in the cloud. Our detailed experimental evaluation shows that LMD-HMM performs best with an accuracy of 98%, outperforming the single-layer hidden markov model. © 2017 IEEE.},
author_keywords={Big Data;  Cloud Computing;  Layered Hidden Markov Model;  Resource Prediction;  Resource Scaling;  Stream Processing},
keywords={Cloud computing;  Cluster computing;  Data handling;  Data reduction;  Distributed computer systems;  Grid computing;  Hidden Markov models;  Information management;  Markov processes, Data stream processing;  Experimental evaluation;  Heterogeneous data sources;  Layered hidden markov models;  Resource prediction;  Resource Scaling;  Stream processing;  Streaming applications, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kolchin2016280,
author={Kolchin, M. and Wetz, P. and Kiesling, E. and Tjoa, A.M.},
title={YAbench: A comprehensive framework for RDF stream processor correctness and performance assessment},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9671},
pages={280-298},
doi={10.1007/978-3-319-38791-8_16},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977595483&doi=10.1007%2f978-3-319-38791-8_16&partnerID=40&md5=bb0b23f1ddfc27911bc7c02869f730f4},
abstract={RDF stream processing (RSP) has become a vibrant area of research in the semantic web community. Recent advances have resulted in the development of several RSP engines that leverage semantics to facilitate reasoning over flows of incoming data. These engines vary greatly in terms of implemented query syntax, their evaluation and operational semantics, and in various performance dimensions. Existing benchmarks tackle particular aspects such as functional coverage, result correctness, or performance. None of them, however, assess RSP engine behavior comprehensively with respect to all these dimensions. In this paper, we introduce YABench, a novel benchmarking framework for RSP engines. YABench extends the concept of correctness checking and provides a flexible and comprehensive tool set to analyze and evaluate RSP engine behavior. It is highly configurable and provides quantifiable and reproducible results on correctness and performance characteristics. To validate our approach, we replicate results of the existing CSRBench benchmark with YABench. We then assess two well-established RSP engines, CQELS and C-SPARQL, through more comprehensive experiments. In particular, we measure precision, recall, performance, and scalability characteristics while varying throughput and query complexity. Finally, we discuss implications on the development of future stream processing engines and benchmarks. © Springer International Publishing Switzerland 2016.},
keywords={Engines;  Semantic Web;  World Wide Web, Functional coverage;  Measure precision;  Operational semantics;  Performance assessment;  Performance characteristics;  Performance dimensions;  Stream processing;  Stream processing engines, Benchmarking},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zacheilas2016174,
author={Zacheilas, N. and Zygouras, N. and Panagiotou, N. and Kalogeraki, V. and Gunopulos, D.},
title={Dynamic load balancing techniques for distributed complex event processing systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9687},
pages={174-188},
doi={10.1007/978-3-319-39577-7_14},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976611187&doi=10.1007%2f978-3-319-39577-7_14&partnerID=40&md5=230a9b1bc0adf89cde2121da9889e5f8},
abstract={Applying real-time, cost-effective Complex Event processing (CEP) in the cloud has been an important goal in recent years. Distributed Stream Processing Systems (DSPS) have been widely adopted by major computing companies such as Facebook and Twitter for performing scalable event processing in streaming data. However, dynamically balancing the load of the DSPS’ components can be particularly challenging due to the high volume of data, the components’ state management needs, and the low latency processing requirements. Systems should be able to cope with these challenges and adapt to dynamic and unpredictable load changes in real-time. Our approach makes the following contributions: (i) we formulate the load balancing problem in distributed CEP systems as an instance of the job-shop scheduling problem, and (ii) we present a novel framework that dynamically balances the load of CEP engines in real-time and adapts to sudden changes in the volume of streaming data by exploiting two balancing policies. Our detailed experimental evaluation using data from the Twitter social network indicates the benefits of our approach in the system’s throughput. © IFIP International Federation for Information Processing 2016.},
keywords={Balancing;  Complex networks;  Cost effectiveness;  Data handling;  Distributed parameter control systems;  Interoperability;  Job shop scheduling;  Network management;  Optimization;  Real time systems;  Social networking (online), Complex event processing;  Complex event processing (CEP);  Distributed stream processing;  Experimental evaluation;  Job shop scheduling problems;  Load balancing problem;  State management;  Twitter social networks, Distributed computer systems},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tudoran2016274,
author={Tudoran, R. and Costan, A. and Nano, O. and Santos, I. and Soncu, H. and Antoniu, G.},
title={JetStream: Enabling high throughput live event streaming on multi-site clouds},
journal={Future Generation Computer Systems},
year={2016},
volume={54},
pages={274-291},
doi={10.1016/j.future.2015.01.016},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942369364&doi=10.1016%2fj.future.2015.01.016&partnerID=40&md5=ab5b5aa8c82875993ce66e9729291470},
abstract={Scientific and commercial applications operate nowadays on tens of cloud datacenters around the globe, following similar patterns: they aggregate monitoring or sensor data, assess the QoS or run global data mining queries based on inter-site event stream processing. Enabling fast data transfers across geographically distributed sites allows such applications to manage the continuous streams of events in real time and quickly react to changes. However, traditional event processing engines often consider data resources as second-class citizens and support access to data only as a side-effect of computation (i.e. they are not concerned by the transfer of events from their source to the processing site). This is an efficient approach as long as the processing is executed in a single cluster where nodes are interconnected by low latency networks. In a distributed environment, consisting of multiple datacenters, with orders of magnitude differences in capabilities and connected by a WAN, this will undoubtedly lead to significant latency and performance variations. This is namely the challenge we address in this paper, by proposing JetStream, a high performance batch-based streaming middleware for efficient transfers of events between cloud datacenters. JetStream is able to self-adapt to the streaming conditions by modeling and monitoring a set of context parameters. It further aggregates the available bandwidth by enabling multi-route streaming across cloud sites, while at the same time optimizing resource utilization and increasing cost efficiency. The prototype was validated on tens of nodes from US and Europe datacenters of the Windows Azure cloud with synthetic benchmarks and a real-life application monitoring the ALICE experiment at CERN. The results show a 3× increase of the transfer rate using the adaptive multi-route streaming, compared to state of the art solutions. © 2015 Elsevier B.V. All rights reserved.},
author_keywords={Big Data;  Cloud computing;  Multi-site;  Stream processing},
keywords={Aggregates;  Bandwidth;  Benchmarking;  Cloud computing;  Data mining;  Data transfer;  Middleware;  Windows operating system, Commercial applications;  Distributed environments;  Event stream processing;  Event-processing engine;  Multi-site;  Performance variations;  Real-life applications;  Stream processing, Big data},
publisher={Elsevier},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kofler2009355,
author={Kofler, I. and Seidl, J. and Timmerer, C. and Hellwagner, H. and Djama, I. and Ahmed, T.},
title={Using MPEG-21 for cross-layer multimedia content adaptation},
journal={Signal, Image and Video Processing},
year={2009},
volume={2},
number={4},
pages={355-370},
doi={10.1007/s11760-008-0088-x},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349115523&doi=10.1007%2fs11760-008-0088-x&partnerID=40&md5=4c287a0fe599207424b98be154b94b57},
abstract={This paper presents a cross-layer model- formulated using interoperable description formats-for the adaptation of scalable H.264/MPEG-4 AVC (i.e., SVC) content in a video streaming system operating on a Wireless LAN access network without QoS mechanisms. SVC content adaptation on the server takes place on the application layer using an adaptation process compliant with the MPEG-21 Digital Item Adaptation (DIA) standard, based on input comprised of MPEG-21 DIA descriptions of content and usage environment parameters. The latter descriptions integrate information from different layers, e.g., device characteristics and packet loss rate, in an attempt to increase the interoperability of this cross-layer model, thus making it applicable to other models. For the sake of deriving model parameters, performance measurements from two wireless access point models were taken in account. Throughout the investigation it emerged that the behavior of the system strongly depends on the access point. Therefore, we investigated the use of end-to-end-based rate control algorithms for steering the content adaptation. Simulations of rate adaptation algorithms were subsequently performed, leading to the conclusion that a TFRC-based adaptation technique (TCP-Friendly Rate Control) performs quite well in adapting to limited bandwidth and varying network conditions. In the paper we demonstrate how TFRC-based content adaptation can be realized using MPEG-21 tools. © 2008 Springer-Verlag London Limited.},
author_keywords={Cross-layer design;  MPEG-21 Digital Item Adaptation;  Multimedia content adaptation;  Rate control},
keywords={Access points;  Ad-aptation process;  Adaptation techniques;  Application layers;  Content adaptations;  Cross layers;  Cross-layer design;  Device characteristics;  H.264/mpeg-4 avc;  Limited bandwidths;  Model parameters;  Mpeg-21;  Mpeg-21 dia;  MPEG-21 Digital Item Adaptation;  Multimedia content adaptation;  Network conditions;  Packet loss rates;  Performance measurements;  Qos mechanisms;  Rate adaptations;  Rate control;  Rate control algorithms;  Tcp-friendly rate controls;  Wireless access points;  Wireless lan access, Electric network topology;  Image coding;  Network layers;  Quality of service;  Videotex;  Wireless local area networks (WLAN);  Wireless telecommunication systems, Motion Picture Experts Group standards},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Marino2007234,
author={Marino, G. and Vercelli, D. and Tecchia, F. and Gasparello, P.S. and Bergamasco, M.},
title={Description and performance analysis of a distributed rendering architecture for virtual environments},
journal={Proceedings 17th International Conference on Artificial Reality and Telexistence, ICAT 2007},
year={2007},
pages={234-241},
doi={10.1109/ICAT.2007.25},
art_number={4414639},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-48349125096&doi=10.1109%2fICAT.2007.25&partnerID=40&md5=b3f362156a79cd899721e43b94ab5d65},
abstract={Complex Virtual Environments applications may require computational resources exceeding the capabilities of a single machine. Our system, called "XVR Network Renderer", allows rendering load to be distributed throughout a cluster of machines operating concurrently. The proposed solution consists in a set of software modules structured as a single-master multiple-slaves architecture. XVR is a development environment that allows rapid development of Virtual Environments applications. The master software intercepts all the OpenGL API calls performed by any XVR application, without requiring any code to be added or modified. The graphical commands are then reexecuted individually by the slave clients. Each slave is typically configured to manage only a subset of the whole virtual context. Our system exploits the tight integration with the underlying XVR. scene-graph manager at its own advantage, providing additional features other than the mere visualization of a high resolution OpenGL context, such as head tracking, GLSL shaders, and the ability to insert (and intercept) "placemarkers" inside the broadcast OpenGL data stream. Finally, the system can be configured to work with a wide range of complex visualization setups, automatically handling stereoscopy, correct perspective correction, overlapping images and other common problems, without ever changing the code of the original application. In this work we describe the proposed architecture and wo discuss the results of our performance analysis. © 2007 IEEE.},
author_keywords={Distributed applications;  Rendering;  Virtual environments;  XVR},
keywords={Chlorine compounds;  Codes (standards);  Codes (symbols);  Data visualization;  Visualization, Applications.;  Common problems;  Computational resources;  Data streaming;  Development environments;  Distributed applications;  Distributed rendering;  Graph manager;  Head tracking;  High resolutions;  International conferences;  Overlapping images;  Performance analyses;  Proposed architectures;  Rapid development;  Rendering;  Single machines;  Software modules;  Virtual environments;  XVR, Virtual reality},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chao2018273,
author={Chao, M. and Yang, C. and Zeng, Y. and Stoleru, R.},
title={F-MStorm: Feedback-based online distributed mobile stream processing},
journal={Proceedings - 2018 3rd ACM/IEEE Symposium on Edge Computing, SEC 2018},
year={2018},
pages={273-285},
doi={10.1109/SEC.2018.00027},
art_number={8567672},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060220075&doi=10.1109%2fSEC.2018.00027&partnerID=40&md5=0a8bb98d3db30c9a4b5f646139855159},
abstract={A distributed mobile stream processing system allows mobile devices to process stream data that exceeds any single device’s computation capability without the help of infrastructure. It is paramount to have such a system in many critical application scenarios, such as military operations and disaster response, yet an efficient online mobile stream processing system is still missing. In this paper, we make the key observation that the unique characteristics of mobile stream processing call for a feedback-based system design, which is in sharp contrast with the static configuration and scheduling of the current mobile stream processing system, “MStorm” [1]. At first, we demonstrate the inefficiencies of MStorm through several real-world experiments. Then, we propose F-MStorm, a feedback-based online distributed mobile stream processing system, which adopts the feedback-based approach in the configuration, scheduling and execution levels of system design. We implement F-MStorm on Android phones and evaluate its performance through benchmark applications. We show that it achieves up to 3x lower response time, 10% higher throughput and consumes 23% less communication energy than the state-of-the-art systems. © 2018 IEEE.},
author_keywords={Edge computing;  Scheduling;  Stream processing},
keywords={Edge computing;  Military operations;  Scheduling;  Systems analysis, Benchmark applications;  Communication energy;  Critical applications;  Feedback-based systems;  Real world experiment;  State-of-the-art system;  Stream processing;  Stream processing systems, Benchmarking},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hoseinyfarahabady2017332,
author={Hoseinyfarahabady, M. and Taheri, J. and Tari, Z. and Zomaya, A.Y.},
title={A Dynamic Resource Controller for a Lambda Architecture},
journal={Proceedings of the International Conference on Parallel Processing},
year={2017},
pages={332-341},
doi={10.1109/ICPP.2017.42},
art_number={8025307},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030622746&doi=10.1109%2fICPP.2017.42&partnerID=40&md5=82093fffe125647f4e193a190d7a5d9f},
abstract={Lambda architecture is a novel event-driven serverless paradigm that allows companies to build scalable and reliable enterprise applications. As an attractive alternative to traditional service oriented architecture (SOA), Lambda architecture can be used in many use cases including BI tools, in-memory graph databases, OLAP, and streaming data processing. In practice, an important aim of Lambda's service providers is devising an efficient way to co-locate multiple Lambda functions with different attributes into a set of available computing resources. However, previous studies showed that consolidated workloads can compete fiercely for shared resources, resulting in severe performance variability/degradation. This paper proposes a resource allocation mechanism for a Lambda platform based on the model predictive control framework. Performance evaluation is carried out by comparing the proposed solution with multiple resource allocation heuristics, namely enhanced versions of spread and binpack, and best-effort approaches. Results confirm that the proposed controller increases the overall resource utilization by 37% on average and achieves a significant improvement in preventing QoS violation incidents compared to others. © 2017 IEEE.},
author_keywords={Dynamic Resource Allocation;  Lambda Platform Processing;  Performance degradation;  Shared Resource Interference},
keywords={Data handling;  Information services;  Memory architecture;  Model predictive control;  Quality of service;  Resource allocation, Dynamic resource allocations;  Enterprise applications;  Performance degradation;  Performance variability;  Resource utilizations;  Shared resources;  Streaming data processing;  Traditional services, Service oriented architecture (SOA)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhou2017,
author={Zhou, L. and Chen, N. and Chen, Z.},
title={Efficient streaming mass spatio-temporal vehicle data access in urban sensor networks based on apache storm},
journal={Sensors (Switzerland)},
year={2017},
volume={17},
number={4},
doi={10.3390/s17040815},
art_number={815},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017411092&doi=10.3390%2fs17040815&partnerID=40&md5=0b9703144e237e22f5f1cb0ad0cd9081},
abstract={The efficient data access of streaming vehicle data is the foundation of analyzing, using and mining vehicle data in smart cities, which is an approach to understand traffic environments. However, the number of vehicles in urban cities has grown rapidly, reaching hundreds of thousands in number. Accessing the mass streaming data of vehicles is hard and takes a long time due to limited computation capability and backward modes. We propose an efficient streaming spatio-temporal data access based on Apache Storm (ESDAS) to achieve real-time streaming data access and data cleaning. As a popular streaming data processing tool, Apache Storm can be applied to streaming mass data access and real time data cleaning. By designing the Spout/bolt workflow of topology in ESDAS and by developing the speeding bolt and other bolts, Apache Storm can achieve the prospective aim. In our experiments, Taiyuan BeiDou bus location data is selected as the mass spatio-temporal data source. In the experiments, the data access results with different bolts are shown in map form, and the filtered buses’ aggregation forms are different. In terms of performance evaluation, the consumption time in ESDAS for ten thousand records per second for a speeding bolt is approximately 300 milliseconds, and that for MongoDB is approximately 1300 milliseconds. The efficiency of ESDAS is approximately three times higher than that of MongoDB. © 2017 by the authors.},
author_keywords={Apache Storm;  BeiDou bus network;  Cloud computing;  Sensor Observation Service;  Streaming spatio-temporal mass data access},
keywords={Bolts;  Buses;  Cloud computing;  Distributed computer systems;  Radio navigation;  Sensor networks;  Smart city;  Storms;  Vehicles, Bus networks;  Number of vehicles;  Real time streaming;  Sensor observation services;  Spatio temporal;  Spatio-temporal data;  Streaming data processing;  Traffic environment, Data handling},
publisher={MDPI AG},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nikolakopoulos20162,
author={Nikolakopoulos, Y. and Papatriantafilou, M. and Brauer, P. and Lundqvist, M. and Gulisano, V. and Tsigas, P.},
title={Highly concurrent stream synchronization in many-core embedded systems},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={18-22-June-2016},
pages={2-9},
doi={10.1145/2934495.2934496},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991096040&doi=10.1145%2f2934495.2934496&partnerID=40&md5=de1f8a031213b434d2df11903e122819},
abstract={Embedded many-core architectures are expected to serve as significant components in the infrastructure of upcoming technologies like networks for the Internet of Things (IoT), facing real-time and stream processing challenges. In this work we explore the applicability of ScaleGate, a synchronization object from the massive data stream processing domain, on many-core embedded systems. We propose a new implementation of ScaleGate on the Epiphany architecture, a scalable embedded many-core co-processor, and study communication patterns that appear in the context of a baseband signal processing application. Our experimental evaluation shows significant improvements over standard barrier-based approaches, due to the asynchrony exploited by the use of ScaleGate. © 2016 Copyright held by the owner/author(s).},
author_keywords={Epiphany;  ScaleGate;  Streaming;  Synchronization},
keywords={Acoustic streaming;  Data handling;  Embedded systems;  Network architecture;  Signal processing;  Synchronization, Baseband signal processing;  Communication pattern;  Epiphany;  Experimental evaluation;  Internet of thing (IOT);  Many-core architecture;  ScaleGate;  Synchronization objects, Computer architecture},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ploennigs20151231,
author={Ploennigs, J. and Chen, B. and Palmes, P. and Lloyd, R.},
title={E2-diagnoser: A system for monitoring, forecasting and diagnosing energy usage},
journal={IEEE International Conference on Data Mining Workshops, ICDMW},
year={2015},
volume={2015-January},
number={January},
pages={1231-1234},
doi={10.1109/ICDMW.2014.56},
art_number={7022741},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936859545&doi=10.1109%2fICDMW.2014.56&partnerID=40&md5=58476db691262836985daa8c8331dba3},
abstract={We propose e2-Diagnoser, a real-time data mining system for the energy management of smart, sensor-equipped buildings. The main features of e2-Diagnoser are: (i) fast extraction of a large portfolio of buildings' benchmarks at multiple places, and (ii) accurate prediction of buildings' energy usage down to sub meter level to detect and diagnose abnormal energy consumptions. Fundamentally, the e2-Diagnoser system is built on a novel statistical learning algorithm using the Generalized Additive Model (GAM) to simultaneously monitor the mean and variation of the energy usage as well as identify the influencing factors such as weather conditions. Its implementation is based on stream processing platform that integrates data from various sources using semantic web technologies and provides an interactive user interface to visualize results. The platform is scalable and can be easily adapted to other applications such as smart-grid networks. Here we describe the architecture, methodology, and show the web-interface to demonstrate the main functions in the e2-Diagnoser. © 2014 IEEE.},
author_keywords={Building Benchmark;  Energy Prediction;  Pattern Extraction;  Smart Buildings},
keywords={Buildings;  Extraction;  Forecasting;  Information management;  Intelligent buildings;  User interfaces, Energy prediction;  Generalized additive model;  Interactive user interfaces;  Pattern extraction;  Real-time data mining;  Semantic Web technology;  Smart grid networks;  Statistical learning, Data mining},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Thomos2008497,
author={Thomos, N. and Frossard, P.},
title={Collaborative video streaming with Raptor network coding},
journal={2008 IEEE International Conference on Multimedia and Expo, ICME 2008 - Proceedings},
year={2008},
pages={497-500},
doi={10.1109/ICME.2008.4607480},
art_number={4607480},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-54049097566&doi=10.1109%2fICME.2008.4607480&partnerID=40&md5=73aa4071ab32895b346e836e5ce509d4},
abstract={We investigate the problem of collaborative video streaming with Raptor network coding over overlay networks. We exploit path and source diversity, as well as basic processing capabilities of network nodes to increase the overall throughput and improve the video quality at the clients. We consider an architecture where several streaming servers simultaneously deliver video information to a set of clients. The servers apply Raptor coding on the video packets for error resiliency, and the forwarding peer nodes further combine the Raptor coded video packets in order to increase the packet diversity in the network. We find the optimal source and channel rate allocation in such a collaborative streaming system. The resulting scheme efficiently exploits the available network resources for improved video quality. The experimental evaluation demonstrates that it typically outperforms Raptor video streaming systems that do not use network coding. © 2008 IEEE.},
author_keywords={Error resiliency;  Network coding;  Optimal rate allocation;  Overlay networks},
keywords={Acoustic streaming;  Distributed computer systems;  Exhibitions;  Network protocols;  Programming theory;  Sensor networks;  Telecommunication networks;  Video streaming;  Videotex, Channel rate allocations;  Collaborative streaming systems;  Error resiliencies;  Error resiliency;  Experimental evaluations;  Network coding;  Network nodes;  Network resources;  Optimal rate allocation;  Overlay networks;  Packet diversities;  Peer nodes;  Processing capabilities;  Raptor coding;  Streaming servers;  Video informations;  Video packets;  Video qualities;  Video streaming systems, Computer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mamouras2019670,
author={Mamouras, K. and Stanford, C. and Alur, R. and Ives, Z.G. and Tannen, V.},
title={Data-trace types for distributed stream processing systems},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
year={2019},
pages={670-685},
doi={10.1145/3314221.3314580},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067657471&doi=10.1145%2f3314221.3314580&partnerID=40&md5=dc22d16106f13724e652cd3cd8acd997},
abstract={Distributed architectures for efficient processing of streaming data are increasingly critical to modern information processing systems. The goal of this paper is to develop type-based programming abstractions that facilitate correct and efficient deployment of a logical specification of the desired computation on such architectures. In the proposed model, each communication link has an associated type specifying tagged data items along with a dependency relation over tags that captures the logical partial ordering constraints over data items. The semantics of a (distributed) stream processing system is then a function from input data traces to output data traces, where a data trace is an equivalence class of sequences of data items induced by the dependency relation. This data-trace transduction model generalizes both acyclic synchronous data-flow and relational query processors, and can specify computations over data streams with a rich variety of partial ordering and synchronization characteristics. We then describe a set of programming templates for data-trace transductions: abstractions corresponding to common stream processing tasks. Our system automatically maps these high-level programs to a given topology on the distributed implementation platform Apache Storm while preserving the semantics. Our experimental evaluation shows that (1) while automatic parallelization deployed by existing systems may not preserve semantics, particularly when the computation is sensitive to the ordering of data items, our programming abstractions allow a natural specification of the query that contains a mix of ordering constraints while guaranteeing correct deployment, and (2) the throughput of the automatically compiled distributed code is comparable to that of hand-crafted distributed implementations. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
author_keywords={Distributed data stream processing;  Types},
keywords={Abstracting;  Bacteriophages;  Computer programming languages;  Data flow analysis;  Data handling;  Data mining;  Distributed computer systems;  Distributed parameter control systems;  Equivalence classes;  Functions;  Semantics;  Specifications, Automatic Parallelization;  Distributed data stream processing;  Distributed implementation;  Distributed stream processing;  Information processing systems;  Programming abstractions;  Stream processing systems;  Types, Search engines},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Laska2018,
author={Laska, M. and Herle, S. and Klamma, R. and Blankenbach, J.},
title={A scalable architecture for real-time stream processing of spatiotemporal IoT stream data — Performance analysis on the example of map matching},
journal={ISPRS International Journal of Geo-Information},
year={2018},
volume={7},
number={7},
doi={10.3390/ijgi7070238},
art_number={238},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065861402&doi=10.3390%2fijgi7070238&partnerID=40&md5=0058eb7f22ba48e1ef051c2c44915338},
abstract={Scalable real-time processing of large amounts of data has become a research topic of particular importance due to the continuously rising amount of data that is generated by devices equipped with sensing components. While existing approaches allow for fault-tolerant and scalable stream processing, we present a pipeline architecture that consists of well-known open source tools to specifically integrate spatiotemporal internet of things (IoT) data streams. In a case study, we utilize the architecture to tackle the online map matching problem, a pre-processing step for trajectory mining algorithms. Given the rising amount of vehicle location data that is generated on a daily basis, existing map matching algorithms have to be implemented in a distributed manner to be executable in a stream processing framework that provides scalability. We demonstrate how to implement state-of-the-art map matching algorithms in our distributed stream processing pipeline and analyze measured latencies. © 2018 by the authors.},
author_keywords={Data mining;  IoT;  Map matching;  Spatiotemporal;  Stream processing},
publisher={MDPI AG},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Maroulis201727,
author={Maroulis, S. and Zacheilas, N. and Kalogeraki, V.},
title={ExpREsS: EneRgy Efficient Scheduling of Mixed Stream and Batch Processing Workloads},
journal={Proceedings - 2017 IEEE International Conference on Autonomic Computing, ICAC 2017},
year={2017},
pages={27-32},
doi={10.1109/ICAC.2017.43},
art_number={8005324},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034425485&doi=10.1109%2fICAC.2017.43&partnerID=40&md5=af942cf230b938a4b442c4e8e51022df},
abstract={Nowadays we see the wide adoption of novel distributed processing frameworks such as Apache Spark for handling batch and stream processing big data applications. An important aspect that has not been examined in these systems is their energy consumption during the application execution. Reducing the power consumption of modern datacenters is a necessity as datacenters contribute over 2% of the total US electric usage. One way of addressing this energy issue is by scheduling the applications in an energy-efficient way. However, efficiently scheduling applications can be challenging as we need to consider the trade-off between the datacenter's energy usage and per application performance requirements. In this work we propose, ExpREsS, a scheduler for orchestrating the execution of Spark applications so that it both minimizes the energy consumption and satisfies the applications' performance requirements. Our approach exploits time-series prediction models for capturing the applications' energy usage and execution times, and then applies a novel DVFS technique to minimize the energy consumption. Our detailed experimental evaluation using realistic workloads on our local cluster illustrates the working and benefits of our approach. © 2017 IEEE.},
author_keywords={Distributed Systems;  Green Computing;  Scheduling},
keywords={Batch data processing;  Big data;  Data handling;  Distributed computer systems;  Economic and social effects;  Electric power utilization;  Energy utilization;  Green computing;  Scheduling, Application performance;  Distributed processing frameworks;  Distributed systems;  Energy-Efficient Scheduling;  Experimental evaluation;  Performance requirements;  Scheduling application;  Time series prediction, Energy efficiency},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fan2016309,
author={Fan, J. and Chen, H. and Hu, F.},
title={Adaptive task scheduling in storm},
journal={Proceedings of 2015 4th International Conference on Computer Science and Network Technology, ICCSNT 2015},
year={2016},
pages={309-314},
doi={10.1109/ICCSNT.2015.7490758},
art_number={7490758},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979281196&doi=10.1109%2fICCSNT.2015.7490758&partnerID=40&md5=8c00362514f50c6fae0d3e5dbf928058},
abstract={Processing of stream data attracts more and more attention of many big companies and organizations. Storm is a well-known distributed stream processing system that is often used for real-Time analysis, online machine learning, continuous computing, distributed remote process call (RPC), etc. In this paper, we study the default scheduler of Storm and other implementations of customized scheduler to discover the primary factors affecting the performance of the cluster. Then, we design and implement an adaptive task scheduler by adding load tracker to monitor the runtime status of the cluster and applying static and dynamic scheduling strategies. At last, we conduct experiments to assess our work by measuring average processing time, overall throughput and stability of the cluster through network bounded and CPU bounded benchmarks. As for average processing time of topologies, the adaptive scheduler achieves about 67% and 30% improvement on cluster of heavy and light load respectively. © 2015 IEEE.},
author_keywords={Apache Storm;  distributed computing;  load monitoring;  Scheduling;  stream processing},
keywords={Artificial intelligence;  Cluster computing;  Computer networks;  Data handling;  Distributed parameter control systems;  E-learning;  Learning systems;  Multitasking;  Scheduling;  Storms, Adaptive task scheduling;  Design and implements;  Distributed stream processing;  Load monitoring;  Primary factors;  Real time analysis;  Static and dynamic scheduling;  Stream processing, Distributed computer systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bouillet2010249,
author={Bouillet, E. and Ranganathan, A.},
title={Scalable, real-time map-matching using IBM's System S},
journal={Proceedings - IEEE International Conference on Mobile Data Management},
year={2010},
pages={249-257},
doi={10.1109/MDM.2010.36},
art_number={5489658},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955194680&doi=10.1109%2fMDM.2010.36&partnerID=40&md5=78c0d9fe9446cac38677c3cbf6607c32},
abstract={Vehicle GPS data is an essential "raw" material for a broad range of applications such as traffic management and control, routing, and navigation. To become useful, the data has to be related to the underlying road network by means of map matching algorithms, which are often computationally expensive. In addition, GPS data is not accurate and often needs to be cleaned to remove erroneous observations. In this paper, we describe how map matching can be run on IBM's System S, which provides a platform to run stream processing applications in a scalable manner. We show how various features of System S, including a component based programming model, data pipelining and parallelization of computation, help us to scale the map-matching and data cleaning processes, both as the rate of incoming GPS data increases and as the size of the underlying road network increases. We provide results of performance evaluations, where we show our system can match GPS data arriving at a rate of 1 million points per second onto a map with 1 billion links. © 2010 IEEE.},
keywords={Component-based programming;  Data cleaning;  GPS data;  Map matching;  Map-matching algorithm;  Parallelizations;  Performance evaluation;  Road network;  Stream processing;  Traffic management, Highway administration;  Motor transportation;  Roads and streets;  Traffic control, Global positioning system},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shahverdi201953,
author={Shahverdi, E. and Awad, A. and Sakr, S.},
title={Big stream processing systems: An experimental evaluation},
journal={Proceedings - 2019 IEEE 35th International Conference on Data Engineering Workshops, ICDEW 2019},
year={2019},
pages={53-60},
doi={10.1109/ICDEW.2019.00-35},
art_number={8750955},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069216125&doi=10.1109%2fICDEW.2019.00-35&partnerID=40&md5=1d4cecac6147c94f034a6f919266e193},
abstract={As the world gets more instrumented and connected, we are witnessing a flood of digital data generated from various hardware (e.g., sensors) or software in the format of flowing streams of data. Real-time processing for such massive amounts of streaming data is a crucial requirement in several application domains including financial markets, surveillance systems, manufacturing, smart cities, and scalable monitoring infrastructure. In the last few years, several big stream processing engines have been introduced to tackle this challenge. In this article, we present an extensive experimental study of five popular systems in this domain, namely, Apache Storm, Apache Flink, Apache Spark, Kafka Streams and Hazelcast Jet. We report and analyze the performance characteristics of these systems. In addition, we report a set of insights and important lessons that we have learned from conducting our experiments. © 2019 IEEE.},
author_keywords={Benchmarking;  Big Stream Processing},
keywords={Benchmarking;  Computer hardware description languages;  Real time systems;  Technical presentations, Experimental evaluation;  Flowing streams;  Performance characteristics;  Realtime processing;  Stream processing;  Stream processing engines;  Stream processing systems;  Surveillance systems, Manufacturing data processing},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Marcu20181480,
author={Marcu, O.-C. and Costan, A. and Antoniu, G. and Pérez-Hernández, M. and Nicolae, B. and Tudoran, R. and Bortoli, S.},
title={KerA: Scalable data ingestion for stream processing},
journal={Proceedings - International Conference on Distributed Computing Systems},
year={2018},
volume={2018-July},
pages={1480-1485},
doi={10.1109/ICDCS.2018.00152},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050975811&doi=10.1109%2fICDCS.2018.00152&partnerID=40&md5=11936b36e6e3ec66388d7a5179a85861},
abstract={Big Data applications are increasingly moving from batch-oriented execution models to stream-based models that enable them to extract valuable insights close to real-time. To support this model, an essential part of the streaming processing pipeline is data ingestion, i.e., the collection of data from various sources (sensors, NoSQL stores, filesystems, etc.) and their delivery for processing. Data ingestion needs to support high throughput, low latency and must scale to a large number of both data producers and consumers. Since the overall performance of the whole stream processing pipeline is limited by that of the ingestion phase, it is critical to satisfy these performance goals. However, state-of-art data ingestion systems such as Apache Kafka build on static stream partitioning and offset-based record access, trading performance for design simplicity. In this paper we propose KerA, a data ingestion framework that alleviate the limitations of state-of-art thanks to a dynamic partitioning scheme and to lightweight indexing, thereby improving throughput, latency and scalability. Experimental evaluations show that KerA outperforms Kafka up to 4x for ingestion throughput and up to 5x for the overall stream processing throughput. Furthermore, they show that KerA is capable of delivering data fast enough to saturate the big data engine acting as the consumer. © 2018 IEEE.},
author_keywords={Dynamic partitioning;  Ingestion;  Stream processing},
keywords={Data handling;  Data mining;  Ingestion (engines);  Pipeline processing systems;  Pipelines;  Throughput, Big data applications;  Data ingestions;  Dynamic partitioning;  Execution model;  Experimental evaluation;  High throughput;  Stream processing;  Streaming processing, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dünner2018252,
author={Dünner, C. and Parnell, T. and Sarigiannis, D. and Ioannou, N. and Anghel, A. and Ravi, G. and Kandasamy, M. and Pozidis, H.},
title={SNaP ML: A hierarchical framework for machine learning},
journal={Advances in Neural Information Processing Systems},
year={2018},
volume={2018-December},
pages={252-262},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064818595&partnerID=40&md5=4e0b9bf06a672b5697d83fc90d24dd04},
abstract={We describe a new software framework for fast training of generalized linear models. The framework, named Snap Machine Learning (Snap ML), combines recent advances in machine learning systems and algorithms in a nested manner to reflect the hierarchical architecture of modern computing systems. We prove theoretically that such a hierarchical system can accelerate training in distributed environments where intra-node communication is cheaper than inter-node communication. Additionally, we provide a review of the implementation of Snap ML in terms of GPU acceleration, pipelining, communication patterns and software architecture, highlighting aspects that were critical for achieving high performance. We evaluate the performance of Snap ML in both single-node and multi-node environments, quantifying the benefit of the hierarchical scheme and the data streaming functionality, and comparing with other widely-used machine learning software frameworks. Finally, we present a logistic regression benchmark on the Criteo Terabyte Click Logs dataset and show that Snap ML achieves the same test loss an order of magnitude faster than any of the previously reported results, including those obtained using TensorFlow and scikit-learn. © 2018 Curran Associates Inc..All rights reserved.},
keywords={Computer architecture;  Computer programming;  Hierarchical systems;  Sodium compounds;  Statistical tests;  Sulfur compounds, Communication pattern;  Distributed environments;  Generalized linear model;  Hierarchical architectures;  Inter-node communication;  Intra-node communication;  Logistic regressions;  Machine learning software, Machine learning},
publisher={Neural information processing systems foundation},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Al-Zubaidy20172238,
author={Al-Zubaidy, H. and Fodor, V. and Dan, G. and Flierl, M.},
title={Reliable Video Streaming with Strict Playout Deadline in Multihop Wireless Networks},
journal={IEEE Transactions on Multimedia},
year={2017},
volume={19},
number={10},
pages={2238-2251},
doi={10.1109/TMM.2017.2742399},
art_number={8013748},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028475488&doi=10.1109%2fTMM.2017.2742399&partnerID=40&md5=6f8e5e5084d9fcf119d6d5c47912c342},
abstract={Motivated by emerging vision-based intelligent services, we consider the problem of rate adaptation for high-quality and low-delay visual information delivery over wireless networks using scalable video coding. Rate adaptation in this setting is inherently challenging due to the interplay between the variability of the wireless channels, the queuing at the network nodes, and the frame-based decoding and playback of the video content at the receiver at very short time scales. To address the problem, we propose a low-complexity model-based rate adaptation algorithm for scalable video streaming systems, building on a novel performance model based on stochastic network calculus. We validate the analytic model using extensive simulations. We show that it allows fast near-optimal rate adaptation for fixed transmission paths, as well as cross-layer optimized routing and video rate adaptation in mesh networks, with less than 10% quality degradation compared to the best achievable performance. © 1999-2012 IEEE.},
author_keywords={Multihop fading channels;  network calculus;  performance analysis;  scalable video coding;  wireless multimedia},
keywords={Calculations;  Communication channels (information theory);  Fading channels;  Image coding;  MESH networking;  Motion compensation;  Stochastic models;  Stochastic systems;  Video signal processing;  Video streaming;  Wireless networks, Achievable performance;  Multihop;  Multihop wireless network;  Network calculus;  Performance analysis;  Scalable video streaming;  Stochastic network calculus;  Wireless multimedia, Scalable video coding},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shukla201790,
author={Shukla, A. and Simmhan, Y.},
title={Benchmarking distributed stream processing platforms for IoT applications},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10080 LNCS},
pages={90-106},
doi={10.1007/978-3-319-54334-5_7},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013939100&doi=10.1007%2f978-3-319-54334-5_7&partnerID=40&md5=a8561f43ff2515258e3a20e6ebb2ed9a},
abstract={Internet of Things (IoT) is a technology paradigm where millions of sensors monitor, and help inform or manage, physical, environmental and human systems in real-time. The inherent closed-loop responsiveness and decision making of IoT applications makes them ideal candidates for using low latency and scalable stream processing platforms. Distributed Stream Processing Systems (DSPS) are becoming essential components of any IoT stack, but the efficacy and performance of contemporary DSPS have not been rigorously studied for IoT data streams and applications. Here, we develop a benchmark suite and performance metrics to evaluate DSPS for streaming IoT applications. The benchmark includes 13 common IoT tasks classified across functional categories and forming micro-benchmarks, and two IoT applications for statistical summarization and predictive analytics that leverage various dataflow patterns of DSPS. These are coupled with stream workloads from real IoT observations on smart cities. We validate the benchmark for the popular Apache Storm DSPS, and present the results. © Springer International Publishing AG 2017.},
author_keywords={Benchmark;  Big data;  Distributed systems;  Fast data;  Internet of things;  Smart cities;  Stream processing;  Velocity;  Workload},
keywords={Benchmarking;  Big data;  Data flow analysis;  Decision making;  Distributed parameter control systems;  Environmental technology;  Predictive analytics;  Real time systems;  Smart city;  Velocity, Dataflow patterns;  Distributed stream processing;  Distributed systems;  Fast data;  Internet of Things (IOT);  Performance metrics;  Stream processing;  Workload, Internet of things},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Du2014275,
author={Du, Y. and Liu, J. and Liu, F. and Chen, L.},
title={A real-time anomalies detection system based on streaming technology},
journal={Proceedings - 2014 6th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2014},
year={2014},
volume={2},
pages={275-279},
doi={10.1109/IHMSC.2014.168},
art_number={6911496},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908869209&doi=10.1109%2fIHMSC.2014.168&partnerID=40&md5=7ef0cf46a5865936b9f8d2c31a84bdda},
abstract={With the wide deployment of flow monitoring in IP networks, flow data has been more and more applied on abnormal traffic detection. In practice, anomalies should be detected as fast as possible from giant quantity of flow data, while, at present, some classical anomalies detecting methods can not achieve this goal. In this paper, we propose and implement a distributed streaming computing system which aims to perform real-time anomalies detection by leveraging Apache Storm, a stream-computing platform. Based on this efficient system, we can uninterruptedly monitor the mutation of flow data and locate the source of anomalies or attacks in real-time by finding the specific abnormal IP addresses. A typical application example proved the capability and benefits of our system and we also have a detailed discussion in performance measurements and scalability. © 2014 IEEE.},
author_keywords={anomalies detection;  Apache Storm;  real-time;  streaming computing},
keywords={Internet protocols;  Multimedia systems;  Storms, Abnormal traffic detection;  Computing system;  Detecting methods;  Distributed streaming;  Performance measurements;  Real time;  Streaming technology;  Typical application, Anomaly detection},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Qi2013111,
author={Qi, K. and Han, Y. and Zhao, Z. and Fang, J.},
title={MapReduce intermediate result cache for concurrent data stream processing},
journal={Jisuanji Yanjiu yu Fazhan/Computer Research and Development},
year={2013},
volume={50},
number={1},
pages={111-121},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874710859&partnerID=40&md5=be7f355a7328686521ca14bcaa04ed0d},
abstract={With the development of Internet of Things applications, real-time processing of sensor data stream over large scale history data brings a new challenge. The traditional MapReduce programming model is designed for batch-based large-scale data processing and cannot satisfy the real-time requirement. To extend the real-time data processing capability of MapReduce by preprocessing, pipelining and localizing, an immediate result cache for key/value data type, which can avoid repeated remote I/O overhead and computation cost by taking full use of local memory and storage, localize stream processing by distributing data across the clusters and support frequent reads and writes of data stream processing, needs to be designed. This paper proposes a scalable, extensible and efficient key/value intermediate result cache, which consists of Hash B-tree structures and SSTable files. Furthermore, to optimize the high concurrency performance, this paper also devises a probability-based B-tree structure as well as its multiplexing search algorithm through the B-tree balance property, and improves the file read/write strategy and replacement algorithm by utilization of the overhead estimation and buffered information. The theoretical analysis and benchmark experiments show that the proposed structures and algorithms further optimize the concurrency performance of MapReduce immediate results, and the immediate result cache is effective to support data stream processing over large-scale data.},
author_keywords={Data stream processing;  High concurrency;  Intermediate result cache;  Large-scale data processing;  MapReduce},
keywords={Data stream processing;  High concurrencies;  Intermediate results;  Large-scale data processing;  Map-reduce, Algorithms;  Benchmarking;  Data communication systems;  Data processing;  Digital storage;  Optimization, Trees (mathematics)},
language={Chinese},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hsu2008,
author={Hsu, C.-H. and Hefeeda, M.},
title={On the accuracy and complexity of rate-distortion models for fine-grained scalable video sequences},
journal={ACM Transactions on Multimedia Computing, Communications and Applications},
year={2008},
volume={4},
number={2},
doi={10.1145/1352012.1352019},
art_number={15},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249090915&doi=10.1145%2f1352012.1352019&partnerID=40&md5=39ff7cde465f2d5db73534d439ff9bb2},
abstract={Rate-distortion (R-D) models are functions that describe the relationship between the bitrate and expected level of distortion in the reconstructed video stream. R-D models enable optimization of the received video quality in different network conditions. Several R-D models have been proposed for the increasingly popular fine-grained scalable video sequences. However, the models' relative performance has not been thoroughly analyzed. Moreover, the time complexity of each model is not known, nor is the range of bitrates in which the model produces valid results. This lack of quantitative performance analysis makes it difficult to select the model that best suits a target streaming system. In this article, we classify, analyze, and rigorously evaluate all R-D models proposed for FGS coders in the literature. We classify R-D models into three categories: analytic, empirical, and semi-analytic. We describe the characteristics of each category. We analyze the R-D models by following their mathematical derivations, scrutinizing the assumptions made, and explaining when the assumptions fail and why. In addition, we implement all R-D models, a total of eight, and evaluate them using a diverse set of video sequences. In our evaluation, we consider various source characteristics, diverse channel conditions, different encoding/decoding parameters, different frame types, and several performance metrics including accuracy, range of applicability, and time complexity of each model. We also present clear systematic ways (pseudo codes) for constructing various R-D models from a given video sequence. Based on our experimental results, we present a justified list of recommendations on selecting the best R-D models for video-on-demand, video conferencing, real-time, and peer-to-peer streaming systems. © 2008 ACM.},
author_keywords={Fine-grained scalable coding;  Multimedia streaming;  Rate-distortion models},
keywords={Image coding;  Image quality;  Mathematical models;  Multimedia systems;  Optimization;  Parameter estimation, Fine-grained scalable coding;  Multimedia streaming;  Rate-distortion models, Video signal processing},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bracciale200725,
author={Bracciale, L. and Piccolo, F.L. and Luzzi, D. and Salsano, S.},
title={OPSS: An Overlay Peer-to-peer Streaming Simulator for large-scale networks},
journal={Performance Evaluation Review},
year={2007},
volume={35},
number={3},
pages={25-27},
doi={10.1145/1328690.1328700},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449513751&doi=10.1145%2f1328690.1328700&partnerID=40&md5=167fe195926a2c1c7e76108c71307cd7},
abstract={In this paper we present OPSS, an Overlay Peer-to-peer Streaming Simulator designed to simulate large scale (i.e. in the order of 100K nodes) peer-to-peer streaming systems. OPSS is able to simulate a fair (i.e. "TCP-like") sharing of the uplink and downlink bandwidth among different connections, and it guarantees extensibility by allowing the implementation of different peer-to-peer streaming algorithms as separate modules. Source code of OPSS is available under the GPL license. © 2018 International Conference on Learning Representations, ICLR. All rights reserved.},
keywords={Computer network performance evaluation, Large-scale network;  Peer-to-peer streaming;  Source codes, Peer to peer networks},
publisher={Association for Computing Machinery},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vijayakumar2006539,
author={Vijayakumar, N.N. and Liu, Y. and Plale, B.},
title={Calder query grid service: Insights and experimental evaluation},
journal={Sixth IEEE International Symposium on Cluster Computing and the Grid, 2006. CCGRID 06},
year={2006},
pages={539-543},
doi={10.1109/CCGRID.2006.25},
art_number={1630867},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751093062&doi=10.1109%2fCCGRID.2006.25&partnerID=40&md5=c1a4d6971b17f5c4ed6fee616033f045},
abstract={We have architected and evaluated a new kind of data resource, one that is composed of a logical collection of ephemeral data streams that could be viewed as a collection of publish-subscribe "channels" over which rich data-access and semantic operations can be performed. This paper contributes new insight to stream processing under the highly asynchronous stream workloads often found in data-driven scientific applications, and presents insights gained through porting a distributed stream processing system to a Grid services framework. Experimental results reveal limits on stream processing rates that are directly tied to differences in stream rates. © 2006 IEEE.},
keywords={Asynchronous stream workloads;  Data resources;  Semantic operations;  Stream processing rates, Computer architecture;  Data transfer;  Distributed computer systems;  Query languages;  Semantics, Parallel processing systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Henning2021,
author={Henning, S. and Hasselbring, W.},
title={Theodolite: Scalability Benchmarking of Distributed Stream Processing Engines in Microservice Architectures},
journal={Big Data Research},
year={2021},
volume={25},
doi={10.1016/j.bdr.2021.100209},
art_number={100209},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100680665&doi=10.1016%2fj.bdr.2021.100209&partnerID=40&md5=156e210ae966e9ae406bc3b308d82dd4},
abstract={Distributed stream processing engines are designed with a focus on scalability to process big data volumes in a continuous manner. We present the Theodolite method for benchmarking the scalability of distributed stream processing engines. Core of this method is the definition of use cases that microservices implementing stream processing have to fulfill. For each use case, our method identifies relevant workload dimensions that might affect the scalability of a use case. We propose to design one benchmark per use case and relevant workload dimension. We present a general benchmarking framework, which can be applied to execute the individual benchmarks for a given use case and workload dimension. Our framework executes an implementation of the use case's dataflow architecture for different workloads of the given dimension and various numbers of processing instances. This way, it identifies how resources demand evolves with increasing workloads. Within the scope of this paper, we present 4 identified use cases, derived from processing Industrial Internet of Things data, and 7 corresponding workload dimensions. We provide implementations of 4 benchmarks with Kafka Streams and Apache Flink as well as an implementation of our benchmarking framework to execute scalability benchmarks in cloud environments. We use both for evaluating the Theodolite method and for benchmarking Kafka Streams' and Flink's scalability for different deployment options. © 2021 Elsevier Inc.},
author_keywords={Benchmarking;  Microservices;  Scalability;  Stream processing},
keywords={Computer architecture;  Distributed parameter control systems;  Engines;  Scalability, Cloud environments;  Data volume;  Data-flow architectures;  Distributed stream processing;  IT Identify;  Microservice;  Resource demands;  Stream processing;  Stream processing engines, Benchmarking},
publisher={Elsevier Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Silvestre20211637,
author={Silvestre, P.F. and Fragkoulis, M. and Spinellis, D. and Katsifodimos, A.},
title={Clonos: Consistent Causal Recovery for Highly-Available Streaming Dataflows},
journal={Proceedings of the ACM SIGMOD International Conference on Management of Data},
year={2021},
pages={1637-1650},
doi={10.1145/3448016.3457320},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108976394&doi=10.1145%2f3448016.3457320&partnerID=40&md5=450488724ec5ab16027af3e2ce85d647},
abstract={Stream processing lies in the backbone of modern businesses, being employed for mission critical applications such as real-time fraud detection, car-trip fare calculations, traffic management, and stock trading. Large-scale applications are executed by scale-out stream processing systems on thousands of long-lived operators, which are subject to failures. Recovering from failures fast and consistently are both top priorities, yet they are only partly satisfied by existing fault tolerance methods due to the strong assumptions these make. In particular, prior solutions fail to address consistency in the presence of nondeterminism, such as calls to external services, asynchronous timers and processing-time windows. This paper describes Clonos, a fault tolerance approach that achieves fast, local operator recovery with exactly-once guarantees and high availability by instantly switching to passive standby operators. Clonos enforces causally consistent recovery, including output deduplication, by tracking nondeterminism within the system through causal logging. To implement Clonos we re-engineered many of the internal subsystems of a state of the art stream processor. We evaluate Clonos' overhead and recovery on the Nexmark benchmark against Apache Flink. Clonos achieves instant recovery with negligible overhead and, unlike previous work, does not make assumptions on the deterministic nature of operators. © 2021 Owner/Author.},
author_keywords={cloud computing;  consistency;  exactly-once;  fault-tolerance;  high-availability;  stream processing},
keywords={Fault tolerance, Fare calculation;  High availability;  Large-scale applications;  Mission critical applications;  Stream processing;  Stream processing systems;  Tolerance approach;  Traffic management, Recovery},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kallas2020,
author={Kallas, K. and Niksic, F. and Stanford, C. and Alur, R.},
title={DiffStream: Differential output testing for stream processing programs},
journal={Proceedings of the ACM on Programming Languages},
year={2020},
volume={4},
number={OOPSLA},
doi={10.1145/3428221},
art_number={153},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097584469&doi=10.1145%2f3428221&partnerID=40&md5=978cd10cfeb9fd500dfe2f286cd82f0e},
abstract={High performance architectures for processing distributed data streams, such as Flink, Spark Streaming, and Storm, are increasingly deployed in emerging data-driven computing systems. Exploiting the parallelism afforded by such platforms, while preserving the semantics of the desired computation, is prone to errors, and motivates the development of tools for specification, testing, and verification. We focus on the problem of differential output testing for distributed stream processing systems, that is, checking whether two implementations produce equivalent output streams in response to a given input stream. The notion of equivalence allows reordering of logically independent data items, and the main technical contribution of the paper is an optimal online algorithm for checking this equivalence. Our testing framework is implemented as a library called DiffStream in Flink. We present four case studies to illustrate how our framework can be used to (1) correctly identify bugs in a set of benchmark MapReduce programs, (2) facilitate the development of difficult-to-parallelize high performance applications, and (3) monitor an application for a long period of time with minimal performance overhead. © 2020 Owner/Author.},
author_keywords={differential testing;  runtime verification;  stream processing},
keywords={Application programs;  Benchmarking;  Data streams;  Distributed parameter control systems;  Semantics, Differential output;  Distributed data streams;  Distributed stream processing;  High performance applications;  High performance architectures;  On-line algorithms;  Stream processing;  Technical contribution, Program debugging},
publisher={Association for Computing Machinery},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Akanbi20201,
author={Akanbi, A. and Masinde, M.},
title={A distributed stream processing middleware framework for real-time analysis of heterogeneous data on big data platform: Case of environmental monitoring},
journal={Sensors (Switzerland)},
year={2020},
volume={20},
number={11},
pages={1-25},
doi={10.3390/s20113166},
art_number={3166},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086008293&doi=10.3390%2fs20113166&partnerID=40&md5=5ae9d0025fac84afc53504b5af99ef64},
abstract={In recent years, the application and wide adoption of Internet of Things (IoT)-based technologies have increased the proliferation of monitoring systems, which has consequently exponentially increased the amounts of heterogeneous data generated. Processing and analysing the massive amount of data produced is cumbersome and gradually moving from classical ‘batch’ processing—extract, transform, load (ETL) technique to real-time processing. For instance, in environmental monitoring and management domain, time-series data and historical dataset are crucial for prediction models. However, the environmental monitoring domain still utilises legacy systems, which complicates the real-time analysis of the essential data, integration with big data platforms and reliance on batch processing. Herein, as a solution, a distributed stream processing middleware framework for real-time analysis of heterogeneous environmental monitoring and management data is presented and tested on a cluster using open source technologies in a big data environment. The system ingests datasets from legacy systems and sensor data from heterogeneous automated weather systems irrespective of the data types to Apache Kafka topics using Kafka Connect APIs for processing by the Kafka streaming processing engine. The stream processing engine executes the predictive numerical models and algorithms represented in event processing (EP) languages for real-time analysis of the data streams. To prove the feasibility of the proposed framework, we implemented the system using a case study scenario of drought prediction and forecasting based on the Effective Drought Index (EDI) model. Firstly, we transform the predictive model into a form that could be executed by the streaming engine for real-time computing. Secondly, the model is applied to the ingested data streams and datasets to predict drought through persistent querying of the infinite streams to detect anomalies. As a conclusion of this study, a performance evaluation of the distributed stream processing middleware infrastructure is calculated to determine the real-time effectiveness of the framework. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Apache Kafka;  Big data;  Drought;  Internet of Things;  Middleware;  Stream processing},
keywords={Big data;  Data streams;  Distributed parameter control systems;  Drought;  Engines;  Environmental management;  Environmental technology;  Forecasting;  Internet of things;  Legacy systems;  Middleware;  Monitoring;  Real time systems, Distributed stream processing;  Effective drought index (EDI);  Environmental Monitoring;  Internet of Things (IOT);  Middleware infrastructure;  Open-source technology;  Prediction and forecasting;  Stream processing engines, Batch data processing},
publisher={MDPI AG},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bordin2020222900,
author={Bordin, M.V. and Griebler, D. and Mencagli, G. and Geyer, C.F.R. and Fernandes, L.G.L.},
title={DSPBench: A Suite of Benchmark Applications for Distributed Data Stream Processing Systems},
journal={IEEE Access},
year={2020},
volume={8},
pages={222900-222917},
doi={10.1109/ACCESS.2020.3043948},
art_number={9290133},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097964214&doi=10.1109%2fACCESS.2020.3043948&partnerID=40&md5=8a241fcc62b85742fa4090ae2d523c07},
abstract={Systems enabling the continuous processing of large data streams have recently attracted the attention of the scientific community and industrial stakeholders. Data Stream Processing Systems (DSPSs) are complex and powerful frameworks able to ease the development of streaming applications in distributed computing environments like clusters and clouds. Several systems of this kind have been released and currently maintained as open source projects, like Apache Storm and Spark Streaming. Some benchmark applications have often been used by the scientific community to test and evaluate new techniques to improve the performance and usability of DSPSs. However, the existing benchmark suites lack of representative workloads coming from the wide set of application domains that can leverage the benefits offered by the stream processing paradigm in terms of near real-time performance. The goal of this article is to present a new benchmark suite composed of 15 applications coming from areas like Finance, Telecommunications, Sensor Networks, Social Networks and others. This article describes in detail the nature of these applications, their full workload characterization in terms of selectivity, processing cost, input size and overall memory occupation. In addition, it exemplifies the usefulness of our benchmark suite to compare real DSPSs by selecting Apache Storm and Spark Streaming for this analysis. © 2013 IEEE.},
author_keywords={apache storm;  benchmarking;  big data;  Data stream processing;  spark streaming},
keywords={Computer systems programming;  Data streams;  Distributed computer systems;  Open source software;  Open systems;  Sensor networks;  Storms, Benchmark applications;  Continuous processing;  Data stream processing;  Distributed computing environment;  Distributed data stream processing;  High-level programming;  Streaming applications;  Workload characterization, Benchmarking},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gautam2019,
author={Gautam, B. and Basava, A.},
title={Performance prediction of data streams on high-performance architecture},
journal={Human-centric Computing and Information Sciences},
year={2019},
volume={9},
number={1},
doi={10.1186/s13673-018-0163-4},
art_number={2},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059645388&doi=10.1186%2fs13673-018-0163-4&partnerID=40&md5=a6c325b9baea97dd169480099f2f4e80},
abstract={Worldwide sensor streams are expanding continuously with unbounded velocity in volume, and for this acceleration, there is an adaptation of large stream data processing system from the homogeneous to rack-scale architecture which makes serious concern in the domain of workload optimization, scheduling, and resource management algorithms. Our proposed framework is based on providing architecture independent performance prediction model to enable resource adaptive distributed stream data processing platform. It is comprised of seven pre-defined domain for dynamic data stream metrics including a self-driven model which tries to fit these metrics using ridge regularization regression algorithm. Another significant contribution lies in fully-automated performance prediction model inherited from the state-of-the-art distributed data management system for distributed stream processing systems using Gaussian processes regression that cluster metrics with the help of dimensionality reduction algorithm. We implemented its base on Apache Heron and evaluated with proposed Benchmark Suite comprising of five domain-specific topologies. To assess the proposed methodologies, we forcefully ingest tuple skewness among the benchmarking topologies to set up the ground truth for predictions and found that accuracy of predicting the performance of data streams increased up to 80.62% from 66.36% along with the reduction of error from 37.14 to 16.06%. © 2019, The Author(s).},
author_keywords={Apache Heron;  Clustering;  Data streams;  High performance computing;  Performance behavior;  Performance prediction;  Regression;  Stream benchmark suite},
keywords={Benchmarking;  Clustering algorithms;  Computer architecture;  Dimensionality reduction;  Distributed parameter control systems;  Forecasting;  Information management;  Reduction;  Regression analysis;  Scheduling;  Topology, Apache Heron;  Benchmark suites;  Clustering;  High performance computing;  Performance behavior;  Performance prediction;  Regression, Data streams},
publisher={Springer Berlin Heidelberg},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lombardi2019342,
author={Lombardi, F. and Muti, A. and Aniello, L. and Baldoni, R. and Bonomi, S. and Querzoni, L.},
title={PASCAL: An architecture for proactive auto-scaling of distributed services},
journal={Future Generation Computer Systems},
year={2019},
volume={98},
pages={342-361},
doi={10.1016/j.future.2019.03.003},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063744271&doi=10.1016%2fj.future.2019.03.003&partnerID=40&md5=aea2d0c898578c7de03b430ba45ce559},
abstract={One of the main characteristics that today makes cloud services so popular is their ability to be elastic, i.e., they can adapt their provisioning to variable workloads, thus increasing resource utilization and reducing operating costs. At the core of any elastic service lies an automatic scaling mechanism that drives provisioning on the basis of a given strategy. In this paper we propose PASCAL, an architecture for Proactive Auto-SCALing of generic distributed services. PASCAL combines a proactive approach, to forecast incoming workloads, with a profiling system, to estimate required provision. Scale-in/out operations are decided according to an application-specific strategy, which aims at provisioning the minimum number of resources needed to sustain the foreseen workload. The main novelties introduced with PASCAL architecture are: (i) a strategy to proactively auto-scale a distributed stream processing system (namely, Apache Storm) with the aim of load balancing operators through an accurate system performance estimation model, and (ii) a strategy to proactively auto-scale a distributed datastore (namely, Apache Cassandra), focused on how to choose when executing scaling actions on the basis of the time needed for the activation/deactivation of storage nodes so as to have the configuration ready when needed. We provide a prototype implementation of PASCAL for both use cases and, through an experimental evaluation conducted on a private cloud, we validate our approach and demonstrate the effectiveness of the proposed strategies in terms of saved resources and response time. © 2019 Elsevier B.V.},
author_keywords={Automatic scaling;  Cassandra;  Cloud;  Distributed storage;  Elasticity;  Storm;  Stream processing},
keywords={Clouds;  Distributed parameter control systems;  Elasticity;  Operating costs;  Storms, Application-specific strategies;  Automatic scaling;  Cassandras;  Distributed storage;  Distributed stream processing;  Experimental evaluation;  Prototype implementations;  Stream processing, Architecture},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Javed2018223,
author={Javed, M.H. and Lu, X. and Panda, D.K.},
title={Cutting the Tail: Designing High Performance Message Brokers to Reduce Tail Latencies in Stream Processing},
journal={Proceedings - IEEE International Conference on Cluster Computing, ICCC},
year={2018},
volume={2018-September},
pages={223-233},
doi={10.1109/CLUSTER.2018.00040},
art_number={8514883},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057288777&doi=10.1109%2fCLUSTER.2018.00040&partnerID=40&md5=76ca6fa088f0bbc43eedbdee28f85532},
abstract={Over the last decade, organizations have become heavily reliant on providing near-instantaneous insights to the end user based on vast amounts of data collected from various sources in real-time. In order to accomplish this task, a stream processing pipeline is constructed, which in its most basic form, consists of a Stream Processing Engine (SPE) and a Message Broker (MB). The SPE is responsible for performing actual computations on the data and providing insights from it. MB, on the other hand, acts as an intermediate queue to which data is written by ephemeral sources and then fetched by the SPE to perform computations on. Due to the inherent real-time nature of such a pipeline, low latency is a highly desirable feature for them. Thus, several existing research works in the community focus on improving latency and throughput of the streaming pipeline. However, there is a dearth of studies optimizing the tail latencies of such pipelines. Moreover, the root cause of this high tail latency is still vague. In this paper, we propose a model-based approach to analyze in-depth the reasons behind high tail latency in streaming systems such as Apache Kafka. Having found the MB to be a major contributor of messages with high tail latencies in a streaming pipeline, we design and implement an RDMA-enhanced high-performance MB, called Frieda, with the higher goal of accelerating any arbitrary stream processing pipeline regardless of the SPE used. Our experiments show a reduction of up to 98% in 99.9th percentile latency for microbenchmarks and up to 31% for full-fledged stream processing pipeline constructed using Yahoo! Streaming Benchmark. © 2018 IEEE.},
author_keywords={Kafka;  Message Broker;  RDMA;  Stream Processing;  Tail Latency},
keywords={Cluster computing;  Computer architecture;  Cutting;  Pipelines, Kafka;  Message brokers;  RDMA;  Stream processing;  Tail Latency, Pipeline processing systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kauffman201854,
author={Kauffman, S. and Havelund, K. and Joshi, R. and Fischmeister, S.},
title={Inferring event stream abstractions},
journal={Formal Methods in System Design},
year={2018},
volume={53},
number={1},
pages={54-82},
doi={10.1007/s10703-018-0317-z},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042612651&doi=10.1007%2fs10703-018-0317-z&partnerID=40&md5=7367eb1197d34fe37f7f90aa1639322e},
abstract={We propose a formalism for specifying event stream abstractions for use in spacecraft telemetry processing. Our work is motivated by the need to quickly process streams with millions of events generated e.g. by the Curiosity rover on Mars. The approach builds a hierarchy of event abstractions for telemetry visualization and querying to aid human comprehension. Such abstractions can also be used as input to other runtime verification tools. Our notation is inspired by Allen’s Temporal Logic, and provides a rule-based declarative way to express event abstractions. We present an algorithm for applying specifications to an event stream and explore modifications to improve the algorithm’s asymptotic complexity. The system is implemented in both Scala and C, with the specification language implemented as internal as well as external DSLs. We illustrate the solution with several examples, a performance evaluation, and a real telemetry analysis scenario. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Allen logic;  Event stream processing;  Runtime verification;  Telemetry comprehension;  Temporal logic},
keywords={Abstracting;  C (programming language);  Specification languages;  Specifications;  Telemetering equipment;  Temporal logic, Allen logic;  Asymptotic complexity;  Event stream processing;  Human comprehensions;  Performance evaluations;  Process streams;  Run-time verification;  Telemetry processing, Computer circuits},
publisher={Springer New York LLC},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Steffl2017137,
author={Steffl, S. and Reda, S.},
title={LACore: A supercomputing-like linear algebra accelerator for SoC-based designs},
journal={Proceedings - 35th IEEE International Conference on Computer Design, ICCD 2017},
year={2017},
pages={137-144},
doi={10.1109/ICCD.2017.29},
art_number={8119202},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041688653&doi=10.1109%2fICCD.2017.29&partnerID=40&md5=4f7647eb41f872aa6733ebb3f01a22b3},
abstract={Linear algebra operations are at the heart of scientific computing solvers, machine learning and artificial intelligence. In this paper, LACore, a novel, programmable accelerator architecture for general-purpose linear algebra applications, is presented. LACore enables many of the architectural features typically available in custom supercomputing machines in an accelerator form factor that can be deployed in System-On-a-Chip (SoC) based designs. LACore has several architectural features including heterogeneous data-streaming LAMemUnits, a configurable systolic datapath that supports scalar, vector and multi-stream output modes, and a decoupled architecture that overlap memory transfer and execution. To evaluate LACore, we implemented its architecture as an extension to the RISC-V ISA in the gem5 cycle-accurate simulator. The LACore ISA was implemented in gcc, and a C-programming software framework, the LACoreAPI, has been developed for high-level programming of the LACore. Using the HPCC benchmark suite, we compare our LACore architecture against three other platforms: an in-order RISC-V CPU, a superscalar x86 CPU with SSE2, and a scaled NVIDIA Fermi GPU. The LACore outperforms the superscalar x86 processor in the benchmark suite by an average of 3.43x, and outperforms the scaled Fermi GPU by an average of 12.04x, within the same or less design area. © 2017 IEEE.},
author_keywords={Accelerator Architectures;  Heterogeneous Computing;  Linear Algebra},
keywords={Algebra;  Artificial heart;  Artificial intelligence;  C (programming language);  Computer programming;  Graphics processing unit;  Learning systems;  Linear algebra;  Object oriented programming;  Programmable logic controllers;  System-on-chip, Accelerator architectures;  Architectural features;  Cycle-accurate simulators;  Decoupled architecture;  Heterogeneous computing;  Heterogeneous data;  High-level programming;  Linear algebra operations, Computer architecture},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ravindra201791,
author={Ravindra, S. and Dayarathna, M. and Jayasena, S.},
title={Latency aware elastic switching-based stream processing over compressed data streams},
journal={ICPE 2017 - Proceedings of the 2017 ACM/SPEC International Conference on Performance Engineering},
year={2017},
pages={91-102},
doi={10.1145/3030207.3030227},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019011610&doi=10.1145%2f3030207.3030227&partnerID=40&md5=140e51db5b98de451fcb0d0e3f81fded},
abstract={Elastic scaling of event stream processing systems has gained significant attention recently due to the prevalence of cloud computing technologies. We investigate on the complexities associated with elastic scaling of an event processing system in a private/public cloud scenario. We develop an Elastic Switching Mechanism (ESM) which reduces the overall average latency of event processing jobs by significant amount considering the cost of operating the system. ESM is augmented with adaptive compressing of upstream data. The ESM conducts one of the two types of switching where either part of the data is sent to the public cloud (data switching) or a selected query is sent to the public cloud (query switching) based on the characteristics of the query. We model the operation of the ESM as the function of two binary switching functions. We show that our elastic switching mechanism with compression is capable of handling out-of-order events more efficiently compared to techniques which does not involve compression. We used two application benchmarks called EmailProcessor and a Social Networking Benchmark (SNB2016) to conduct multiple experiments to evaluate the effectiveness of our approach. In a single query deployment with EmailProcessor benchmark we observed that our elastic switching mechanism provides 1.24 seconds average latency improvement per processed event which is 16.70% improvement compared to private cloud only deployment. When presented the option of scaling EmailProcessor with four public cloud VMs ESM further reduced the average latency by 37.55% compared to the single public cloud VM. In a multi-query deployment with both EmailProcessor and SNB2016 we obtained a reduction of average latency of both the queries by 39.61 seconds which is a decrease of 7% of overall latency. These performance figures indicate that our elastic switching mechanism with compressed data streams can effectively reduce the average elapsed time of stream processing happening in private/public clouds. © 2017 ACM.},
author_keywords={Cloud computing;  Compressed event processing;  Data compression;  Elastic data stream processing;  Event-based systems;  Iass;  Software performance engineering;  System sizing and capacity planning},
keywords={Benchmarking;  Cloud computing;  Data communication systems;  Data compression;  Distributed computer systems;  Query processing;  Switching, Capacity planning;  Data stream processing;  Event Processing;  Event-based system;  Iass;  Software performance engineerings, Data handling},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kritikakis2016,
author={Kritikakis, C. and Chrysos, G. and Dollas, A. and Pnevmatikatos, D.N.},
title={An FPGA-based high-throughput stream join architecture},
journal={FPL 2016 - 26th International Conference on Field-Programmable Logic and Applications},
year={2016},
doi={10.1109/FPL.2016.7577354},
art_number={7577354},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994908348&doi=10.1109%2fFPL.2016.7577354&partnerID=40&md5=1bed25655eb8320cd726d51647910c95},
abstract={Stream join is a fundamental operation that combines information from different high-speed and high-volume data streams. This paper presents an FPGA-based architecture that maps the most performance-efficient stream join algorithm, i.e. ScaleJoin, to reconfigurable logic. The system was fully implemented on a Convey HC-2ex hybrid computer and the experimental performance evaluation shows that the proposed system outperforms by up to one order of magnitude the corresponding fully optimized parallel software-based solution running on a high-end 48-core multiprocessor platform. The proposed architecture can be used as a generic template for mapping stream processing algorithms to reconfigurable logic, taking into consideration real-world challenges. © 2016 EPFL.},
author_keywords={FPGA architecture;  join operator;  ScaleJoin;  stream processing},
keywords={Computer architecture;  Computer circuits;  Field programmable gate arrays (FPGA);  Hybrid computers;  Reconfigurable architectures, Experimental performance evaluations;  FPGA architectures;  FPGA-based architectures;  Join operators;  Multi-processor platforms;  Proposed architectures;  ScaleJoin;  Stream processing, Reconfigurable hardware},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Martin2014270,
author={Martin, A. and Marinho, R. and Brito, A. and Fetzer, C.},
title={Predicting energy consumption with StreamMine3G},
journal={DEBS 2014 - Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems},
year={2014},
pages={270-275},
doi={10.1145/2611286.2611325},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903196373&doi=10.1145%2f2611286.2611325&partnerID=40&md5=ef9336b27017e42691143b39b4a25cd3},
abstract={In this paper, we present our approach on solving the DEBS Grand Challenge using StreamMine3G, a distributed, highly scalable, elastic and fault tolerant ESP system. We will provide an overview about the system architecture of Stream-Mine3G and implementation details of an application aimed at consumption prediction and outlier detection. Using our elastic approach, we can provide an accurate prediction as we can keep a practically unbounded history able to deal with high volume, highly fluctuating workloads. Our system also provides techniques for dealing with incomplete data in the source stream, which is a common problem when processing data from a large number of sources. Finally, we provide performance measurements showing that we are able to process the dataset given as part of the 2014 DEBS Challenge (135 GB) at a throughput of up to 40 kEvents/s. © 2014 ACM.},
author_keywords={CEP;  complex event processing;  ESP;  event stream processing;  fault tolerance;  migration;  scalability;  state management},
keywords={Data handling;  Energy utilization;  Fault tolerance;  Scalability;  Software architecture, CEP;  Complex event processing;  ESP;  Event stream processing;  migration;  State management, Electric load forecasting},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Whittier20132,
author={Whittier, J.C. and Nittel, S. and Plummer, M.A. and Liang, Q.},
title={Towards window stream queries over continuous phenomena},
journal={Proceedings of the 4th ACM SIGSPATIAL International Workshop on GeoStreaming, IWGS 2013},
year={2013},
pages={2-11},
doi={10.1145/2534303.2534305},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894627672&doi=10.1145%2f2534303.2534305&partnerID=40&md5=98f7d0bd3c7b03c4884c271e8fc9921a},
abstract={Technological advances have created an unprecedented availability of inexpensive sensors capable of streaming environmental data in real-time. Data stream engines (DSE) with tuple processing rates of around 500k tuples/s have demonstrated their ability to both keep up with large numbers of spatio-temporal data streams, and execute stream window queries over them efficiently. Typically, geographically distributed sensors take samples asynchronously; however, when approximating the reality of a continuous phenomenon - - such as the radiation field over an urban region- the objective is to integrate their values correctly over space as well as over time. This paper presents an approach to extend DSEs with support enabling sliding window queries over dynamic continuous phenomena, which return both spatio-temporal snapshot and movies as window query results. We introduce a novel grid-pane index as a main memory index structure shared between multi-queries over a phenomenon and an adaptive, data driven kNN algorithm for efficiently approximating cells based on available stream data samples. AkNN implements a spatio-temporal inverse distance weighting interpolation (IDW) method that integrates time with space via an anisotropic ratio. Further, we introduce the shell list template that allows quick calculation of NN cells by distance in a space-time (ST) cuboid. We performed extensive performance evaluations using the Fukushima nuclear event in March 2011 as a test data set. © 2013 ACM.},
author_keywords={continuous phenomena;  data streams system;  main memory spatio-temporal index;  panes;  scalable spatio-temporal interpolation;  sensor data streams;  stream queries},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mazur20111102,
author={Mazur, E. and Li, B. and Diao, Y. and Shenoy, P.},
title={Towards scalable one-pass analytics using MapReduce},
journal={IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
year={2011},
pages={1102-1111},
doi={10.1109/IPDPS.2011.251},
art_number={6008898},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455229796&doi=10.1109%2fIPDPS.2011.251&partnerID=40&md5=95a427475127ad843594dfd601a4b4b8},
abstract={An integral part of many data-intensive applications is the need to collect and analyze enormous datasets efficiently. Concurrent with such application needs is the increasing adoption of MapReduce as a programming model for processing large datasets using a cluster of machines. Current MapReduce systems, however, require the data set to be loaded into the cluster before running analytical queries, and thereby incur high delays to start query processing. Furthermore, existing systems are geared towards batch processing. In this paper, we seek to answer a fundamental question: what architectural changes are necessary to bring the benefits of the MapReduce computation model to incremental, one-pass analytics, i.e., to support stream processing and online aggregation? To answer this question, we first conduct a detailed empirical performance study of current MapReduce implementations including Hadoop and MapReduce Online using a variety of workloads. By doing so, we identify several drawbacks of existing systems for one-pass analytics. Based on the insights from our study, we list key design requirements for incremental one-pass analytics and argue for architectural changes of MapReduce systems to overcome their current limitations. We conclude by sketching an initial design of our new MapReduce-based platform for incremental one-pass analytics and showing promising preliminary results. © 2011 IEEE.},
author_keywords={Data streams;  MapReduce;  Parallel data processing;  Performance analysis},
keywords={Analytical queries;  Architectural changes;  Computation model;  Current limitation;  Data sets;  Data stream;  Data-intensive application;  Design requirements;  Empirical performance;  Existing systems;  Initial design;  Integral part;  Large datasets;  Map-reduce;  One-pass;  Online aggregations;  Parallel data processing;  Performance analysis;  Programming models;  Stream processing, Batch data processing;  Computer programming;  Data handling;  Distributed parameter networks;  Search engines, Parallel processing systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pianese2007317,
author={Pianese, F. and Perino, D.},
title={Resource and locality awareness in an incentive-based P2P live streaming system},
journal={Proceedings of the 2007 Workshop on Peer-to-Peer Streaming and IP-TV, P2P-TV'07},
year={2007},
pages={317-322},
doi={10.1145/1326320.1326323},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149123281&doi=10.1145%2f1326320.1326323&partnerID=40&md5=c5d52fc736a605d5b5bba34de30a720b},
abstract={One of the main challenges in P2P live streaming is the efficient allocation of the available resources. This paper presents an experimental evaluation of the effects of a local pairwise incentive mechanism applied to an unstructured mesh-based architecture. We focus on the relationship between resource availability in the system and the average quality of its data distribution paths, both in terms of bandwidth efficiency and awareness to network locality. We show via large scale testbed experiments based on the PULSE live streaming system that the introduction of appropriate incentive-based policies as the main peer selection mechanism can lead to a global content distribution mesh which has properties similar to tree-based structured systems. © 2007 ACM.},
keywords={Bandwidth efficiency;  Content distribution;  Experimental evaluation;  Incentive mechanism;  ITS data;  Live streaming;  Locality awareness;  Network locality;  Peer selection;  Resource availability;  Structured systems;  Tree-based, Distributed computer systems;  Video streaming, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tu200469,
author={Tu, Y.-C. and Sun, J. and Prabhakar, S.},
title={Performance analysis of a hybrid media streaming system},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2004},
volume={5305},
pages={69-82},
doi={10.1117/12.538806},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-8844231005&doi=10.1117%2f12.538806&partnerID=40&md5=2f0428888b027d988ee61cde226455fc},
abstract={Recent research efforts have demonstrated the promising potential of building cost-effective media streaming systems on top of peer-to-peer (P2P) networks. A P2P media streaming architecture can reach large size and streaming capacity that are difficult to achieve in conventional server-based streaming services. Hybrid streaming systems that combine the use of dedicated streaming servers and P2P networks were proposed to build on the advantages of both paradigms. However, the dynamics of such systems and the impact of various factors on system behaviors are not totally clear. In this paper, we present an analytical framework to quantitatively study the features of a hybrid media streaming model. Based on this framework, we derive an equation to describe the capacity growth of a single-file streaming system. We then extend the analysis to multi-file scenarios by solving an optimization problem. We also show that the system model achieves optimal allocation of server bandwidth among different media objects. The unpredictable departure/failure of peers is a critical factor that affects performance of P2P systems. To model peer failures in our system, we propose the concept of peer lifespan. The original equation is enhanced with coefficients generated from the distribution of peer lifespan. Results from large-scale simulations support our analysis.},
author_keywords={Content Distribution Networks;  Hybrid system;  Mathematical analysis;  Media streaming;  Peer-to-peer networks},
keywords={Algorithms;  Computer simulation;  Discrete time control systems;  Internet;  Mathematical models;  Quality of service;  Sensitivity analysis;  Servers, Content distribution networks;  Hybrid system;  Mathematical analysis;  Media streaming;  Peer-to-peer networks, Multimedia systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jonathan2020221,
author={Jonathan, A. and Chandra, A. and Weissman, J.},
title={WASP: Wide-area Adaptive Stream Processing},
journal={Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference},
year={2020},
pages={221-235},
doi={10.1145/3423211.3425668},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098528268&doi=10.1145%2f3423211.3425668&partnerID=40&md5=cb7339e3ee4026008c63896b73abe48d},
abstract={Adaptability is critical for stream processing systems to ensure stable, low-latency, and high-throughput processing of long-running queries. Such adaptability is particularly challenging for wide-area stream processing due to the highly dynamic nature of the wide-area environment, which includes unpredictable workload patterns, variable network bandwidth, occurrence of stragglers, and failures. Unfortunately, existing adaptation techniques typically achieve these performance goals by compromising the quality/accuracy of the results, and they are often application-dependent. In this work, we rethink the adaptability property of wide-area stream processing systems and propose a resource-aware adaptation framework, called WASP. WASP adapts queries through a combination of multiple techniques: task re-assignment, operator scaling, and query re-planning, and applies them in a WAN-aware manner. It is able to automatically determine which adaptation action to take depending on the type of queries, dynamics, and optimization goals. We have implemented a WASP prototype on Apache Flink. Experimental evaluation with the YSB benchmark and a real Twitter trace shows that WASP can handle various dynamics without compromising the quality of the results. © 2020 Association for Computing Machinery.},
keywords={Benchmarking;  Middleware, Adaptation framework;  Adaptation techniques;  Experimental evaluation;  Network bandwidth;  Optimization goals;  Stream processing;  Stream processing systems;  Workload patterns, Wide area networks},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rodrigo2019223,
author={Rodrigo, A. and Dayarathna, M. and Jayasena, S.},
title={Latency-Aware Secure Elastic Stream Processing with Homomorphic Encryption},
journal={Data Science and Engineering},
year={2019},
volume={4},
number={3},
pages={223-239},
doi={10.1007/s41019-019-00100-5},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074188166&doi=10.1007%2fs41019-019-00100-5&partnerID=40&md5=cb1cce520b4ff041a24afdf594bc21c5},
abstract={Increasingly organizations are elastically scaling their stream processing applications into the infrastructure as a service clouds. However, state-of-the-art approaches for elastic stream processing do not consider the potential threats of exposing their data to third parties in cloud environments. We present the design and implementation of an Elastic Switching Mechanism for data stream processing which is based on homomorphic encryption (HomoESM). The HomoESM not only elastically scales data stream processing applications into public clouds but also preserves the privacy of such applications. Using a real-world test setup, which includes an E-mail Filter benchmark and a Web server access log processor benchmark (EDGAR), we demonstrate the effectiveness of our approach. Experiments on Amazon EC2 indicate that the proposed approach for homomorphic encryption provides a significant result which is 10–17% improvement in average latency in the case of E-mail Filter benchmark and EDGAR benchmark, respectively. Furthermore, EDGAR add/subtract operations, multiplication, and comparison operations showed up to 6.13%, 7.81%, and 26.17% average latency improvements, respectively. Finally, we evaluate the potential of scaling the homomorphic stream processor in the public cloud. These results indicate the potential for real-world deployments of secure elastic data stream processing applications. © 2019, The Author(s).},
author_keywords={Cloud computing;  Compressed event processing;  Data compression;  Elastic data stream processing;  IaaS;  System sizing and capacity planning},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Trotter2019136,
author={Trotter, M. and Wood, T. and Hwang, J.},
title={Forecasting a Storm: Divining Optimal Configurations using Genetic Algorithms and Supervised Learning},
journal={Proceedings - 2019 IEEE International Conference on Autonomic Computing, ICAC 2019},
year={2019},
pages={136-146},
doi={10.1109/ICAC.2019.00025},
art_number={8831217},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073254760&doi=10.1109%2fICAC.2019.00025&partnerID=40&md5=9e78264083035cd09730f94cd7d164e8},
abstract={With the advent of Big Data platforms like Apache Storm, computations once deemed infeasible locally become possible at scale. However, doing so entails orchestrating powerful yet expensive clusters. With its focus on stream processing, Storm optimizes for low-latency and high throughput. However, to realize this goal and thereby maximize the utility of these clusters' resources, operators must execute these tasks under their optimal configurations. Yet, the search space for finding such configurations is so vast and time-consuming to explore so as to be effectively intractable due to issues like the temporal overhead of testing new candidate configurations, the sheer number of permutations of parameters within each configuration and their interdependence among each other. In order to efficiently cover the search space, we automate the process with genetic algorithms. Moreover, we fuse this technique not only with additional cluster information gleaned from JMX profiling and Storm performance data but also with classifiers constructed from training data from past executions of a plethora of Storm topologies. Utilizing a diverse set of Storm benchmark topologies as evaluation data, we show that the fully enhanced genetic algorithms can efficiently find configurations that perform on average 4.67x better than 'rules of thumb'-derived manual baselines. Moreover, we demonstrate that our fully refined classifiers enhance the GA throughput on average across the topologies by 22% while reducing search time by a factor of 6.47x. © 2019 IEEE.},
author_keywords={Apache Storm;  Automatic Performance Tuning;  Genetic Algorithm;  Supervised Learning},
keywords={Genetic algorithms;  Learning algorithms;  Machine learning;  Storms;  Supervised learning;  Topology, Automatic performance tuning;  Data platform;  Enhanced genetic algorithms;  High throughput;  Performance data;  Search spaces;  Stream processing;  Training data, Classification (of information)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zvara2019578,
author={Zvara, Z. and Szabó, P.G.N. and Balázs, B. and Benczúr, A.},
title={Optimizing distributed data stream processing by tracing},
journal={Future Generation Computer Systems},
year={2019},
volume={90},
pages={578-591},
doi={10.1016/j.future.2018.06.047},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052855347&doi=10.1016%2fj.future.2018.06.047&partnerID=40&md5=cdbc458d6715b32da489dc63addfab39},
abstract={Heterogeneous mobile, sensor, IoT, smart environment, and social networking applications have recently started to produce unbounded, fast, and massive-scale streams of data that have to be processed “on the fly”. Systems that process such data have to be enhanced with detection for operational exceptions and with triggers for both automated and manual operator actions. In this paper, we illustrate how tracing in distributed data processing systems can be applied to detecting changes in data and operational environment to maintain the efficiency of heterogeneous data stream processing systems under potentially changing data quality and distribution. By the tracing of individual input records, we can (1) identify outliers in a web crawling and document processing system and use the insights to define URL filtering rules; (2) identify heavy keys, such as NULL, that should be filtered before processing; (3) give hints to improve the key-based partitioning mechanisms; and (4) measure the limits of overpartitioning if heavy thread-unsafe libraries are imported. By using Apache Spark as illustration, we show how various data stream processing efficiency issues can be mitigated or optimized by our distributed tracing engine. We describe and qualitatively compare two different designs, one based on reporting to a distributed database and another based on trace piggybacking. Our prototype implementation consists of wrappers suitable for JVM environments in general, with minimal impact on the source code of the core system. Our tracing framework is the first to solve tracing in multiple systems across boundaries and to provide detailed performance measurements suitable for automated optimization, not just debugging. © 2018 Elsevier B.V.},
author_keywords={Apache Spark;  Data provenance;  Data stream processing;  Distributed data processing;  Distributed tracing},
keywords={Efficiency;  Information retrieval systems;  Program debugging;  Web crawler, Data provenance;  Data stream processing;  Distributed data processing;  Distributed data stream processing;  Distributed tracing;  Operational environments;  Prototype implementations;  Social networking applications, Data handling},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ma201526,
author={Ma, K. and Yang, B.},
title={Live Data Replication Approach from Relational Tables to Schema-Free Collections Using Stream Processing Framework},
journal={Proceedings - 2015 10th International Conference on P2P, Parallel, Grid, Cloud and Internet Computing, 3PGCIC 2015},
year={2015},
pages={26-31},
doi={10.1109/3PGCIC.2015.64},
art_number={7424537},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964491297&doi=10.1109%2f3PGCIC.2015.64&partnerID=40&md5=eac363ff36d46f40a4a21354f9fb632a},
abstract={Recent researches focus on the data replication issue from relational tables to schema-free collections in a batch processing way. However, there are few publications on live data replication in real time. In this paper, we attempt to address this legacy issue with new stream processing framework. The process of replication consists of log-based change data capture and stream-based data replication. Data replication mappings are present, and the proposed architecture of stream processing framework including column grouping, column merging and column versioning, is introduced to avoid data lost in case of failure. Finally, our experimental evaluation of live data replication approach with stream processing framework shows the higher effectiveness and efficiency than current methods. © 2015 IEEE.},
author_keywords={data migration;  Data replication;  extract;  MapReduce;  NoSQL;  relational table;  stream processing;  transform and load},
keywords={Distributed computer systems;  Internet, Data migration;  Data replication;  extract;  Map-reduce;  NoSQL;  Relational tables;  Stream processing, Batch data processing},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ravishankar2012617,
author={Ravishankar, C. and Ananthanarayanan, S. and Garg, S. and Kennings, A.},
title={Analysis and evaluation of greedy thread swapping based dynamic power management for MPSoC platforms},
journal={Proceedings - International Symposium on Quality Electronic Design, ISQED},
year={2012},
pages={617-624},
doi={10.1109/ISQED.2012.6187557},
art_number={6187557},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863643182&doi=10.1109%2fISQED.2012.6187557&partnerID=40&md5=034cf1852f252bc0a34432da007bc12a},
abstract={Thread migration (TM) is a recently proposed dynamic power management technique for heterogeneous multi-processor system-on-chip (MPSoC) platforms that eliminates the area and power overheads incurred by fine-grained dynamic voltage and frequency scaling (DVFS) based power management. In this paper, we take the first step towards formally analyzing and experimentally evaluating the use of power-aware TM for parallel data streaming applications on MPSoC platforms. From an analysis perspective, we characterize the optimal mapping of threads to cores and prove the convergence properties of a complexity effective greedy thread swapping based TM algorithm to the globally optimal solution. The proposed techniques are evaluated on a 9-core FPGA based MPSoC prototype equipped with fully-functional TM and DVFS support, and running a parallelized video encoding benchmark based on the Motion Picture Experts Group (MPEG-2) standard. Our experimental results validate the proposed theoretical analysis, and show that the proposed TM algorithm provides within 8% of the DVFS performance under the same power budget, and assuming no overheads for DVFS. Assuming voltage regulator inefficiency of 80%, the proposed TM algorithm has 9% higher performance than DVFS, again under the same total power budget. © 2012 IEEE.},
author_keywords={DVFS;  FPGA;  Multi-core;  Power management;  Thread migration},
keywords={Analysis and evaluation;  Convergence properties;  DVFS;  Dynamic power management;  Dynamic voltage and frequency scaling;  Motion picture experts groups;  Multi core;  Multi processor system on chips;  Optimal mapping;  Optimal solutions;  Parallel data;  Power budgets;  Power managements;  Power overhead;  Power-aware;  Thread migration;  Thread swapping;  Total power;  Video encodings, Algorithms;  Data reduction;  Energy management;  Microprocessor chips;  Motion Picture Experts Group standards;  Multiprocessing systems;  Voltage regulators, Field programmable gate arrays (FPGA)},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ericson201133,
author={Ericson, K. and Pallickara, S.},
title={On the performance of distributed clustering algorithms in file and streaming processing systems},
journal={Proceedings - 2011 4th IEEE International Conference on Utility and Cloud Computing, UCC 2011},
year={2011},
pages={33-40},
doi={10.1109/UCC.2011.15},
art_number={6123478},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856364546&doi=10.1109%2fUCC.2011.15&partnerID=40&md5=f25317206db48155113327e8363ca039},
abstract={There is often a need to cluster voluminous amounts of data. Such clustering has application in fields such as pattern recognition, data mining, bioinformatics, and recommendation systems. Here we evaluate the performance of 4 clustering algorithms viz. K-means, Fuzzy k-means, Dirichlet, and Latent Dirichlet Allocation within two different cloud runtimes: Hadoop and Granules. Our benchmarks use identical clustering code with both Hadoop and Granules. The difference between these implementations stem from how the Hadoop and Granules runtimes (1) support and manage the lifecycle of individual computations, and (2) how they orchestrate exchange of data between different stages of the computational pipeline during successive iterations of the clustering algorithm. We also include an analysis of our results for each of these clustering algorithms in a distributed setting. © 2011 IEEE.},
author_keywords={Clustering;  Distributed Stream Processing;  Granules;  Hadoop;  Machine Learning;  Mahout},
keywords={Clustering;  Granules;  Hadoop;  Machine-learning;  Mahout;  Stream processing, Bioinformatics;  Cloud computing;  Cluster computing;  Distributed parameter control systems;  Granulation;  Pattern recognition systems;  Statistics, Clustering algorithms},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kuntschke2006769,
author={Kuntschke, R. and Kemper, A.},
title={Data stream sharing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={4254 LNCS},
pages={769-788},
doi={10.1007/11896548_58},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845263474&doi=10.1007%2f11896548_58&partnerID=40&md5=f6ccd406007c9745b3696a72d1853e29},
abstract={Recent research efforts in the fields of data stream processing and data stream management systems (DSMSs) show the increasing importance of processing data streams, e. g., in the e-science domain. Together with the advent of peer-to-peer (P2P) networks and grid computing, this leads to the necessity of developing new techniques for distributing and processing continuous queries over data streams in such networks. In this paper, we present a novel approach for optimizing the integration, distribution, and execution of newly registered continuous queries over data streams in grid-based P2P networks. We introduce Windowed XQuery (WXQuery), our XQuery-based subscription language for continuous queries over XML data streams supporting window-based operators. Concentrating on filtering and window-based aggregation, we present our stream sharing algorithms as well as experimental evaluation results from the astrophysics application domain to assess our approach. © Springer-Verlag Berlin Heidelberg 2006.},
keywords={Algorithms;  Computation theory;  Computer networks;  Distributed computer systems;  Query languages;  XML, Data streams;  Grid computing;  Peer-to-peer (P2P) networks;  Window-based aggregation, Data reduction},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Henning202185,
author={Henning, S. and Hasselbring, W.},
title={How to measure scalability of distributed stream processing engines?},
journal={ICPE 2021 - Companion of the ACM/SPEC International Conference on Performance Engineering},
year={2021},
pages={85-88},
doi={10.1145/3447545.3451190},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104992476&doi=10.1145%2f3447545.3451190&partnerID=40&md5=b51b8b5c241ab332266ae392185f6ec2},
abstract={Scalability is promoted as a key quality feature of modern big data stream processing engines. However, even though research made huge efforts to provide precise definitions and corresponding metrics for the term scalability, experimental scalability evaluations or benchmarks of stream processing engines apply different and inconsistent metrics. With this paper, we aim to establish general metrics for scalability of stream processing engines. Derived from common definitions of scalability in cloud computing, we propose two metrics: a load capacity function and a resource demand function. Both metrics relate provisioned resources and load intensities, while requiring specific service level objectives to be fulfilled. We show how these metrics can be employed for scalability benchmarking and discuss their advantages in comparison to other metrics, used for stream processing engines and other software systems. © 2021 Association for Computing Machinery.},
author_keywords={Cloud computing;  Metrics;  Scalability;  Stream processing},
keywords={Data streams;  Distributed parameter control systems;  Engines;  Functions, Data stream processing;  Distributed stream processing;  Precise definition;  Resource demands;  Scalability evaluation;  Service level objective;  Software systems;  Stream processing engines, Scalability},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Koo2020,
author={Koo, J. and Faseeh Qureshi, N.M. and Siddiqui, I.F. and Abbas, A. and Bashir, A.K.},
title={IoT-enabled directed acyclic graph in spark cluster},
journal={Journal of Cloud Computing},
year={2020},
volume={9},
number={1},
doi={10.1186/s13677-020-00195-6},
art_number={50},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090902034&doi=10.1186%2fs13677-020-00195-6&partnerID=40&md5=90539dfa562271883a15e6a76a3f6f9d},
abstract={Real-time data streaming fetches live sensory segments of the dataset in the heterogeneous distributed computing environment. This process assembles data chunks at a rapid encapsulation rate through a streaming technique that bundles sensor segments into multiple micro-batches and extracts into a repository, respectively. Recently, the acquisition process is enhanced with an additional feature of exchanging IoT devices’ dataset comprised of two components: (i) sensory data and (ii) metadata. The body of sensory data includes record information, and the metadata part consists of logs, heterogeneous events, and routing path tables to transmit micro-batch streams into the repository. Real-time acquisition procedure uses the Directed Acyclic Graph (DAG) to extract live query outcomes from in-place micro-batches through MapReduce stages and returns a result set. However, few bottlenecks affect the performance during the execution process, such as (i) homogeneous micro-batches formation only, (ii) complexity of dataset diversification, (iii) heterogeneous data tuples processing, and (iv) linear DAG workflow only. As a result, it produces huge processing latency and the additional cost of extracting event-enabled IoT datasets. Thus, the Spark cluster that processes Resilient Distributed Dataset (RDD) in a fast-pace using Random access memory (RAM) defies expected robustness in processing IoT streams in the distributed computing environment. This paper presents an IoT-enabled Directed Acyclic Graph (I-DAG) technique that labels micro-batches at the stage of building a stream event and arranges stream elements with event labels. In the next step, heterogeneous stream events are processed through the I-DAG workflow, which has non-linear DAG operation for extracting queries’ results in a Spark cluster. The performance evaluation shows that I-DAG resolves homogeneous IoT-enabled stream event issues and provides an effective stream event heterogeneous solution for IoT-enabled datasets in spark clusters. © 2020, The Author(s).},
author_keywords={Apache spark;  Directed acyclic graph;  Internet of Things (IoT);  MapReduce;  Micro-batch stream},
keywords={Cluster computing;  Data encapsulation;  Data streams;  Directed graphs;  Metadata;  Random access storage, Acquisition process;  Directed acyclic graph (DAG);  Distributed computing environment;  Random access memory;  Real time acquisition;  Real time data streaming;  Record information;  Resilient distributed dataset, Internet of things},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tantalaki2020117182,
author={Tantalaki, N. and Souravlas, S. and Roumeliotis, M. and Katsavounis, S.},
title={Pipeline-Based Linear Scheduling of Big Data Streams in the Cloud},
journal={IEEE Access},
year={2020},
volume={8},
pages={117182-117202},
doi={10.1109/ACCESS.2020.3004612},
art_number={9123756},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087830448&doi=10.1109%2fACCESS.2020.3004612&partnerID=40&md5=abab09519ac2c1fcff7e186573056681},
abstract={Nowadays, there is an accelerating need to efficiently and timely handle large amounts of data that arrives continuously. Streams of big data led to the emergence of several Distributed Stream Processing Systems (DSPS) that assign processing tasks to the available resources (dynamically or not) and route streaming data between them. Efficient scheduling of processing tasks can reduce application latencies and eliminate network congestions. However, the available DSPSs' in-built scheduling techniques are far from optimal. In this work, we extend our previous work, where we proposed a linear scheme for the task allocation and scheduling problem. Our scheme takes advantage of pipelines to handle efficiently applications, where there is need for heavy communication (all-to-all) between tasks assigned to pairs of components. In this work, we prove that our scheme is periodic, we provide a communication refinement algorithm and a mechanism to handle many-to-one assignments efficiently. For concreteness, our work is illustrated based on Apache Storm semantics. The performance evaluation depicts that our algorithm achieves load balance and constraints the required buffer space. For throughput testing, we compared our work to the default Storm scheduler, as well as to R-Storm. Our scheme was found to outperform both the other strategies and achieved an average of 25%-40% improvement compared to Storm's default scheduler under different scenarios, mainly as a result of reduced buffering (≈ 45% less memory). Compared to R-storm, the results indicate an average of 35%-45% improvement. © 2013 IEEE.},
author_keywords={big data;  distributed systems;  pipelines;  scheduling;  Stream processing},
keywords={Big data;  Distributed parameter control systems;  Pipelines;  Scheduling;  Semantics;  Storms;  Well testing, Communication refinement;  Distributed stream processing;  Efficient scheduling;  Large amounts of data;  Linear scheduling;  Network congestions;  Scheduling techniques;  Task allocation and scheduling, Data streams},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Stehle2020616,
author={Stehle, E. and Jacobsen, H.-A.},
title={ParPaRaw: Massively parallel parsing of delimiter-separated raw data},
journal={Proceedings of the VLDB Endowment},
year={2020},
volume={13},
number={5},
pages={616-628},
doi={10.14778/3377369.3377372},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086255184&doi=10.14778%2f3377369.3377372&partnerID=40&md5=3e648f5aff68f8521d0f37234cdbf4b6},
abstract={Parsing is essential for a wide range of use cases, such as stream processing, bulk loading, and in-situ querying of raw data. Yet, the compute-intense step often constitutes a major bottleneck in the data ingestion pipeline, since parsing of inputs that require more involved parsing rules is challenging to parallelise. This work proposes a massively parallel algorithm for parsing delimiter-separated data formats on GPUs. Other than the state-of-the-art, the proposed approach does not require an initial sequential pass over the input to determine a thread's parsing context. That is, how a thread, beginning somewhere in the middle of the input, should interpret a certain symbol (e.g., whether to interpret a comma as a delimiter or as part of a larger string enclosed in double-quotes). Instead of tailoring the approach to a single format, we are able to perform a massively parallel finite state machine (FSM) simulation, which is more exible and powerful, supporting more expressive parsing rules with general applicability. Achieving a parsing rate of as much as 14.2 GB/s, our experimental evaluation on a GPU with 3 584 cores shows that the presented approach is able to scale to thousands of cores and beyond. With an endto- end streaming approach, we are able to exploit the fullduplex capabilities of the PCIe bus and hide latency from data transfers. Considering the end-to-end performance, the algorithm parses 4:8 GB in as little as 0:44 seconds, including data transfers.},
keywords={Computer aided software engineering;  Data transfer;  In situ processing;  Program processors, Data ingestions;  End-to-end performance;  Experimental evaluation;  Massively parallels;  Situ querying;  State of the art;  Stream processing;  Streaming approach, Data streams},
publisher={VLDB Endowment},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hesse20191381,
author={Hesse, G. and Matthies, C. and Glass, K. and Huegle, J. and Uflacker, M.},
title={Quantitative impact evaluation of an abstraction layer for data stream processing systems},
journal={Proceedings - International Conference on Distributed Computing Systems},
year={2019},
volume={2019-July},
pages={1381-1392},
doi={10.1109/ICDCS.2019.00138},
art_number={8884832},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072948058&doi=10.1109%2fICDCS.2019.00138&partnerID=40&md5=9eed3282bd891630816a69468dd691bb},
abstract={With the demand to process ever-growing data volumes, a variety of new data stream processing frameworks have been developed. Moving an implementation from one such system to another, e.g., for performance reasons, requires adapting existing applications to new interfaces. Apache Beam addresses these high substitution costs by providing an abstraction layer that enables executing programs on any of the supported streaming frameworks. In this paper, we present a novel benchmark architecture for comparing the performance impact of using Apache Beam on three streaming frameworks: Apache Spark Streaming, Apache Flink, and Apache Apex. We find significant performance penalties when using Apache Beam for application development in the surveyed systems. Overall, usage of Apache Beam for the examined streaming applications caused a high variance of query execution times with a slowdown of up to a factor of 58 compared to queries developed without the abstraction layer. All developed benchmark artifacts are publicly available to ensure reproducible results. © 2019 IEEE.},
author_keywords={Abstraction Layer;  Data Stream Processing;  Performance Benchmarking},
keywords={Abstracting;  Benchmarking;  Data handling, Abstraction layer;  Application development;  Data stream processing;  Impact evaluation;  Performance benchmarking;  Performance impact;  Performance penalties;  Streaming applications, Distributed computer systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Renart2019885,
author={Renart, E.G. and Balouek-Thomert, D. and Parashar, M.},
title={An edge-based framework for enabling data-driven pipelines for IoT systems},
journal={Proceedings - 2019 IEEE 33rd International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2019},
year={2019},
pages={885-894},
doi={10.1109/IPDPSW.2019.00146},
art_number={8778344},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070362953&doi=10.1109%2fIPDPSW.2019.00146&partnerID=40&md5=518d4e0dce1ca4c68b94542474a4f908},
abstract={Due to the proliferation of the Internet of Things (IoT) paradigm, the number of devices connected to the Internet is growing. These devices are generating unprecedented amounts of data at the edges of the infrastructure. Although the generated data provides great potential, identifying and processing relevant data points hidden in streams of unimportant data, and doing this in near real time, remains a significant challenge. Existing stream processing platforms require the data to be transported to the cloud for processing, resulting in latencies that can prevent timely decision making or may reduce the amount of data processed. To tackle this problem, we designed an IoT Edge Framework, called R-Pulsar, that extends cloud capabilities to local devices and provides a programming model for deciding what, when, and where data get collected and processed. In this paper, we discuss motivating use cases and the architectural design of R-Pulsar. We have deployed and tested R-Pulsar on embedded devices (Raspberry Pi and Android phone) and present an experimental evaluation that demonstrates that R-Pulsar can enable timely data analytics by effectively leveraging edge and cloud resources. © 2019 IEEE.},
author_keywords={Big Data;  Edge analytics;  Edge Computing;  Stream Processing},
keywords={Big data;  Data Analytics;  Data handling;  Decision making;  Edge computing;  Pulsars, Data driven;  Edge analytics;  Embedded device;  Experimental evaluation;  Internet of thing (IOT);  Near-real time;  Programming models;  Stream processing, Internet of things},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Prosperi201942,
author={Prosperi, L. and Costan, A. and Silva, P. and Antoniu, G.},
title={Planner: Cost-Efficient Execution Plans Placement for Uniform Stream Analytics on Edge and Cloud},
journal={Proceedings of WORKS 2018: 13th Workshop on Workflows in Support of Large-Scale Science, Held in conjunction with SC 2018: The International Conference for High Performance Computing, Networking, Storage and Analysis},
year={2019},
pages={42-51},
doi={10.1109/WORKS.2018.00010},
art_number={8638380},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063041529&doi=10.1109%2fWORKS.2018.00010&partnerID=40&md5=b86a6a3372ad41871c35d031ac96ad5b},
abstract={Stream processing applications handle unbounded and continuous flows of data items which are generated from multiple geographically distributed sources. Two approaches are commonly used for processing: Cloud-based analytics and Edge analytics. The first one routes the whole data set to the Cloud, incurring significant costs and late results from the high latency networks that are traversed. The latter can give timely results but forces users to manually define which part of the computation should be executed on Edge and to interconnect it with the remaining part executed in the Cloud, leading to sub-optimal placements. In this paper, we introduce Planner, a middleware for uniform and transparent stream processing across Edge and Cloud. Planner automatically selects which parts of the execution graph will be executed at the Edge in order to minimize the network cost. Real-world micro-benchmarks show that Planner reduces the network usage by 40% and the makespan (end-to-end processing time) by 15% compared to state-of-the-art. © 2018 IEEE.},
author_keywords={Edge-analytics;  Hybrid-stream-processing;  Stream-processing},
keywords={Middleware, Continuous flows;  Distributed sources;  Edge-analytics;  High-latency networks;  Micro-benchmark;  Optimal placements;  State of the art;  Stream processing, Digital storage},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Truong2018754,
author={Truong, T.M. and Harwood, A. and Sinnott, R.O. and Chen, S.},
title={Performance Analysis of Large-Scale Distributed Stream Processing Systems on the Cloud},
journal={IEEE International Conference on Cloud Computing, CLOUD},
year={2018},
volume={2018-July},
pages={754-761},
doi={10.1109/CLOUD.2018.00103},
art_number={8457872},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057437413&doi=10.1109%2fCLOUD.2018.00103&partnerID=40&md5=0af8fed415a07e3d8b9d16b88a2fbb01},
abstract={Real-time data processing is often a necessity as it can provide insights that have less value if discovered off-line or after the fact. However, large-scale stream processing systems are non-trivial to build and deploy. While there are many frameworks that allow users to create large-scale distributed systems, there remains many challenges in understanding the performance, cost of deployment and considerations and impact of potential (partial) outages on real-time systems performance. Our work considers the performance of Cloud-based stream processing systems in terms of back-pressure and expected utilization. The performance of an exemplar stream application is explored using different Cloud-based virtual machine resources and where the scale of deployment and cost benefits are taken into consideration in relation to the overall performance. To achieve this, we develop an algorithm based on queueing theory to predict the throughput and latency of stream data processing while supporting system stability. Our methodology for making fundamental measurements is applicable to mainstream stream processing frameworks such as Apache Storm and Heron. The method is especially suitable for large-scale distributed stream processing where jobs can run for extended time periods. We benchmark the performance of the system on the national research cloud of Australia (Nectar), and present a performance analysis based on estimating the overall effective utilization. © 2018 IEEE.},
author_keywords={Cloud;  Performance analysis;  Stream processing},
keywords={Benchmarking;  Cloud computing;  Clouds;  Computation theory;  Data handling;  Distributed parameter control systems;  Interactive computer systems;  Queueing theory;  System stability, Distributed stream processing;  Large-scale distributed system;  Performance analysis;  Real-time data processing;  Stream application;  Stream data processing;  Stream processing;  Stream processing systems, Real time systems},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ivanov2018141,
author={Ivanov, T. and Taaffe, J.},
title={Exploratory analysis of spark structured streaming},
journal={ICPE 2018 - Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
year={2018},
volume={2018-January},
pages={141-146},
doi={10.1145/3185768.3186360},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052016134&doi=10.1145%2f3185768.3186360&partnerID=40&md5=a4feb3926ce7e405a98aa3146390d03d},
abstract={In the Big Data era, stream processing has become a common requirement for many data-intensive applications. This has lead to many advances in the development and adaption of large scale streaming systems. Spark and Flink have become a popular choice for many developers as they combine both batch and streaming capabilities in a single system. However, introducing the Spark Structured Streaming in version 2.0 opened up completely new features for SparkSQL, which are alternatively only available in Apache Calcite. This work focuses on the new Spark Structured Streaming and analyses it by diving into its internal functionalities. With the help of a micro-benchmark consisting of streaming queries, we perform initial experiments evaluating the technology. Our results show that Spark Structured Streaming is able to run multiple queries successfully in parallel on data with changing velocity and volume sizes. © 2018 Association for Computing Machinery.},
author_keywords={Big Data Benchmarking;  Spark;  Spark Structured Streaming},
keywords={Calcite;  Electric sparks, Data-intensive application;  Exploratory analysis;  Large-scale streaming;  Micro-benchmark;  Multiple queries;  Stream processing, Big data},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dayarathna20171443,
author={Dayarathna, M. and Li, Y. and Wen, Y. and Fan, R.},
title={Energy consumption analysis of data stream processing: a benchmarking approach},
journal={Software - Practice and Experience},
year={2017},
volume={47},
number={10},
pages={1443-1462},
doi={10.1002/spe.2458},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003758583&doi=10.1002%2fspe.2458&partnerID=40&md5=32705c54707aa26d488a0d1482c011c6},
abstract={Energy efficiency of data analysis systems has become a very important issue in recent times because of the increasing costs of data center operations. Although distributed streaming workloads have increasingly been present in modern data centers, energy-efficient scheduling of such applications remains as a significant challenge. In this paper, we conduct an energy consumption analysis of data stream processing systems in order to identify their energy consumption patterns. We follow stream system benchmarking approach to solve this issue. Specifically, we implement Linear Road benchmark on six stream processing environments (S4, Storm, ActiveMQ, Esper, Kafka, and Spark Streaming) and characterize these systems' performance on a real-world data center. We study the energy consumption characteristics of each system with varying number of roads as well as with different types of component layouts. We also use a microbenchmark to capture raw energy consumption characteristics. We observed that S4, Esper, and Spark Streaming environments had highest average energy consumption efficiencies compared with the other systems. Using a neural networkbased technique with the power/performance information gathered from our experiments, we developed a model for the power consumption behavior of a streaming environment. We observed that energy-efficient execution of streaming application cannot be specifically attributed to the system CPU usage. We observed that communication between compute nodes with moderate tuple sizes and scheduling plans with balanced system overhead produces better power consumption behaviors in the context of data stream processing systems. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd.},
author_keywords={benchmarking;  data stream processing;  distributed systems;  energy consumption analysis;  linear road;  workload characterization},
keywords={Benchmarking;  Data communication systems;  Data handling;  Electric power utilization;  Energy utilization;  Information analysis;  Roads and streets;  Scheduling;  Transportation, Data stream processing;  Distributed systems;  Energy consumption analysis;  Linear road;  Workload characterization, Energy efficiency},
publisher={John Wiley and Sons Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{HoseinyFarahabady2017137,
author={HoseinyFarahabady, M. and Zomaya, A.Y. and Tari, Z.},
title={QoS-and Contention-Aware Resource Provisioning in a Stream Processing Engine},
journal={Proceedings - IEEE International Conference on Cluster Computing, ICCC},
year={2017},
volume={2017-September},
pages={137-146},
doi={10.1109/CLUSTER.2017.21},
art_number={8048925},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040483402&doi=10.1109%2fCLUSTER.2017.21&partnerID=40&md5=ce0091898c47ea75861bd32e6cfca413},
abstract={This paper addresses the shared resource contention problem associated with the auto-parallelization of running queries in distributed stream processing engines. In such platforms, analyzing a large amount of data often requires to execute user-defined queries over continues raw-inputs in a parallel fashion at each single host. However, previous studies showed that the collocated applications can fiercely compete for shared resources, resulting in a severe performance degradation among applications. This paper presents an advanced resource allocation strategy for handling scenarios in which the target applications have different quality of service (QoS) requirements while shared-resource interference is considered as a key performance-limiting parameter.To properly allocate the best possible resource to each query, the proposed controller predicts the performance degradation of the running pane-level as well as the window-level queries when co-running with other queries. This is addressed as an optimization problem where a set of cost functions is defined to achieve the following goals: A) reduce the sum of QoS violation incidents over all machines; b) keep the CPU utilization level within an accepted range; and c) avoid fierce shared resource interference among collocated applications. Particle swarm optimization is used to find an acceptable solution at each round of the controlling period. The performance of the proposed solution is benchmarked with Round-Robin and best-effort strategies, and the experimental results clearly demonstrate that the proposed controller has the following advantages over its opponents: it increases the overall resource utilization by 15% on average while can reduce the average tuple latencies by 14%. It also achieves an average 123% improvement in preventing QoS violation incidents. © 2017 IEEE.},
author_keywords={Parallelization Factor;  Shared Resource Interference;  Stream Processing;  Workload Consolidation},
keywords={Cluster computing;  Computer architecture;  Cost functions;  Distributed parameter control systems;  Engines;  Optimization;  Particle swarm optimization (PSO);  Routers, Distributed stream processing;  Parallelizations;  Qualityof-service requirement (QoS);  Resource allocation strategies;  Shared resource contentions;  Shared resources;  Stream processing;  Workload consolidation, Quality of service},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2017107,
author={Wang, G.-L. and Han, Y.-B. and Zhang, Z.-M. and Zhu, M.-L.},
title={Cloud-based integration and service of streaming data},
journal={Jisuanji Xuebao/Chinese Journal of Computers},
year={2017},
volume={40},
number={1},
pages={107-125},
doi={10.11897/SP.J.1016.2017.00107},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016516767&doi=10.11897%2fSP.J.1016.2017.00107&partnerID=40&md5=07235c3d9632855f16db7e12a59d241b},
abstract={Big data management and processing are a good place where a Cloud infrastructure shows power. Service offers an important way to implement the delivery and usage model for various Cloud-based resources and abilities. With the prevalence of large-scale sensor data, the scale of system has expanded dramatically, the complexity of the multivariate and heterogeneous data has escalated, and the concurrency and frequency of streaming data has increased a lot. These bring great challenges to the traditional streaming data system in addressing issues in areas such as processing efficiency, scalability and fault tolerance. The Cloud computing techniques can be the foundation to manage and process large scale streaming data because of their high scalability, parallel processing capabilities, support of service delivery model and fault tolerance. The issue of streaming data integration and services based on Cloud computing is the focus of this paper. The paper starts with the application requirements of large-scale streaming data processing and integration, presents a framework for streaming integration, discusses the state-of-the-art key technologies including streaming data query, service customizations, mechanisms for scalability and reliability, evaluation metrics and benchmark etc. The uses of Cloud computing for large-scale streaming data integration and service provision are scrutinized, and challenges and future trends are summarized. © 2017, Science Press. All right reserved.},
author_keywords={Cloud data integration;  Cloud service;  Data service;  Streaming data},
keywords={Benchmarking;  Big data;  Cloud computing;  Data handling;  Fault tolerance;  Information management;  Network function virtualization;  Scalability, Application requirements;  Cloud data;  Cloud infrastructures;  Cloud services;  Data services;  Large-scale streaming;  Service customization;  Streaming data, Data integration},
publisher={Science Press},
language={Chinese},
document_type={Article},
source={Scopus},
}

@ARTICLE{Saleh20162140,
author={Saleh, B. and Qiu, D.},
title={Performance Analysis of Network-Coding-Based P2P Live Streaming Systems},
journal={IEEE/ACM Transactions on Networking},
year={2016},
volume={24},
number={4},
pages={2140-2153},
doi={10.1109/TNET.2015.2448597},
art_number={7163643},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937688092&doi=10.1109%2fTNET.2015.2448597&partnerID=40&md5=ef0ede0e7e6f38d760578bc1a7e0c02c},
abstract={Peer-to-peer (P2P) video streaming is a scalable and cost-effective technology to stream video content to a large population of users and has attracted a lot of research for over a decade now. Recently, network coding has been introduced to improve the efficiency of these systems and to simplify the protocol design. There are already some successful commercial applications that utilize network coding. However, previous analytical studies of network-coding-based P2P streaming systems mainly focused on fundamental properties of the system and ignored the influence of the protocol details. In this study, a unique stochastic model is developed to reveal how segments of the video stream evolve over their lifetime in the buffer before they go into playback. Different strategies for segment selection have been studied with the model, and their performance has been compared. A new approximation of the probability of linear independence of coded blocks has been proposed to study the redundancy of network coding. Finally, extensive numerical results and simulations have been provided to validate our model. From these results, in-depth insights into how system parameters and segment selection strategies affect the performance of the system have been obtained. © 2015 IEEE.},
author_keywords={Network coding;  peer-to-peer;  performance analysis;  video streaming},
keywords={Codes (symbols);  Cost effectiveness;  Peer to peer networks;  Stochastic models;  Stochastic systems;  Video streaming, Analytical studies;  Commercial applications;  Cost-effective technology;  Fundamental properties;  Linear independence;  P2p streaming systems;  Performance analysis;  Segment selection, Network coding},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zahmatkesh2016299,
author={Zahmatkesh, S. and Della Valle, E. and Dell’Aglio, D.},
title={When a FILTER makes the difference in continuously answering SPARQL queries on streaming and quasi-static linked data},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9671},
pages={299-316},
doi={10.1007/978-3-319-38791-8_17},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977526045&doi=10.1007%2f978-3-319-38791-8_17&partnerID=40&md5=ff144337a524aa8d07fd1e0efb533ba9},
abstract={We are witnessing a growing interest for Web applications that (i) require to continuously combine highly dynamic data stream with background data and (ii) have reactivity as key performance indicator. The Semantic Web community showed that RDF Stream Processing (RSP) is an adequate framework to develop this type of applications. However, when the background data is distributed over theWeb, even RSP engines risk losing reactiveness due to the time necessary to access the background data. State-of-the-art RSP engines remain reactive using a local replica of the background data, but such a replica progressively become stale if not updated to reflect the changes in the remote background data. For this reason, recently, the RSP community investigated maintenance policies (collectively named Acqua) that guarantee reactiveness while maximizing the freshness of the replica. Acqua’s policies apply to queries that join a basic graph pattern in a window clause with another basic graph pattern in a service clause. In this paper, we extend the class of queries considered in Acqua adding a FILTER clause that selects mapping in the background data. We propose a new maintenance policy (namely, the Filter Update Policy) and we show how to combine it with Acqua policies. A set of experimental evaluations empirically proves the ability of the proposed policies to guarantee reactiveness while keeping the replica fresher than with the Acqua policies. © Springer International Publishing Switzerland 2016.},
keywords={Bandpass filters;  Benchmarking;  Engines;  Maintenance;  World Wide Web, Experimental evaluation;  Key performance indicators;  Linked datum;  Maintenance policy;  Sparql queries;  State of the art;  Stream processing;  WEB application, Semantic Web},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gu2011347,
author={Gu, Y. and Wu, Q. and Liu, X. and Yu, D.},
title={Improving throughput and reliability of distributed scientific workflows for streaming data processing},
journal={Proc.- 2011 IEEE International Conference on HPCC 2011 - 2011 IEEE International Workshop on FTDCS 2011 -Workshops of the 2011 Int. Conf. on UIC 2011- Workshops of the 2011 Int. Conf. ATC 2011},
year={2011},
pages={347-354},
doi={10.1109/HPCC.2011.52},
art_number={6063011},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81555209726&doi=10.1109%2fHPCC.2011.52&partnerID=40&md5=550a02f147437c7788cb441821dd8a88},
abstract={With the advent of next-generation scientific applications, the workflow-based computing technology has become an indispensable research method for managing and streamlining large-scale distributed data processing. This paper investigates a problem of mapping distributed workflows for streaming data processing in faulty networks where nodes and links are subject to probabilistic failures. We formulate this problem as a bi-objective optimization problem in terms of both throughput and reliability, and propose a decentralized layer-oriented method to achieve high throughput for smooth data flow while satisfying a prespecified overall failure rate bound for a guaranteed level of reliability. The superiority of the proposed mapping solution is illustrated by both extensive simulation-based performance comparisons with existing algorithms and experimental results from a real-life scientific workflow deployed in wide-area networks. © 2011 IEEE.},
author_keywords={distributed computing;  fault tolerance;  frame rate;  Reliability;  workflow mapping},
keywords={Bi-objective optimization;  Computing technology;  Data flow;  Distributed data processing;  Frame rate;  High throughput;  Nodes and links;  Overall failure;  Performance comparison;  Probabilistic failure;  research methods;  Scientific applications;  Scientific workflows;  Simulation-based;  Streaming data processing;  Work-flows, Computer simulation;  Distributed computer systems;  Fault tolerance;  Mapping;  Reliability;  Throughput;  Wide area networks, Data handling},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Karki2010111,
author={Karki, R. and Seenivasan, T. and Claypool, M. and Kinicki, R.},
title={Performance analysis of home streaming video using Orb},
journal={Proceedings of the International Workshop on Network and Operating System Support for Digital Audio and Video},
year={2010},
pages={111-116},
doi={10.1145/1806565.1806593},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954713294&doi=10.1145%2f1806565.1806593&partnerID=40&md5=3bcd97e1af32e3b4f9e0530e4e93a8e1},
abstract={A new paradigm in video streaming is emerging, that of personal video servers in the home streaming video to remote clients on the Internet. The potential impact of such technologies demands careful study to assess performance and impact. This project studies one such personal video streaming system called Orb in a closed network environment, allowing us to control network bandwidths. Our performance evaluation focuses on Orb's method of bandwidth estimation, video performance and bitrates, and resource usage during transcoding. Analysis shows Orb uses simplistic, but effective, methods of determining available bandwidth, dynamic temporal and spatial scaling, and significant CPU cycles when transcoding. The results should be useful for subsequent comparison with other home streaming technologies and capacity planning for network providers. © 2010 ACM.},
author_keywords={orb;  streaming;  video},
keywords={Available bandwidth;  Bandwidth estimation;  Bitrates;  Capacity planning;  Control network;  CPU cycles;  Network environments;  Network provider;  Performance analysis;  Performance evaluation;  Potential impacts;  Remote clients;  Resource usage;  Spatial scaling;  Streaming technology;  Streaming videos;  Transcoding;  Video servers, Acoustic streaming;  Audio systems;  Bandwidth;  Computer operating systems;  Multimedia systems;  Video streaming;  Videotex, Computer graphics},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bouillet20081003,
author={Bouillet, E. and Dube, P. and George, D. and Liu, Z. and Pendarakis, D. and Zhang, Li.},
title={Distributed multi-layered workload synthesis for testing stream processing systems},
journal={Proceedings - Winter Simulation Conference},
year={2008},
pages={1003-1011},
doi={10.1109/WSC.2008.4736167},
art_number={4736167},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-60749122763&doi=10.1109%2fWSC.2008.4736167&partnerID=40&md5=ac0167af9ecedb4e8d4dcc5e9326ecfa},
abstract={Testing and benchmarking of stream processing systems requires workload representative of real world scenarios with myriad of users, interacting through different applications over different modalities with different underlying protocols. The workload should have realistic volumetric and contextual statistics at different levels: user level, application level, packet level etc. Further realistic workload is inherently distributed in nature. We present a scalable framework for synthesis of distributed workload based on identifying different layers of workload corresponding to different time-scales. The architecture is extensible and modular, promotes reuse of libraries at different layers and offers the flexibility to add additional plug-ins at different layers without sacrificing the efficiency. © 2008 IEEE.},
keywords={Application levels;  Multi-layered;  Packet levels;  Plug-ins;  Real-world scenarios;  Stream processing systems;  Time-scales;  User levels, Benchmarking},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jacob20212613,
author={Jacob, V. and Song, F. and Stiegler, A. and Rad, B. and Diao, Y. and Tatbul, N.},
title={Exathlon: A benchmark for explainable anomaly detection over time series},
journal={Proceedings of the VLDB Endowment},
year={2021},
volume={14},
number={11},
pages={2613-2626},
doi={10.14778/3476249.3476307},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119680773&doi=10.14778%2f3476249.3476307&partnerID=40&md5=6696024a0ea972821a0560fbc1377066},
abstract={Access to high-quality data repositories and benchmarks have been instrumental in advancing the state of the art in many experimental research domains. While advanced analytics tasks over time series data have been gaining lots of attention, lack of such community resources severely limits scientific progress. In this paper, we present Exathlon, the first comprehensive public benchmark for explainable anomaly detection over high-dimensional time series data. Exathlon has been systematically constructed based on real data traces from repeated executions of large-scale stream processing jobs on an Apache Spark cluster. Some of these executions were intentionally disturbed by introducing instances of six different types of anomalous events (e.g., misbehaving inputs, resource contention, process failures). For each of the anomaly instances, ground truth labels for the root cause interval as well as those for the extended effect interval are provided, supporting the development and evaluation of a wide range of anomaly detection (AD) and explanation discovery (ED) tasks. We demonstrate the practical utility of Exathlon’s dataset, evaluation methodology, and end-to-end data science pipeline design through an experimental study with three state-of-the-art AD and ED techniques. © 2021, VLDB Endowment. All rights reserved.},
keywords={Benchmarking;  Petroleum reservoir evaluation;  Time series;  Time series analysis, Anomaly detection;  Community resources;  Data repositories;  Experimental research;  High quality data;  Research domains;  Scientific progress;  State of the art;  Time-series data;  Times series, Anomaly detection},
publisher={VLDB Endowment},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang20201628,
author={Wang, L. and Zhang, Y. and Chen, X. and Jin, R.},
title={Online Computation Performance Analysis for Distributed Machine Learning Pipelines in Fog Manufacturing},
journal={IEEE International Conference on Automation Science and Engineering},
year={2020},
volume={2020-August},
pages={1628-1633},
doi={10.1109/CASE48305.2020.9216979},
art_number={9216979},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094125283&doi=10.1109%2fCASE48305.2020.9216979&partnerID=40&md5=a6c479b92be22b3ad4676ede5cce3034},
abstract={Smart manufacturing enables real-time data streaming from interconnected manufacturing processes to improve manufacturing quality, throughput, flexibility, and cost reduction via computation services. In these computation services, machine learning pipelines integrate various types of computation method options to match the contextualized, on-demand computation needs for the maximum prediction accuracy or the best model structure interpretation. On the other hand, there is a pressing need to integrate Fog computing in manufacturing, which will reduce communication time latency and dependency on connections, improve responsiveness and reliability of the computation services, and maintain data privacy. However, there is a knowledge gap in using machine learning pipelines in Fog manufacturing. Existing offloading strategies are not effective, due to the lack of accurate prediction model for the performance of computation services before the execution of those heterogeneous computation tasks. In this paper, machine learning pipelines are implemented in Fog manufacturing. The computation performance of each sub-step of pipelines is predicted and analyzed via linear regression models and random forest regression models. A Fog manufacturing testbed is adopted to validate the performance of the employed models. The results show that the models can adequately predict the performance of computation services, which can be further integrated into Fog manufacturing to better support offloading strategies for machine learning pipelines. © 2020 IEEE.},
author_keywords={Computation Services;  Fog Computing;  Fog Manufacturing;  Machine Learning Pipeline},
keywords={Cost reduction;  Decision trees;  E-learning;  Engineering education;  Fog;  Forecasting;  Machine learning;  Manufacture;  Pipelines;  Predictive analytics;  Privacy by design;  Regression analysis;  Throughput, Computation performance;  Distributed machine learning;  Heterogeneous computation;  Linear regression models;  Manufacturing process;  Manufacturing quality;  On-demand computations;  Real time data streaming, Fog computing},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{RezaHoseinyfarahabady2020629,
author={Reza Hoseinyfarahabady, M. and Jannesari, A. and Taheri, J. and Bao, W. and Zomaya, A.Y. and Tari, Z.},
title={Q-Flink: A QoS-Aware Controller for Apache Flink},
journal={Proceedings - 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGRID 2020},
year={2020},
pages={629-638},
doi={10.1109/CCGrid49817.2020.00-30},
art_number={9139645},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089065207&doi=10.1109%2fCCGrid49817.2020.00-30&partnerID=40&md5=d16267f697a347ba96d3f62a310e829c},
abstract={Modern stream-data processing platforms are required to execute processing pipelines over high-volume, yet high-velocity, datasets under tight latency constraints. Apache Flink has emerged as an important new technology of large-scale platform that can distribute processing over a large number of computing nodes in a cluster (i.e., scale-out processing). Flink allows application developers to design and execute queries over continuous raw-inputs to analyze a large amount of streaming data in a parallel and distributed fashion. To increase the throughput of computing resources in stream processing platforms, a service provider might be tempted to use a consolidation strategy to pack as many processing applications as possible on the working nodes, with the hope of increasing the total revenue by improving the overall resource utilization. However, there is a hidden trap for achieving such a higher throughput solely by relying on an interference-oblivious consolidation strategy. In practice, collocated applications in a shared platform can fiercely compete with each others for obtaining the capacity of shared resources (e.g., cache and memory bandwidth) which in turn can lead to a severe performance degradation for all consolidated workloads.This paper addresses the shared resource contention problem associated with the auto-resource controlling mechanism of Apache Flink engine running across a distributed cluster. A controlling strategy is proposed to handle scenarios in which stream processing applications may have different quality of service (QoS) requirements while the resource interference is considered as the key performance-limiting parameter. The performance evaluation is carried out by comparing the proposed controller with the default Flink resource allocation strategy in a testbed cluster with total 32 Intel Xeon cores under different workload traffic with up to 4000 streaming applications chosen from various benchmarking tools. Experimental results demonstrate that the proposed controller can successfully decrease the average latency of high priority applications by 223% during the burst traffic while maintaining the requested QoS enforcement levels. © 2020 IEEE.},
author_keywords={Apache Flink;  Computer System Modeling and Profiling;  Massive Data Stream Processing;  Meta-Scheduling;  Model Predictive Controller;  Resource Allocation},
keywords={Benchmarking;  Cluster computing;  Controllers;  Data streams;  Economics;  Quality control;  Quality of service, Application developers;  Performance degradation;  Processing applications;  Qualityof-service requirement (QoS);  Resource allocation strategies;  Shared resource contentions;  Stream data processing;  Streaming applications, Pipeline processing systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ordonez-Ante2020,
author={Ordonez-Ante, L. and Van Seghbroeck, G. and Wauters, T. and Volckaert, B. and De Turck, F.},
title={Explora: Interactive querying of multidimensional data in the context of smart cities},
journal={Sensors (Switzerland)},
year={2020},
volume={20},
number={9},
doi={10.3390/s20092737},
art_number={2737},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084625563&doi=10.3390%2fs20092737&partnerID=40&md5=225c28e6dd544dc90ab4dddb3228d9e3},
abstract={Citizen engagement is one of the key factors for smart city initiatives to remain sustainable over time. This in turn entails providing citizens and other relevant stakeholders with the latest data and tools that enable them to derive insights that add value to their day-to-day life. The massive volume of data being constantly produced in these smart city environments makes satisfying this requirement particularly challenging. This paper introduces EXPLORA, a generic framework for serving interactive low-latency requests, typical of visual exploratory applications on spatiotemporal data, which leverages the stream processing for deriving—on ingestion time—synopsis data structures that concisely capture the spatial and temporal trends and dynamics of the sensed variables and serve as compacted data sets to provide fast (approximate) answers to visual queries on smart city data. The experimental evaluation conducted on proof-of-concept implementations of EXPLORA, based on traditional database and distributed data processing setups, accounts for a decrease of up to 2 orders of magnitude in query latency compared to queries running on the base raw data at the expense of less than 10% query accuracy and 30% data footprint. The implementation of the framework on real smart city data along with the obtained experimental results prove the feasibility of the proposed approach. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Continuous views;  Interactive querying;  Microservices;  Sensor data;  Smart city data;  Spatiotemporal data;  Synopsis data structures},
keywords={Query languages;  Query processing;  Smart city, Citizen engagements;  Distributed data processing;  Experimental evaluation;  Interactive querying;  Multidimensional data;  Orders of magnitude;  Spatial and temporal trends;  Spatio-temporal data, Data streams},
publisher={MDPI AG},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Henning20193512,
author={Henning, S. and Hasselbring, W.},
title={Scalable and Reliable Multi-Dimensional Aggregation of Sensor Data Streams},
journal={Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
year={2019},
pages={3512-3517},
doi={10.1109/BigData47090.2019.9006452},
art_number={9006452},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081400615&doi=10.1109%2fBigData47090.2019.9006452&partnerID=40&md5=80a37cb98330d8a200e7987d368a4daf},
abstract={Ever-increasing amounts of data and requirements to process them in real time lead to more and more analytics platforms and software systems being designed according to the concept of stream processing. A common area of application is the processing of continuous data streams from sensors, for example, IoT devices or performance monitoring tools. In addition to analyzing pure sensor data, analyses of data for groups of sensors often need to be performed as well. Therefore, data streams of the individual sensors have to be continuously aggregated to a data stream for a group. Motivated by a real-world application scenario, we propose that such a stream aggregation approach has to allow for aggregating sensors in hierarchical groups, support multiple such hierarchies in parallel, provide reconfiguration at runtime, and preserve the scalability and reliability qualities induced by applying stream processing techniques. We propose a stream processing architecture fulfilling these requirements, which can be integrated into existing big data architectures. We present a pilot implementation of such an extended architecture and show how it is used in industry. Furthermore, in experimental evaluations we show that our solution scales linearly with the amount of sensors and provides adequate reliability in the case of faults. © 2019 IEEE.},
keywords={Big data;  Computer architecture;  Real time systems, Application scenario;  Data architectures;  Experimental evaluation;  Hierarchical groups;  Multi dimensional;  Performance monitoring;  Pilot implementation;  Stream processing, Data streams},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bartnik2019127,
author={Bartnik, A. and Monte, B.D. and Rabl, T. and Markl, V.},
title={On-the-fly reconfiguration of query plans for stateful stream processing engines},
journal={Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
year={2019},
volume={P-289},
pages={127-146},
doi={10.18420/btw2019-09},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066506074&doi=10.18420%2fbtw2019-09&partnerID=40&md5=a54920d0af9318e29d1807802ec103da},
abstract={Stream Processing Engines (SPEs) must tolerate the dynamic nature of unbounded data streams and provide means to quickly adapt to fluctuations in the data rate. Many major SPEs however provide very little functionality to adjust the execution of a potentially infinite streaming query at runtime. Each modification requires a complete query restart, which involves an expensive redistribution of the state of a query and may require external systems in order to guarantee correct processing semantics. This results in significant downtime, which increase the operational cost of those SPEs. We present a modification protocol that enables modifying specific operators as well as the data flow of a running query while ensuring exactly-once processing semantics. We provide an implementation for Apache Flink, which enables stateful operator migration across machines, the introduction of new operators into a running query, and changes to a specific operator based on external triggers. Our results on two benchmarks show that migrating operators for queries with small state is as fast as using the savepoint mechanism of Flink. Migrating operators in the presence of large state even outperforms the savepoint mechanism by a factor of more than 2.3. Introducing and replacing operators at runtime is performed in less than 10 s. Our modification protocol demonstrates the general feasibility of runtime modifications and opens the door for many other modification use cases, such as online algorithm tweaking and up- or downscaling operator instances. © 2019 Gesellschaft fur Informatik (GI). All rights reserved.},
author_keywords={Data Stream Processing;  Fault Tolerance;  Query Plan Maintenance;  Resource Elasticity},
keywords={Engines;  Fault tolerance;  Query processing;  Semantics, Data stream processing;  Down-scaling;  Dynamic nature;  External systems;  On the flies;  On-line algorithms;  Specific operators;  Stream processing engines, Data handling},
publisher={Gesellschaft fur Informatik (GI)},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mortazavi-Dehkordi201977,
author={Mortazavi-Dehkordi, M. and Zamanifar, K.},
title={Efficient resource scheduling for the analysis of Big Data streams},
journal={Intelligent Data Analysis},
year={2019},
volume={23},
number={1},
pages={77-102},
doi={10.3233/IDA-173691},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062227223&doi=10.3233%2fIDA-173691&partnerID=40&md5=9a384b8fc0106f54403441de7bf682bb},
abstract={The emergence of Big Data has had a profound impact on how data are analyzed. Open source distributed stream processing platforms have gained popularity for analyzing streaming Big Data as they provide low latency required for streaming Big Data applications using cluster resources. However, existing resource schedulers are still lacking the efficiency that Big Data analytical applications require. Recent works have already considered streaming Big Data characteristics to improve the efficiency of scheduling in the platforms. Nevertheless, they have not taken into account the specific attributes of analytical applications. This study, therefore, presents Bframework, an efficient resource scheduling framework used by streaming Big Data analysis applications based on cluster resources. Bframework proposes a query model using Directed Graphs (DGs) and introduces operator assignment and operator scheduling algorithms based on a novel partitioning algorithm. Bframework is highly adaptable to the fluctuation of streaming Big Data and the availability of cluster resources. Experiments with the benchmark and well-known real-world queries show that Bframework can significantly reduce the latency of streaming Big Data analysis queries up to about 65%. © 2019 - IOS Press and the authors. All rights reserved.},
author_keywords={analytical query;  distributed stream processing;  resource scheduling;  Streaming Big Data},
keywords={Data handling;  Directed graphs;  Distributed parameter control systems;  Efficiency;  Information analysis;  Scheduling;  Scheduling algorithms, Analytical applications;  Analytical queries;  Big data applications;  Data characteristics;  Distributed stream processing;  Partitioning algorithms;  Resource scheduling framework;  Resource-scheduling, Big data},
publisher={IOS Press},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nardelli201953,
author={Nardelli, M. and Russo Russo, G. and Cardellini, V. and Lo Presti, F.},
title={A multi-level elasticity framework for distributed data stream processing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11339 LNCS},
pages={53-64},
doi={10.1007/978-3-030-10549-5_5},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061702257&doi=10.1007%2f978-3-030-10549-5_5&partnerID=40&md5=9ca5cf92fe34fe10591914a99ea2a57d},
abstract={Data Stream Processing (DSP) applications should be capable to efficiently process high-velocity continuous data streams by elastically scaling the parallelism degree of their operators so to deal with high variability in the workload. Moreover, to efficiently use computing resources, modern DSP frameworks should seamlessly support infrastructure elasticity, which allows to exploit resources available on-demand in geo-distributed Cloud and Fog systems. In this paper we propose E2DF, a framework to autonomously control the multi-level elasticity of DSP applications and the underlying computing infrastructure. E2DF revolves around a hierarchical approach, with two control layers that work at different granularity and time scale. At the lower level, fully decentralized Operator and Region managers control the reconfiguration of distributed DSP operators and resources. At the higher level, centralized managers oversee the overall application and infrastructure adaptation. We have integrated the proposed solution into Apache Storm, relying on a previous extension we developed, and conducted an experimental evaluation. It shows that, even with simple control policies, E2DF can improve resource utilization without application performance degradation. © Springer Nature Switzerland AG 2019.},
author_keywords={Data Stream Processing;  Elasticity;  Hierarchical control},
keywords={Elasticity;  Managers, Application performance;  Computing infrastructures;  Data stream processing;  Different granularities;  Distributed data stream processing;  Experimental evaluation;  Hierarchical control;  Resource utilizations, Data streams},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2018169,
author={Chen, Z. and Wei, S. and Yu, W. and Nguyen, J.H. and Hatcher, W.G.},
title={A cloud/edge computing streaming system for network traffic monitoring and threat detection},
journal={International Journal of Security and Networks},
year={2018},
volume={13},
number={3},
pages={169-186},
doi={10.1504/IJSN.2018.093556},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050800014&doi=10.1504%2fIJSN.2018.093556&partnerID=40&md5=3b528968866347750dde4ae880fe0f60},
abstract={Abstract: The unyielding trend of increasing cyber threats has made cyber security paramount in protecting personal and private intellectual property. To provide a highly secured network environment, network threat detection systems must handle real-time big data from varied places in enterprise networks. In this paper, we introduce a streaming-based threat detection system that can rapidly analyse highly intensive network traffic data in real-time, utilising streaming-based clustering algorithms to detect abnormal network activities. The developed system integrates the high-performance data analysis capabilities of Flume, Spark and Hadoop into a cloud-computing environment to provide network monitoring and intrusion detection. Our performance evaluation validates that the developed system can cope with a significant volume of streaming data in a high detection accuracy and good system performance. We further extend our system for edge computing and discuss some key challenges, as well as some potential solutions, aiming to improve the scalability of our system. Copyright © 2018 Inderscience Enterprises Ltd.},
author_keywords={Big network data analysis;  Cloud computing;  Edge computing;  Network traffic monitoring;  Streaming analysis;  Threat detection},
keywords={Cloud computing;  Clustering algorithms;  Edge computing;  Information analysis;  Intrusion detection;  Real time systems, Analysis capabilities;  Cloud computing environments;  Enterprise networks;  Network data;  Network environments;  Network traffic monitoring;  Threat detection;  Threat detection system, Computer networks},
publisher={Inderscience Publishers},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Schaffner2018265,
author={Schaffner, M. and Scheidegger, F. and Cavigelli, L. and Kaeslin, H. and Benini, L. and Smolic, A.},
title={Towards Edge-Aware Spatio-Temporal Filtering in Real-Time},
journal={IEEE Transactions on Image Processing},
year={2018},
volume={27},
number={1},
pages={265-280},
doi={10.1109/TIP.2017.2757259},
art_number={8055438},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030767930&doi=10.1109%2fTIP.2017.2757259&partnerID=40&md5=bc8159136a511ef32ffc24f4abc25513},
abstract={Spatio-temporal edge-aware (STEA) filtering methods have recently received increased attention due to their ability to efficiently solve or approximate important image-domain problems in a temporally consistent manner - which is a crucial property for video-processing applications. However, existing STEA methods are currently unsuited for real-time, embedded stream-processing settings due to their high processing latency, large memory, and bandwidth requirements, and the need for accurate optical flow to enable filtering along motion paths. To this end, we propose an efficient STEA filtering pipeline based on the recently proposed permeability filter (PF), which offers high quality and halo reduction capabilities. Using mathematical properties of the PF, we reformulate its temporal extension as a causal, non-linear infinite impulse response filter, which can be efficiently evaluated due to its incremental nature. We bootstrap our own accurate flow using the PF and its temporal extension by interpolating a quasi-dense nearest neighbour field obtained with an improved PatchMatch algorithm, which employs binarized octal orientation maps (BOOM) descriptors to find correspondences among subsequent frames. Our method is able to create temporally consistent results for a variety of applications such as optical flow estimation, sparse data upsampling, visual saliency computation and disparity estimation. We benchmark our optical flow estimation on the MPI Sintel dataset, where we currently achieve a Pareto optimal quality-efficiency tradeoff with an average endpoint error of 7.68 at 0.59 s single-core execution time on a recent desktop machine. © 2017 IEEE.},
author_keywords={binary descriptor;  Edge-aware filter;  optical flow;  patch-match;  spatio-temporal filter},
keywords={Benchmarking;  Bins;  Boolean functions;  Communication channels (information theory);  Data structures;  Edge detection;  Estimation;  Impulse response;  Optical flows;  Pareto principle;  Standards;  Video signal processing, Descriptors;  Edge aware;  Image edge detection;  Optical imaging;  patchmatch;  Spatio-temporal filter, Adaptive optics},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wang20171,
author={Wang, Y. and Tari, Z. and Reza Hoseinyfarahabady, M. and Zomaya, A.Y.},
title={QoS-aware resource allocation for stream processing engines using priority channels},
journal={2017 IEEE 16th International Symposium on Network Computing and Applications, NCA 2017},
year={2017},
volume={2017-January},
pages={1-9},
doi={10.1109/NCA.2017.8171365},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046437984&doi=10.1109%2fNCA.2017.8171365&partnerID=40&md5=0c6471624b2de65756de6e0543ef3a27},
abstract={This paper addresses the challenging problem of guaranteeing quality-of-service (QoS) requirements associated with parallel running queries in distributed stream processing engines. In such platforms, the real-time processing of streaming data often requires executing a set of user-defined queries over continues data flows. However, previous studies showed that guaranteeing QoS enforcement (such as end-to-end response time) for a collection of applications is a complex problem. This paper presents an advanced resource allocation strategy to tackle such a problem by considering the traffic pattern of individual data streams. To properly allocate resource for streaming queries execution, we define a certain number of priority channels to categorize the streaming data across the system. The resource allocation is addressed as an optimization problem where a set of cost functions is defined to achieve the following goals: a) reduce the sum of QoS violation incidents across all applications; b) increase the CPU utilization level, and (c) avoid the additional costs caused by frequent reconfigurations. The proposed solution does not depend on any assumption about the incoming data rate or the query processing time. The performance of the proposed solution is benchmarked, and the experimental results reveal that the proposed scheme increases the overall resource utilization by 23% on average and reduces the QoS violations by 29% against round-robin strategy. It could also prevent QoS violation incidents at different levels by tuning the cost function. © 2017 IEEE.},
author_keywords={Apache Storm;  Dynamic Resource Allocation;  End-to-End Response Time;  Quality of Service Enforcements;  Stream Processing Engine},
keywords={Cost functions;  Costs;  Data handling;  Distributed parameter control systems;  Engines;  Query languages;  Resource allocation;  Routers, Apache storms;  Distributed stream processing;  Dynamic resource allocations;  End-to-end response time;  Qos-aware resource allocations;  Qualityof-service requirement (QoS);  Resource allocation strategies;  Stream processing engines, Quality of service},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Suárez-Cetrulo201767,
author={Suárez-Cetrulo, A.L. and Cervantes, A.},
title={An online classification algorithm for large scale data streams: iGNGSVM},
journal={Neurocomputing},
year={2017},
volume={262},
pages={67-76},
doi={10.1016/j.neucom.2016.12.093},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020486793&doi=10.1016%2fj.neucom.2016.12.093&partnerID=40&md5=464f2be0f9dbe24d178e6743b455398c},
abstract={Stream Processing has recently become one of the current commercial trends to face huge amounts of data. However, normally these techniques need specific infrastructures and high resources in terms of memory and computing nodes. This paper shows how mini-batch techniques and topology extraction methods can help making gigabytes of data to be manageable for just one server using computationally costly Machine Learning techniques as Support Vector Machines. The algorithm iGNGSVM is proposed to improve the performance of Support Vector Machines in datasets where the data is continuously arriving. It is benchmarked against a mini-batch version of LibSVM, achieving good accuracy rates and performing faster than this. © 2017 Elsevier B.V.},
author_keywords={Data classification;  Growing Neural Gas;  Large datasets;  Online learning;  Support Vector Machines;  Topology extraction},
keywords={Data mining;  Data streams;  Extraction;  Large dataset;  Learning systems;  Support vector machines;  Topology, Data classification;  Extraction method;  Growing neural gas;  Large datasets;  Machine learning techniques;  On-line classification;  Online learning;  Stream processing, Classification (of information), accuracy;  algorithm;  Article;  calculation;  classification algorithm;  cost benefit analysis;  data analysis;  extraction;  linear system;  mathematical computing;  nonlinear system;  online system;  priority journal;  support vector machine},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{LeVan2017238,
author={Le Van, C. and Gao, F. and Ali, M.I.},
title={Optimizing the performance of concurrent RDF stream processing queries},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10249 LNCS},
pages={238-253},
doi={10.1007/978-3-319-58068-5_15},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019979326&doi=10.1007%2f978-3-319-58068-5_15&partnerID=40&md5=52771fe0d7cf6adbb82038dfbb761a33},
abstract={With the growing popularity of Internet of Things (IoT) and sensing technologies, a large number of data streams are being generated at a very rapid pace. To explore the potentials of the integration of IoT and semantic technologies, a few RDF Stream Processing (RSP) query engines are made available which are capable of processing, analyzing and reasoning over semantic data streams in real-time. This way, RSP mitigates data interoperability issues and promotes knowledge discovery and smart decision making for time-sensitive applications. However, a major hurdle in the wide adoption of RSP systems is their query performance. Particularly, the ability of RSP engines to handle a large number of concurrent queries is very limited which refrains large scale stream processing applications (e.g. smart city applications) to adopt RSP. In this paper, we propose a shared-join based approach to improve the performance of an RSP engine for concurrent queries. We also leverage query federation mechanisms to allow distributed query processing over multiple RSP engine instances in order to gain performance for concurrent and distributed queries. We apply load balancing strategies to distribute queries and further optimize the concurrent query performance. We provide a proof of concept implementation by extending CQELS RSP engine and evaluate our approach using existing benchmark datasets for RSP. We also compare the performance of our proposed approach with the state of the art implementation of CQELS RSP engine. © Springer International Publishing AG 2017.},
author_keywords={Linked data;  Query optimization;  RDF stream processing},
keywords={Data communication systems;  Decision making;  Engines;  Internet of things;  Search engines;  Smart city, Data interoperability;  Distributed query processing;  Internet of Things (IOT);  Linked datum;  Load balancing strategy;  Query optimization;  Stream processing;  Time sensitive applications, Semantic Web},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nalepa201693,
author={Nalepa, F. and Batko, M. and Zezula, P.},
title={Performance analysis of distributed stream processing applications through Colored Petri Nets},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9548},
pages={93-106},
doi={10.1007/978-3-319-29817-7_9},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975677691&doi=10.1007%2f978-3-319-29817-7_9&partnerID=40&md5=ad6d5de190cf7f368b33aa3b43c83ca7},
abstract={Nowadays, a lot of data are produced every second and they need to be processed immediately. Processing such unbounded streams of data is often run in a distributed environment in order to achieve high throughput. The challenge is the ability to predict the performance- related characteristics of such applications. Knowledge of these properties is essential for decisions about the amount of needed computational resources, how the computations should be spread in the distributed environment, etc. In this paper, we present performance analysis of distributed stream processing applications using Colored Petri Nets (CPNs). We extend our previously proposed model with processing strategies which are used to specify performance effects when multiple tasks are placed on the same resource. We also show a detailed conversion of the whole proposed model to the CPNs. The conversion is validated through simulations of the CPNs which are compared to real streaming applications. © Springer International Publishing Switzerland 2016.},
author_keywords={Colored Petri Nets;  Data stream model;  Performance analysis;  Stream processing},
keywords={Distributed parameter control systems, Colored Petri Nets;  Computational resources;  Data stream model;  Distributed environments;  Distributed stream processing;  Performance analysis;  Stream processing;  Streaming applications, Petri nets},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nalepa2015520,
author={Nalepa, F. and Batko, M. and Zezula, P.},
title={Model for performance analysis of distributed stream processing applications},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9262},
pages={520-533},
doi={10.1007/978-3-319-22852-5_42},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943608712&doi=10.1007%2f978-3-319-22852-5_42&partnerID=40&md5=8805505cf8594f6a035fd3076b979560},
abstract={Nowadays, a lot of data is produced every second and it needs to be processed immediately. Processing such unbounded streams of data is often applied in a distributed environment in order to achieve high throughput. There is a challenge to predict the performance-related characteristics of such applications. Knowledge of these properties is essential for decisions about the amount of needed computational resources, how the computations should be spread in the distributed environment, etc. In this paper, we propose a model to represent such streaming applications with the respect to their performance related properties.We present a conversion of the model to Colored Petri Nets (CPNs) which is used for performance analysis of the original application. The behavior of the proposed model and its conversion to the CPNs is validated through experiments. Our prediction was able to achieve nearly 100% precise maximum delays of real stream processing applications. © Springer International Publishing Switzerland 2015.},
author_keywords={Data stream Model;  Performance analysis;  Stream processing},
keywords={Distributed parameter control systems;  Expert systems;  Petri nets, Colored Petri Nets;  Computational resources;  Data stream model;  Distributed environments;  Distributed stream processing;  Performance analysis;  Stream processing;  Streaming applications, Knowledge based systems},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Simoncelli2013253,
author={Simoncelli, D. and Dusi, M. and Gringoli, F. and Niccolini, S.},
title={Scaling out the performance of service monitoring applications with BlockMon},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={7799 LNCS},
pages={253-255},
doi={10.1007/978-3-642-36516-4_26},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875611245&doi=10.1007%2f978-3-642-36516-4_26&partnerID=40&md5=41edc05ea2e6932eaccbcdad50913ef2},
abstract={To cope with real-time data analysis as the amount of data being exchanged over the network increases, an idea is to re-design algorithms originally implemented on the monitoring probe to work in a distributed manner over a stream-processing platform. In this paper we show preliminary performance analysis of a Twitter trending algorithm when running over BlockMon, an open-source monitoring platform which we extended to run distributed data-analytics algorithms: we show that it performs up to 23.5x and 34.2x faster on BlockMon than on Storm and Apache S4 respectively, two emerging stream-processing platforms. © 2013 Springer-Verlag Berlin Heidelberg.},
keywords={Data Analytics, Distributed data analytics;  Monitoring platform;  Monitoring probes;  Open sources;  Performance analysis;  Real time data analysis;  Service monitoring;  Stream processing, Data streams},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Keong2011735,
author={Keong, C.Y. and Hoong, P.K. and Ting, C.-Y.},
title={Efficient hybrid push-pull based P2P media streaming system},
journal={Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
year={2011},
pages={735-740},
doi={10.1109/ICPADS.2011.55},
art_number={6121348},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856604767&doi=10.1109%2fICPADS.2011.55&partnerID=40&md5=6ff9a38cae2041a4dc49cb8d3f75f911},
abstract={Peer-to-Peer (P2P) communication is a popular protocol that has significant impacted and also changed the way for files being distributed over the large networks. Variants of P2P protocols are also applied for other media distribution such as audio and video streaming. The P2P protocol is widely adapted by researchers as a method to handle larger group of users. Coolstreaming, the first large scale P2P streaming experiment till today, applied a mesh-based streaming system which has slowly evolved from a pure pull system to a hybrid push-pull system. In this paper, we present our proposed push-pull scheduling for P2P streaming that can heuristically select the most optimal frame to be pushed based on the rules that we designed. Our proposed solution incorporates the fast content distribution characteristic of both Push and Pull approaches. For performance evaluation, we compare our scheduling algorithm with the pure pull Coolstreaming scheduling and Random push-pull scheduling where both scheduling serve as a benchmark in three main criteria - end-to-end delay, frame miss-ratio and frame redundancy. Simulation results showed that our proposed heuristic push-pull overall outperformed the other scheduling schemes, where our proposed scheduling algorithm demonstrates as a better solution towards reducing mesh delay in P2P streaming. © 2011 IEEE.},
author_keywords={Coolstreaming;  Mesh-based pull;  P2P streaming;  Push-pull scheduling},
keywords={Audio and video streaming;  Content distribution;  Coolstreaming;  End to end delay;  Hybrid push-pull;  Large networks;  Media distribution;  Mesh-based pull;  P2P media streaming;  P2P protocols;  P2P streaming;  Peer-to-peer communications;  Performance evaluation;  Pull systems;  Scheduling schemes, Distributed computer systems;  Scheduling;  Video streaming, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zotos2011,
author={Zotos, N. and Xilouris, G. and Kourtis, A. and Renzi, D. and Shao, B.},
title={Performance evaluation of H264/SVC streaming system featuring real-time in-network adaptation},
journal={IEEE International Workshop on Quality of Service, IWQoS},
year={2011},
doi={10.1109/IWQOS.2011.5931331},
art_number={5931331},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960688118&doi=10.1109%2fIWQOS.2011.5931331&partnerID=40&md5=412beba4e0eb4a0fcda7e5f2e3281b8d},
abstract={In the recent years one of the most active research topics in multimedia networking is the exploitation of the Scalable Video Coding (SVC) as a scalable solution for efficient network resources utilization. SVC introduces scalability by exploiting a layered encoding of the video stream, thus enabling real-time in-network adaptation by selectively allowing the transmission of appropriate layers. This paper presents an architecture that exploits SVC capabilities in order to provide end-to-end QoS assurance via in-network video adaptation. The adaptation system management is based on MPEG-21 framework while the network QoS mechanisms are based on DiffServ standard. The performance evaluation of the proposed architecture is performed over a real test-bed infrastructure. © 2011 IEEE.},
author_keywords={DiffServ;  end-to-end;  H264;  in-network adaptation;  QoS;  streaming;  SVC},
keywords={DiffServ;  end-to-end;  H264;  In-network adaptation;  SVC, Motion Picture Experts Group standards;  Multimedia systems;  Quality control;  Quality of service;  Video signal processing, Network architecture},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ferreira2010306,
author={Ferreira, H. and Duarte, S. and Preguiça, N.},
title={4Sensing- Decentralized processing for participatory sensing data},
journal={Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
year={2010},
pages={306-313},
doi={10.1109/ICPADS.2010.20},
art_number={5695617},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951800750&doi=10.1109%2fICPADS.2010.20&partnerID=40&md5=f2013f7a381c9e4cf829f690d2494cc2},
abstract={Participatory Sensing is an emerging application paradigm that leverages the growing ubiquity of sensor-capable smart phones to allow communities carry out wide-area sensing tasks, as a side-effect of people's everyday lives and movements. This paper proposes a decentralized infrastructure for supporting Participatory Sensing applications. It describes an architecture and a domain specific programming language for modeling, prototyping and developing the distributed processing of participatory sensing data with the goal of allowing faster and easier development of these applications. Moreover, a case-study application is also presented as the basis for an experimental evaluation. © 2010 IEEE.},
author_keywords={Data streaming;  Decentralized processing;  Mobile computing;  Participatory sensing},
keywords={Data streaming;  Decentralized processing;  Distributed processing;  Domain specific programming languages;  Emerging applications;  Experimental evaluation;  Participatory sensing;  Prototyping;  Sensing applications;  Sensing data;  Side effect;  Smart phones;  Wide area, Computer systems;  Data reduction;  Software prototyping, Mobile computing},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang2008199,
author={Zhang, X. and Furtlehner, C. and Sebag, M.},
title={Distributed and incremental clustering based on weighted affinity propagation},
journal={Frontiers in Artificial Intelligence and Applications},
year={2008},
volume={179},
number={1},
pages={199-210},
doi={10.3233/978-1-58603-893-9-199},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875939206&doi=10.3233%2f978-1-58603-893-9-199&partnerID=40&md5=92c51ded364c433bb50f14a893c9762b},
abstract={A new clustering algorithm Affinity Propagation (AP) is hindered by its quadratic complexity. The Weighted Affinity Propagation (WAP) proposed in this paper is used to eliminate this limitation, support two scalable algorithms. Distributed AP clustering handles large datasets by merging the exemplars learned from subsets. Incremental AP extends AP to online clustering of data streams. The paper validates all proposed algorithms on benchmark and on real-world datasets. Experimental results show that the proposed approaches offer a good trade-off between computational effort and performance. © 2008 The authors and IOS Press. All rights reserved.},
author_keywords={Affinity Propagation;  Data Clustering;  Data Streaming;  K-centers},
keywords={Algorithms;  Economic and social effects, Affinity propagation;  Computational effort;  Data clustering;  Data streaming;  Incremental clustering;  K-center;  New clustering algorithms;  Quadratic complexity, Clustering algorithms},
publisher={IOS Press},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shen2008106,
author={Shen, Z. and Kawashima, H. and Kitagawa, H.},
title={Lineage-based probabilistic event stream processing},
journal={2008 9th International Conference on Mobile Data Management Workshops, MDMW 2008},
year={2008},
pages={106-113},
doi={10.1109/MDMW.2008.12},
art_number={4839090},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650665059&doi=10.1109%2fMDMW.2008.12&partnerID=40&md5=a93f0c36358046ff550ac1a81b81d528},
abstract={Many sensor network applications such as monitoring video camera streams or management of RFID data streams or tiny sensor data streams from Motes or SunSPOTs require the ability to detect composite events over high-volume data streams. Sensor data inputs from physical world are usually noisy, incomplete and unreliable because sensing devices are usually unreliable. Thus they are usually expressed with probability in ubiquitous sensor network environment. To manage this kind of data, the probabilistic event stream processing system is a natural consequence. In this paper, we propose a query language to support probabilistic queries for composite event stream matching. The language allows users to express Kleene closure patterns for complex event detection in physical world. We also propose a working framework for query processing over probabilistic event streams. Our method first detects sequence patterns over probabilistic data streams by using a new data structure, AIG which handles a record sets of active states with a NFA-based approach. After detecting active states, our method then computes the probability of each detected sequence pattern on its lineage. That is, query processing and confidence computation are decoupled. By the benefit of lineage, the probability of an output event can be directly calculated without considering the query plan. We conduct a performance evaluation of our method comparing with naive one which is called possible worlds approach. The result clearly shows the effectiveness of our approach. While our approach shows scalable throughput, naive approach degrades its performance rapidly. The experiments are conducted with the window size, the number of event types and the number of alternatives.},
keywords={Computer hardware description languages;  Probability;  Query languages;  Query processing;  Sensor networks;  Sensor nodes;  Video cameras, Complex event detection;  Event stream processing;  Natural consequences;  Probabilistic data streams;  Rfid data streams;  Sensor network applications;  Sequence patterns;  Ubiquitous sensor networks, Information management},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zeitler2006,
author={Zeitler, E. and Risch, T.},
title={Processing High-Volume stream queries on a supercomputer},
journal={ICDEW 2006 - Proceedings of the 22nd International Conference on Data Engineering Workshops},
year={2006},
doi={10.1109/ICDEW.2006.118},
art_number={1623943},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940284699&doi=10.1109%2fICDEW.2006.118&partnerID=40&md5=07684930bc52a48d5f47b21eb19c8b07},
abstract={Scientific instruments, such as radio telescopes, colliders, sensor networks, and simulators generate very high volumes of data streams that scientists analyze to detect and understand physical phenomena. The high data volume and the need for advanced computations on the streams require substantial hardware resources and scalable stream processing. We address these challenges by developing data stream management technology to support high-volume stream queries utilizing massively parallel computer hardware. We have developed a data stream management system prototype for state-of-The-Art parallel hardware. The performance evaluation uses real measurement data from LOFAR, a radio telescope antenna array being developed in the Netherlands. © 2006 IEEE.},
keywords={Antenna arrays;  Computer hardware;  Data communication systems;  Database systems;  Hardware;  Information management;  Radio telescopes;  Reconfigurable hardware;  Sensor networks;  Supercomputers;  Technical presentations, Data stream management;  Data stream management systems;  Hardware resources;  Massively parallel computers;  Parallel hardware;  Physical phenomena;  Real measurements;  Scientific instrument, Computer hardware description languages},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tang200691,
author={Tang, A. and Liu, Z. and Xia, C. and Zhang, L.},
title={Distributed resource allocation for stream data processing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={4208 LNCS},
pages={91-100},
doi={10.1007/11847366_10},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750337744&doi=10.1007%2f11847366_10&partnerID=40&md5=e3153a71940fbd218dd37b9114bb774f},
abstract={Data streaming applications are becoming more and more common due to the rapid development in the areas such as sensor networks, multimedia streaming, and on-line data mining, etc. These applications are often running in a decentralized, distributed environment. The requirements for processing large volumes of streaming data at real time have posed many great design challenges. It is critical to optimize the ongoing resource consumption of multiple, distributed, cooperating, processing units. In this paper, we consider a generic model for the general stream data processing systems. We address the resource allocation problem for a collection of processing units so as to maximize the weighted sum of the throughput of different streams. Each processing unit may require multiple input data streams simultaneously and produce one or many valuable output streams. Data streams flow through such a system after processing at multiple processing units. Based on this framework, we develop distributed algorithms for finding the best resource allocation schemes in such data stream processing networks. Performance analysis on the optimality and complexity of these algorithms are also provided. © Springer-Verlag Berlin Heidelberg 2006.},
author_keywords={Distributed algorithm;  Resource allocation;  Stream processing},
keywords={Data flow analysis;  Distributed computer systems;  Multimedia systems;  Problem solving;  Real time systems;  Resource allocation, Distributed algorithms;  Resource consumption;  Stream processing, Data processing},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nandi2022340,
author={Nandi, A. and Xhafa, F.},
title={A federated learning method for real-time emotion state classification from multi-modal streaming},
journal={Methods},
year={2022},
volume={204},
pages={340-347},
doi={10.1016/j.ymeth.2022.03.005},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126905773&doi=10.1016%2fj.ymeth.2022.03.005&partnerID=40&md5=8beb373245068150bee183a9f710f0d3},
abstract={Emotional and physical health are strongly connected and should be taken care of simultaneously to ensure completely healthy persons. A person's emotional health can be determined by detecting emotional states from various physiological measurements (EDA, RB, EEG, etc.). Affective Computing has become the field of interest, which uses software and hardware to detect emotional states. In the IoT era, wearable sensor-based real-time multi-modal emotion state classification has become one of the hottest topics. In such setting, a data stream is generated from wearable-sensor devices, data accessibility is restricted to those devices only and usually a high data generation rate should be processed to achieve real-time emotion state responses. Additionally, protecting the users’ data privacy makes the processing of such data even more challenging. Traditional classifiers have limitations to achieve high accuracy of emotional state detection under demanding requirements of decentralized data and protecting users’ privacy of sensitive information as such classifiers need to see all data. Here comes the federated learning, whose main idea is to create a global classifier without accessing the users’ local data. Therefore, we have developed a federated learning framework for real-time emotion state classification using multi-modal physiological data streams from wearable sensors, called Fed-ReMECS. The main findings of our Fed-ReMECS framework are the development of an efficient and scalable real-time emotion classification system from distributed multimodal physiological data streams, where the global classifier is built without accessing (privacy protection) the users’ data in an IoT environment. The experimental study is conducted using the popularly used multi-modal benchmark DEAP dataset for emotion classification. The results show the effectiveness of our developed approach in terms of accuracy, efficiency, scalability and users’ data privacy protection. © 2022 Elsevier Inc.},
author_keywords={Data-driven systems;  Federated learning;  Feed-forward neural network;  Multi-modal data streaming;  Real-time emotion classification;  Wearable sensors},
keywords={electroencephalography;  emotion;  human;  physiology;  procedures, Electroencephalography;  Emotions;  Humans},
publisher={Academic Press Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Eskandari2021219,
author={Eskandari, L. and Mair, J. and Huang, Z. and Eyers, D.},
title={I-Scheduler: Iterative scheduling for distributed stream processing systems},
journal={Future Generation Computer Systems},
year={2021},
volume={117},
pages={219-233},
doi={10.1016/j.future.2020.11.011},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097636401&doi=10.1016%2fj.future.2020.11.011&partnerID=40&md5=82d882be3266f56d8a1c67b61a96fffa},
abstract={Task allocation in Data Stream Processing Systems (DSPSs) has a significant impact on performance metrics such as data processing latency and system throughput. An application processed by DSPSs can be represented as a Directed Acyclic Graph (DAG), where each vertex represents a task and the edges show the dataflow between the tasks. Task allocation can be defined as the assignment of the vertices in the DAG to the physical compute nodes such that the data movement between the nodes is minimised. Finding an optimal task placement for DSPSs is NP-hard. Thus, approximate scheduling approaches are required to improve the performance of DSPSs. In this paper, we propose a heuristic scheduling algorithm which reliably and efficiently finds highly communicating tasks by exploiting graph partitioning algorithms and a mathematical optimisation software package. We evaluate the communication cost of our method using three micro-benchmarks, showing that we can achieve results that are close to optimal. We further compare our scheduler with two popular existing schedulers, R-Storm and Aniello et al.’s ‘Online scheduler’ using two real-world applications. Our experimental results show that our proposed scheduler outperforms R-Storm, increasing throughput by up to 30%, and improves on the Online scheduler by 20%–86% as a result of finding a more efficient schedule.1 © 2020},
author_keywords={Big data;  Graph partitioning;  Heterogeneous cluster;  Scheduling;  Stream processing},
keywords={Data flow analysis;  Data streams;  Directed graphs;  Distributed parameter control systems;  Graph algorithms;  Heuristic algorithms;  Iterative methods;  NP-hard;  Optimization;  Storms, Communicating tasks;  Data stream processing;  Directed acyclic graph (DAG);  Distributed stream processing;  Graph partitioning algorithms;  Heuristic scheduling algorithms;  Mathematical optimisation;  Performance metrics, Scheduling},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{VanDongen202193745,
author={Van Dongen, G. and Poel, D.V.D.},
title={A Performance Analysis of Fault Recovery in Stream Processing Frameworks},
journal={IEEE Access},
year={2021},
volume={9},
pages={93745-93763},
doi={10.1109/ACCESS.2021.3093208},
art_number={9466838},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112096514&doi=10.1109%2fACCESS.2021.3093208&partnerID=40&md5=3f2b66ac5d151152b91df7c29f014f7b},
abstract={Distributed stream processing frameworks have gained widespread adoption in the last decade because they abstract away the complexity of parallel processing. One of their key features is built-in fault tolerance. In this work, we dive deeper into the implementation, performance, and efficiency of this critical feature for four state-of-the-art frameworks. We include the established Spark Streaming and Flink frameworks and the more novel Spark Structured Streaming and Kafka Streams frameworks. We test the behavior under different types of faults and settings: master failure with and without high-availability setups, driver failures for Spark frameworks, worker failure with or without exactly-once semantics, application and task failures. We highlight differences in behavior during these failures on several aspects, e.g., whether there is an outage, downtime, recovery time, data loss, duplicate processing, accuracy, and the cost and behavior of different message delivery guarantees. Our results highlight the impact of framework design on the speed of fault recovery and explain how different use cases may benefit from different approaches. Due to their task-based scheduling approach, the Spark frameworks can recover within 30 seconds and in most cases without necessitating an application restart. Kafka Streams has only a few seconds of downtime, but is slower at catching up on delays. Finally, Flink can offer end-to-end exactly-once semantics at a low cost but requires job restarts for most failures leading to high recovery times of around 50 seconds. © 2013 IEEE.},
author_keywords={apache flink;  apache kafka;  Apache spark;  benchmarking;  big data;  distributed computing;  fault tolerance;  kafka streams;  stream processing frameworks;  structured streaming},
keywords={Distributed parameter control systems;  Fault tolerance;  Maintenance;  Semantics, Critical features;  Distributed stream processing;  Framework designs;  High availability;  Parallel processing;  Performance analysis;  State of the art;  Stream processing, Recovery},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Carpio2020,
author={Carpio, F. and Delgado, M. and Jukan, A.},
title={Engineering and Experimentally Benchmarking a Container-based Edge Computing System},
journal={IEEE International Conference on Communications},
year={2020},
volume={2020-June},
doi={10.1109/ICC40277.2020.9148636},
art_number={9148636},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089412345&doi=10.1109%2fICC40277.2020.9148636&partnerID=40&md5=66242524a254d1c70b206c6987368c0a},
abstract={While edge computing is envisioned to superbly serve latency sensitive applications, the implementation-based studies benchmarking its performance are few and far between. To address this gap, we engineer a modular edge cloud computing system architecture that is built on latest advances in containerization techniques, including Kafka, for data streaming, Docker, as application platform, and Firebase Cloud, as realtime database system. We benchmark the performance of the system in terms of scalability, resource utilization and latency by comparing three scenarios: cloud-only, edge-only and combined edge-cloud. The measurements show that edge-only solution outperforms other scenarios only when deployed with data located at one edge only, i.e., without edge computing wide data synchronization. In case of applications requiring data synchronization through the cloud, edge-cloud scales around a factor 10 times better than cloudonly, until certain number of concurrent users in the system, and above this point, cloud-only scales better. In terms of resource utilization, we observe that whereas the mean utilization increases linearly with the number of user requests, the maximum values for the memory and the network I/O heavily increase when with an increasing amount of data. © 2020 IEEE.},
author_keywords={cloud;  edge computing;  IoT;  networking},
keywords={Benchmarking;  Containers;  Edge computing, Application platforms;  Computing system;  Concurrent users;  Data streaming;  Data synchronization;  Real-time database systems;  Resource utilizations;  Sensitive application, Computer architecture},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Onishi202066,
author={Onishi, T. and Michaelis, J. and Kanemasa, Y.},
title={Recovery-conscious adaptive watermark generation for time-order event stream processing},
journal={Proceedings - 5th ACM/IEEE Conference on Internet of Things Design and Implementation, IoTDI 2020},
year={2020},
pages={66-78},
doi={10.1109/IoTDI49375.2020.00014},
art_number={9097610},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085945001&doi=10.1109%2fIoTDI49375.2020.00014&partnerID=40&md5=4c43ca8d7a4683e95f1a7d3308e814a5},
abstract={Achieving low latency in time-order real-time event stream processing of data produced by a large number of IoT devices is difficult due to the buffering time required for sorting event messages arrived. Despite the existence of real-time IoT applications that highly depend on the order of event occurrences, event messages arriving from the devices to the event processing system are not time-ordered in many cases. Therefore, such an event processing system needs to buffer arriving event messages, sort, and then process them. This buffering time tends to be decided as a safe large period to guarantee correctness in terms of time-ordering, and leads to an increase of latency. Furthermore, failure recovery mechanism of a stateful stream processing deployed into distributed processing nodes makes this problem more difficult. It frequently causes large time-order reversal among event messages in the distributed stream processing system, that cannot be handled with a buffering time designed for real-time processing during normal periods. In this paper, we introduce a novel technique to adaptively determine the minimum necessary amount of buffering time to guarantee time-order sorting depending on the event-time progress in arrival event messages. To be more precise, instead of specifying the actual buffering time, the method adaptively controls the minimum time margin to determine the current 'watermark' that shows the event time upon which a processing unit in a distributed stream processing system believes to have received all the messages. Further, we developed a complete function (called 'time alignment') to sort the time-order of messages while keeping low latency in our developed distributed event stream processing platform. Through the experimental evaluations, it is shown that the developed method has the capability to determine a proper watermark to achieve both a strong guarantee on time-order correctness and low latency in real-time event stream processing. © 2020 IEEE.},
author_keywords={CPS;  Distributed system;  Event stream processing;  Event time;  IoT;  Low latency;  Time order;  Watermark},
keywords={Data streams;  Distributed parameter control systems;  Sorting;  Watermarking, Adaptive watermarks;  Complete functions;  Distributed events;  Distributed processing;  Distributed stream processing;  Experimental evaluation;  Failure recovery mechanism;  Realtime processing, Internet of things},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Katsipoulakis20201105,
author={Katsipoulakis, N.R. and Labrinidis, A. and Chrysanthis, P.K.},
title={SPEAr: Expediting stream processing with accuracy guarantees},
journal={Proceedings - International Conference on Data Engineering},
year={2020},
volume={2020-April},
pages={1105-1116},
doi={10.1109/ICDE48307.2020.00100},
art_number={9101830},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085858145&doi=10.1109%2fICDE48307.2020.00100&partnerID=40&md5=f941f08ba98733d9f323e337289a41e8},
abstract={Stream Processing Engines (SPEs) are used for realtime and continuous processing with stateful operations. This type of processing poses numerous challenges due to its associated complexity, unpredictable input, and need for timely results. As a result, users tend to overprovision resources, and online scaling is required in order to overcome overloaded situations. Current attempts for expediting stateful processing are impractical, due to their inability to uphold the quality of results, maintain performance, and reduce memory requirements.In this paper, we present the SPEAr system, which can expedite processing of stateful operations automatically by trading accuracy for performance. SPEAr detects when it can accelerate processing by employing online sampling and accuracy estimation at no additional cost. We built SPEAr on top of Storm and our experiments indicate that it can reduce processing times by more than an order of magnitude, use more than an order of magnitude less memory, and offer accuracy guarantees in real-world benchmarks. © 2020 IEEE.},
keywords={Accuracy estimation;  Additional costs;  Continuous processing;  On-line sampling;  Processing time;  Quality of results;  Stream processing;  Stream processing engines, Data processing},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Prasad2020,
author={Prasad, K.S.N. and Rao, A.S. and Ramana, A.V.},
title={Ensemble framework for concept-drift detection in multidimensional streaming data},
journal={International Journal of Computers and Applications},
year={2020},
doi={10.1080/1206212X.2020.1711617},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079727145&doi=10.1080%2f1206212X.2020.1711617&partnerID=40&md5=6f3aa434b26336013adc823eb6f6941e},
abstract={The potential objective of data mining (DM) over the data streaming is the detection of concept-drift. Concept-Drift signifies a diversity among the data tuples streamed in the sequence. The concept-drift often appears as incremental or abrupt. The incremental drift denotes the gradual increment of the drift between the tuples of streaming data. The other format of the drift is abrupt, which signifies the drift between tuples of data streaming in sequence. The proposed method is an Ensemble Framework for Concept-Drift Detection in Multidimensional Streaming Data (EFCDD). In addition, the proposed method EFCDD deals with the recurrent drift of the concept in streaming data. To state the drift, the projection diversity of the values representing the field positions or field-IDs, which are in use for framing the structure of the records streaming form the intended sources. The experimental study was carried out by mocking the streams of those transmitting records of the benchmark datasets often used in DM. The outcomes of the experimental study evince the scalability and prominence of EFCDD toward the detection of drift in concept. The proposal performance is measured by comparing simulation outcomes with the other existing model. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={Data mining;  EX-Stream;  multidimensional;  online imbalance datasets;  streaming data},
keywords={Data mining;  Data reduction, Benchmark datasets;  Concept drifts;  Data streaming;  Data tuples;  EX-Stream;  multidimensional;  online imbalance datasets;  Streaming data, Data streams},
publisher={Taylor and Francis Ltd.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fang20192471,
author={Fang, J. and Chao, P. and Zhang, R. and Zhou, X.},
title={Integrating workload balancing and fault tolerance in distributed stream processing system},
journal={World Wide Web},
year={2019},
volume={22},
number={6},
pages={2471-2496},
doi={10.1007/s11280-018-0656-0},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059758264&doi=10.1007%2fs11280-018-0656-0&partnerID=40&md5=31020c13b929714b84dc8c51fccbed4e},
abstract={Distributed Stream Processing Engine (DSPE) is designed for processing continuous streams so as to achieve the real-time performance with low latency guaranteed. To satisfy such requirement, the availability and efficiency are the main concern of the DSPE system, which can be achieved by a proper design of the fault tolerance module and the workload balancing module, respectively. However, the inherent characteristics of data streams, including persistence, dynamic and unpredictability, pose great challenges in satisfying both properties. As far as we know, most of the state-of-the-art DSPE systems take either fault tolerance or workload balancing as its single optimization goal, which in turn receives a higher resource overhead or longer recovery time. In this paper, we combine the fault tolerance and workload balancing mechanisms in the DSPE to reduce the overall resource consumption while keeping the system interactive, high-throughput, scalable and highly available. Based on our data-level replication strategy, our method can handle the dynamic data skewness and node failure scenario: during the distribution fluctuation of the incoming stream, we rebalance the workload by selectively inactivate the data in high-load nodes and activate their replicas on low-load nodes to minimize the migration overhead within the stateful operator; when a fault occurs in the process, the system activates the replicas of the data affected to ensure the correctness while keeping the workload balanced. Extensive experiments on various join workloads on both benchmark data and real data show our superior performance compared with baseline systems. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Distributed systems;  High availability computing;  Load Balancing;  Real-time data processing},
keywords={Balancing;  Benchmarking;  Data handling;  Distributed parameter control systems;  Fault tolerance;  Fault tolerant computer systems;  Real time systems;  Resource allocation, Distributed stream processing;  Distributed systems;  Distribution fluctuations;  High availability;  Inherent characteristics;  Real time performance;  Real-time data processing;  Replication strategies, Distributed computer systems},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kang201961,
author={Kang, J. and Samarasinghe, G. and Senanayake, U. and Conjeti, S. and Sowmya, A.},
title={Deep learning for volumetric segmentation in spatio-temporal data: Application to segmentation of prostate in DCE-MRI},
journal={Proceedings - International Symposium on Biomedical Imaging},
year={2019},
volume={2019-April},
pages={61-65},
doi={10.1109/ISBI.2019.8759314},
art_number={8759314},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073915234&doi=10.1109%2fISBI.2019.8759314&partnerID=40&md5=ee43c505d4fff3259ae5726212250968},
abstract={Segmentation of the prostate in MR images is an essential step that underpins the success of subsequent analysis methods, such as cancer lesion detection inside the tumour and registration between different modalities. This work focuses on leveraging deep learning for analysis of longitudinal volumetric datasets, particularly for the task of segmentation, and presents proof-of-concept for segmentation of the prostate in 3D+T DCE-MRI sequences. A two-stream processing pipeline is proposed for this task, comprising a spatial stream modelled using a volumetric fully convolutional network and a temporal stream modeled using recurrent neural networks with Long-Short-term Memory (LSTM) units. The predictions of the two streams are fused using deep neural networks. The proposed method has been validated on a public benchmark dataset of 17 patients, each with 40 temporal volumes. When averaged over three experiments, a highly competitive Dice overlap score of 0.8688 and sensitivity of 0.8694 were achieved. As a spatiotemporal segmentation method, it can easily migrate to other datasets. © 2019 IEEE.},
author_keywords={Deep learning;  LSTM;  Prostate segmentation},
keywords={Deep learning;  Deep neural networks;  Image segmentation;  Magnetic resonance imaging;  Medical imaging;  Urology, Benchmark datasets;  Convolutional networks;  LSTM;  Prostate segmentation;  Spatio-temporal data;  Spatio-temporal segmentation;  Volumetric data sets;  Volumetric segmentations, Long short-term memory},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2019773,
author={Wang, W. and Zhang, C. and Chen, X. and Li, Z. and Ding, H. and Wen, X.},
title={An on-the-fly scheduling strategy for distributed stream processing platform},
journal={Proceedings - 16th IEEE International Symposium on Parallel and Distributed Processing with Applications, 17th IEEE International Conference on Ubiquitous Computing and Communications, 8th IEEE International Conference on Big Data and Cloud Computing, 11th IEEE International Conference on Social Computing and Networking and 8th IEEE International Conference on Sustainable Computing and Communications, ISPA/IUCC/BDCloud/SocialCom/SustainCom 2018},
year={2019},
pages={773-780},
doi={10.1109/BDCloud.2018.00116},
art_number={8672226},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063880241&doi=10.1109%2fBDCloud.2018.00116&partnerID=40&md5=820069379c220a8f6db32f77e8bbfcdf},
abstract={Distributed stream processing can accomplish real-time processing of continuous streaming big data to obtain valuable information with high velocity. To maintain continuously stable and efficient running of stream applications, however, continuous online scheduling operations are required in the context of highly dynamic data stream. For this reason, this paper proposes the on-the-fly scheduling strategy in a distributed stream processing environment, which dynamically predicts abnormal events through double exponential smoothing and adopts traffic-aware active migration protocol to adjust the network routing structure on-the-fly to balance the inter-worker load. Moreover, an evaluation method is proposed to quantitatively analyze the various scheduling objectives. Finally, we commendably apply the scheduling strategy to a stream processing platform, which regards docker instance as basic scheduling units. Meanwhile, based on the platform and the evaluation method, we complete performance comparison experiments of the scheduling algorithm. The experimental results indicate that our algorithm has excellent performance in throughput of topology, average processing time and balance of task load, which is suitable for deployment in a distributed environment with large-scale nodes and tasks. © 2018 IEEE.},
author_keywords={Distributed stream processing;  Evaluation method;  Exponential smoothing;  Prediction algorithm;  Traffic-aware migration},
keywords={Big data;  Cloud computing;  Distributed parameter control systems;  Multitasking;  Scheduling;  Sustainable development;  Ubiquitous computing, Distributed stream processing;  Evaluation method;  Exponential smoothing;  Prediction algorithms;  Traffic aware, Scheduling algorithms},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sahin2017,
author={Sahin, S. and Gedik, B.},
title={C-Stream: A co-routine-based elastic stream processing engine},
journal={ACM Transactions on Parallel Computing},
year={2017},
volume={4},
number={3},
doi={10.1145/3184120},
art_number={15},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054894126&doi=10.1145%2f3184120&partnerID=40&md5=6e5feaabd58d82a437c213ae9b4afc96},
abstract={Stream processing is a computational paradigm for on-the-fly processing of live data. This paradigm lends itself to implementations that can provide high throughput and low latency by taking advantage of various forms of parallelism that are naturally captured by the stream processing model of computation, such as pipeline, task, and data parallelism. In this article, we describe the design and implementation of C-Stream, which is an elastic stream processing engine. C-Stream encompasses three unique properties. First, in contrast to the widely adopted event-based interface for developing streaming operators, C-Stream provides an interface wherein each operator has its own driver loop and relies on data availability application programming interfaces (APIs) to decide when to perform its computations. This self-control-based model significantly simplifies the development of operators that require multiport synchronization. Second, C-Stream contains a dynamic scheduler that manages the multithreaded execution of the operators. The scheduler, which is customizable via plug-ins, enables the execution of the operators as co-routines, using any number of threads. The base scheduler implements back-pressure, provides data availability APIs, and manages preemption and termination handling. Last, C-Stream varies the degree of parallelism to resolve bottlenecks by both dynamically changing the number of threads used to execute an application and adjusting the number of replicas of data-parallel operators.We provide an experimental evaluation of C-Stream. The results show that C-Stream is scalable, highly customizable, and can resolve bottlenecks by dynamically adjusting the level of data parallelism used. © 2017 ACM.},
author_keywords={C-Stream;  Elastic stream processing engine},
keywords={Application programming interfaces (API);  Data handling;  Engines;  Pipeline processing systems;  Scheduling, Computational paradigm;  Degree of parallelism;  Design and implementations;  Dynamic schedulers;  Experimental evaluation;  Multithreaded executions;  Stream processing;  Stream processing engines, C (programming language)},
publisher={Association for Computing Machinery},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ma2017351,
author={Ma, K. and Yang, B.},
title={Stream-based live entity resolution approach with adaptive duplicate count strategy},
journal={International Journal of Web and Grid Services},
year={2017},
volume={13},
number={3},
pages={351-373},
doi={10.1504/IJWGS.2017.085167},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025097602&doi=10.1504%2fIJWGS.2017.085167&partnerID=40&md5=cfca61bf3bc8e75fbc7e3a3a889aa738},
abstract={Recently, researchers have been more concerned about large-scale news and tweet data generated by the social media. Some cloud service providers utilise the data to find public sentiments for the tenants. The challenge is how to clean the big data in the cloud before making further analysis. To address this issue, we propose a new live entity resolution approach at a time to find duplicates from the news and tweet data. We investigate possible solutions to address live entity resolution in the cloud, to make sliding window size adaptive using multistep distance and window size dependent duplicate count strategy with alterable window step, and find duplicates by overlapping boundary objects in adjacent blocks. Finally, our experimental evaluation based on the news data on large datasets shows the high effectiveness and efficiency of the proposed approaches. Copyright © 2017 Inderscience Enterprises Ltd.},
author_keywords={Big data;  Cloud computing;  Entity resolution;  MapReduce;  NoSQL;  Sorted neighbourhood;  Stream processing},
keywords={Big data;  Cloud computing;  Large dataset, Entity resolutions;  Map-reduce;  NoSQL;  Sorted neighbourhood;  Stream processing, Data streams},
publisher={Inderscience Publishers},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Su2015949,
author={Su, Y. and Shi, F. and Talpur, S. and Wang, Y. and Hu, S. and Wei, J.},
title={Achieving self-aware parallelism in stream programs},
journal={Cluster Computing},
year={2015},
volume={18},
number={2},
pages={949-962},
doi={10.1007/s10586-014-0412-x},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939954217&doi=10.1007%2fs10586-014-0412-x&partnerID=40&md5=4f7ed2a05e2ea96fd7fb86630227e2bf},
abstract={The age of big data open the door to a new approach in data exploration and utilization. With the increasing complexities and dynamics of modern IT systems and services, it has become a challenge to effectively exploit parallelism on multicore platforms in computing systems that are heterogeneous, dynamic and decentralised. Self-aware software is a response to these demands in dealing with distributed applications in changing environments. It is a closed-loop system with a series of optimization strategies to adjust itself dynamicly during data processing. We focus on incorporating adaptation mechanisms into the stream programs for exposing distributed parallelism. In the traditional stream programming models, changing data and status normally require human supervision to adjust the stream graph for performance. As one-time optimization strategy, the reconfiguration and maintenance lead to costly and time-consuming procedures during the operating phase. To address these problems, we propose a self-aware stream programming model called StreamAware. A key property of this model is that exposing self-aware parallelism in the message driven execution paradigm, which provides dynamic and reconfigurable stream graph in adapting to the data flow changes. The model defines the self-awareness loop based on finite state machine for stream applications to adjust their own stream graph with continuous optimization strategy. This paper presents three different self-aware systems built using StreamAware. The empirical evaluation demonstrate how these systems can exploit self-aware parallelism using the Parsec benchmark problems, optimize performance per Watt, and respond to significant changes in stream processing. © 2014, Springer Science+Business Media New York.},
author_keywords={Distributed computing;  Message-driven execution;  Multicore;  Reconfigration;  Self-aware parallelism;  Stream programming},
keywords={Application programs;  Benchmarking;  Big data;  Closed loop systems;  Data flow analysis;  Data handling;  Distributed computer systems;  Flow graphs;  Logic circuits;  Optimization, Message-driven;  Multi core;  Reconfigration;  Self-aware;  Stream programming, Multicore programming},
publisher={Springer New York LLC},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rooholamin2015237,
author={Rooholamin, S.A. and Ziavras, S.G.},
title={Modular vector processor architecture targeting at data-level parallelism},
journal={Microprocessors and Microsystems},
year={2015},
volume={39},
number={4-5},
pages={237-249},
doi={10.1016/j.micpro.2015.04.007},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929581773&doi=10.1016%2fj.micpro.2015.04.007&partnerID=40&md5=860950e272dd2b9a65a2a27efebdf8f8},
abstract={Taking advantage of DLP (Data-Level Parallelism) is indispensable in most data streaming and multimedia applications. Several architectures have been proposed to improve both the performance and energy consumption for such applications. Superscalar and VLIW (Very Long Instruction Word) processors along with SIMD (Single-Instruction Multiple-Data) and vector processor (VP) accelerators, are among the available options for designers to accomplish their desired requirements. We present an innovative architecture for a VP which separates the path for performing data shuffle and memory-indexed accesses from the data path for executing other vector instructions that access the memory. This separation speeds up the most common memory access operations by avoiding extra delays and unnecessary stalls. In our lane-based VP design, each vector lane uses its own private memory to avoid any stalls during memory access instructions. The proposed VP, which is developed in VHDL and prototyped on an FPGA, serves as a coprocessor for one or more scalar cores. Benchmarking shows that our VP can achieve very high performance. For example, it achieves a larger than 1500-fold speedup in the color space converting benchmark compared to running the code on a scalar core. The inclusion of distributed data shuffle engines across vector lanes has a spectacular impact on the execution time, primarily for applications like FFT (Fast-Fourier Transform) that require large amounts of data shuffling. Compared to running the benchmark on a VP without the shuffle engines, the speedup is 5.92 and 7.33 for the 64-point FFT without and with compiler optimization, respectively. Compared to runs on the scalar core, the achieved speedups for this benchmark are 52.07 and 110.45 without and with compiler optimization, respectively. © 2015 Elsevier B.V. All rights reserved.},
author_keywords={Benchmarking;  Parallelism;  Performance;  Speedup;  Vector processor},
keywords={Array processing;  Benchmarking;  Digital signal processing;  Energy utilization;  Engines;  Fast Fourier transforms;  Memory architecture;  Parallel processing systems;  Program compilers;  Vectors, FFT (fast Fourier transform);  Multimedia applications;  Parallelism;  Performance;  Simd (single instruction multiple data);  Speedup;  Vector processors;  VLIW (very long instruction word) processor, Very long instruction word architecture},
publisher={Elsevier},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Seyfabad2014908,
author={Seyfabad, M.S. and Akbari, B.},
title={CAC-live: Centralized assisted cloud P2P live streaming},
journal={22nd Iranian Conference on Electrical Engineering, ICEE 2014},
year={2014},
pages={908-913},
doi={10.1109/IranianCEE.2014.6999665},
art_number={6999665},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943806966&doi=10.1109%2fIranianCEE.2014.6999665&partnerID=40&md5=5ac2ad680579d1c20d825ec3091409f3},
abstract={Peer-to-Peer (P2P) live video streaming over the Internet is a developing technology that recently has gained more attention. The use of P2P network with leveraging of local resources of peers, increases scalability and reduces costs. One of the limitations of P2P live video streaming systems is the lack of adequate resources such as available upload bandwidth, both at video source and inside P2P overlay network that lead to reduce the quality of service (QoS) experienced by the users. One solution for this problem is to employ additional on-demand resources such as virtual machines (VM) that are rented from a cloud provider to increase the amount of total available bandwidth. In this paper, we propose an architecture for improving the QoS of the peers by using virtual machines (VMs) dynamically are rented from cloud providers. Estimation of required VMs is performed through a central-based method and the number of VMs is calculated periodically. Our simulation based performance evaluation shows the efficiency of the proposed method. © 2015 IEEE.},
author_keywords={cloud computing;  live video;  overlay network;  p2p streaming},
keywords={Bandwidth;  Cloud computing;  Network security;  Overlay networks;  Quality of service;  Video streaming;  Virtual machine, Available bandwidth;  Live video;  Live video streaming;  Local resources;  P2p overlay networks;  P2P streaming;  Performance evaluations;  Upload bandwidths, Peer to peer networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gijsbers2014988,
author={Gijsbers, B. and Grelck, C.},
title={An efficient scalable runtime system for macro data flow processing using S-net},
journal={International Journal of Parallel Programming},
year={2014},
volume={42},
number={6},
pages={988-1011},
doi={10.1007/s10766-013-0271-8},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906951667&doi=10.1007%2fs10766-013-0271-8&partnerID=40&md5=6b8fd90ef35369e8754844190294822f},
abstract={S-Net is a declarative coordination language and component technology aimed at radically facilitating software engineering for modern parallel compute systems by near-complete separation of concerns between application (component) engineering and concurrency orchestration. S-Net builds on the concept of stream processing to structure networks of communicating asynchronous components implemented in a conventional (sequential) language. In this paper we present the design, implementation and evaluation of a new and innovative runtime system for S-Net streaming networks. The Front runtime system outperforms the existing implementations of S-Net by orders of magnitude for stress-test benchmarks, significantly reduces runtimes of fully-fledged parallel applications with compute-intensive components and achieves good scalability on our 48-core test system. © 2013 Springer Science+Business Media New York.},
author_keywords={Declarative parallel programming languages and libraries: semantics and implementation;  High-level programming models},
keywords={Parallel programming;  Semantics, Asynchronous components;  Component technologies;  Coordination language;  Declarative parallel programming languages and libraries: semantics and implementation;  High-level programming models;  Orders of magnitude;  Parallel application;  Separation of concerns, Software engineering},
publisher={Springer New York LLC},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Viel2014199,
author={Viel, E. and Ueda, H.},
title={Data stream partitioning re-optimization based on runtime dependency mining},
journal={Proceedings - International Conference on Data Engineering},
year={2014},
pages={199-206},
doi={10.1109/ICDEW.2014.6818327},
art_number={6818327},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901817403&doi=10.1109%2fICDEW.2014.6818327&partnerID=40&md5=ca7746ec6bc650ae6fead89872b11406},
abstract={In distributed data stream processing, a program made of multiple queries can be parallelized by partitioning input streams according to the values of specific attributes, or partitioning keys. Applying different partitioning keys to different queries requires re-partitioning intermediary streams, causing extra communication and reduced throughput. Re-partitionings can be avoided by detecting dependencies between the partitioning keys applicable to each query. Existing partitioning optimization methods analyze query syntax at compile-time to detect inter-key dependencies and avoid re-partitionings. This paper extends those compile-time methods by adding a runtime re-optimization step based on the mining of temporal approximate dependencies (TADs) between partitioning keys. A TAD is defined in this paper as a type of dependency that can be approximately valid over a moving time window. Our evaluation, based on a simulation of the Linear Road Benchmark, showed a 94.5% reduction of the extra communication cost. © 2014 IEEE.},
author_keywords={Data-stream processing;  Dependency mining;  Distributed processing;  DSMS;  Partitioning optimization;  Temporal approximate dependencies},
keywords={Communication, Approximate dependencies;  Communication cost;  Data stream partitioning;  Distributed data stream processing;  Distributed processing;  DSMS;  Multiple queries;  Optimization method, Technical presentations},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Spanos2012178,
author={Spanos, D.-E. and Stavrou, P. and Mitrou, N. and Konstantinou, N.},
title={SensorStream: A semantic real-time stream management system},
journal={International Journal of Ad Hoc and Ubiquitous Computing},
year={2012},
volume={11},
number={2-3},
pages={178-193},
doi={10.1504/IJAHUC.2012.050252},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869762111&doi=10.1504%2fIJAHUC.2012.050252&partnerID=40&md5=08c2ad70a19a9f702cc67e5f919a1f3a},
abstract={As data proliferates at increasing rates, the need for real-time stream processing applications increases as well. In the same way that Data Stream Management Systems (DSMS) have emerged from the database community, there is now a similar concern in managing dynamic knowledge among the Semantic Web community. Unfortunately, early relevant approaches are, to a large extent, theoretical and do not present convincing evidence of their efficiency in real dynamic environments. In this paper, we present a framework for the effective, real-time processing of streaming data and we define and analyse in depth its key components. Our framework serves as a basis for the implementation of the SensorStream prototype, on which we run numerous performance and scalability measurements that outline its behaviour and demonstrate its suitability and scalability for solutions that require real-time information processing from distributed and heterogeneous data sources. Copyright © 2012 Inderscience Enterprises Ltd.},
author_keywords={Data management;  Distributed sources;  Dynamic knowledge;  Heterogeneity;  Information processing;  Ontology;  Performance measurements;  Real-time;  Scalable framework;  Semantic management;  Sensors;  Stream data;  Windowing},
keywords={Data handling;  Data processing;  Database systems;  Delay tolerant networks;  Ontology;  Scalability;  Sensors, Distributed sources;  Heterogeneity;  Performance measurements;  Real time;  Scalable framework;  Stream data;  Windowing, Information management},
publisher={Inderscience Publishers},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Qi2012718,
author={Qi, K. and Zhao, Z. and Fang, J. and Han, Y.},
title={MapReduce-based data stream processing over large history data},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7636 LNCS},
pages={718-732},
doi={10.1007/978-3-642-34321-6_57},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868382730&doi=10.1007%2f978-3-642-34321-6_57&partnerID=40&md5=c6f25f59ea36d48e7d918bdec2062ff3},
abstract={With the development of Internet of Things applications based on sensor data, how to process high speed data stream over large scale history data brings a new challenge. This paper proposes a new programming model RTMR, which improves the real-time capability of traditional batch processing based MapReduce by preprocessing and caching, along with pipelining and localizing. Furthermore, to adapt the topologies to application characteristics and cluster environments, a model analysis based RTMR cluster constructing method is proposed. The benchmark built on the urban vehicle monitoring system shows RTMR can provide the real-time capability and scalability for data stream processing over large scale data. © Springer-Verlag Berlin Heidelberg 2012.},
author_keywords={Data stream processing;  Large scale data processing;  MapReduce},
keywords={Batch data processing;  Distributed computer systems;  Topology, Cluster environments;  Data stream processing;  Internet of things applications;  Large scale data;  Large-scale data processing;  Map-reduce;  Programming models;  Real time capability, Data handling},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ajwani2011409,
author={Ajwani, D. and Ali, S. and Katrinis, K. and Li, C.-H. and Park, A.J. and Morrison, J.P. and Schenfeld, E.},
title={A flexible workload generator for simulating stream computing systems},
journal={IEEE International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems - Proceedings},
year={2011},
pages={409-417},
doi={10.1109/MASCOTS.2011.54},
art_number={6005385},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053024732&doi=10.1109%2fMASCOTS.2011.54&partnerID=40&md5=e4f324548980b389da109b55ccae8fd3},
abstract={Stream computing is an emerging computational model for performing complex operations on and across multi-source, high volume data ?ows. Given that the deployment of the model has only started, the pool of mature applications employing this model is fairly small, and therefore the availability of workloads for various types of applications is scarce. Thus, there is a need for synthetic generation of large-scale workloads for evaluation of stream computing applications at scale. This paper presents a framework for producing synthetic workloads for stream computing systems. Our framework extends known random graph generation concepts with stream computing spe-cific features, providing researchers with realistic input stream graphs and allowing them to focus on system development, optimization and analysis. Serving the goal of covering a disparity of potential applications, the presented framework exhibits high user-controlled configurability. The produced workloads could be used to drive simulations for performance evaluation and for proof-of-concept prototyping of processing, networking and operating system hardware and software. © 2011 IEEE.},
keywords={Complex operations;  Computational model;  Configurability;  Drive simulation;  Input streams;  Multisources;  Performance evaluation;  Potential applications;  Proof of concept;  Random graphs;  Stream computing;  Synthetic generation;  Synthetic workloads;  System development;  Volume data, Lakes, Telecommunication systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Leidi201136,
author={Leidi, T. and Heeb, T. and Colla, M. and Thiran, J.-P.},
title={Event-driven scheduling for parallel stream processing},
journal={Proceedings - 6th International Symposium on Parallel Computing in Electrical Engineering, PARELEC 2011},
year={2011},
pages={36-41},
doi={10.1109/PARELEC.2011.14},
art_number={5770398},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958748958&doi=10.1109%2fPARELEC.2011.14&partnerID=40&md5=bf6d4fe076da9927b4c976344f364a54},
abstract={To optimize real-time stream-processing applications for chip-level multi processors, several challenges have to be met. Poor scalability and poor internal data pressure may result from serial dependencies within or between the algorithms. Load imbalances introduced by the parallel-processing hardware and execution environment may also limit performance. To maximize the throughput and minimize the latency of parallel stream-processing applications, we propose an approach that complements run-time dynamic load balancing with static pre-compile partitioning. In our solution, the dynamic features are based on event-driven scheduling, while the static features benefit from profile-guided automatic optimizations. In this paper, we present some recent enhancements of DSPE, an open-source development environment, featuring model and source code generators for prototyping, refining and customizing real-time stream-processing applications. By using our approach on micro-benchmarks and sample applications, we also show that it is possible to reduce the impact of the different speed-up constrainers. © 2011 IEEE.},
author_keywords={code generation;  dynamic load-balancing;  event-driven scheduling;  parallel processing;  profile-guided optimizations;  stream processing},
keywords={Code Generation;  dynamic load-balancing;  event-driven scheduling;  parallel processing;  profile-guided optimizations;  Stream processing, Benchmarking;  Dynamic loads;  Electrical engineering;  Network management;  Optimization;  Parallel architectures, Refining},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Takeshi2011958,
author={Takeshi, N. and Akihiko, I. and Kozo, O. and Shinji, K.},
title={QoS analysis of real-time distributed systems based on hybrid analysis of probabilistic model checking technique and simulation},
journal={IEICE Transactions on Information and Systems},
year={2011},
volume={E94-D},
number={5},
pages={958-966},
doi={10.1587/transinf.E94.D.958},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955628189&doi=10.1587%2ftransinf.E94.D.958&partnerID=40&md5=0a499706189df4fc575e41762df4d134},
abstract={For the Internet, system developers often have to estimate the QoS by simulation techniques or mathematical analysis. Probabilistic model checking can evaluate performance, dependability and stability of information processing systems with random behaviors. We apply a hybrid analysis approach onto real-time distributed systems. In the hybrid analysis approach, we perform stepwise analysis using probabilistic models of target systems in different abstract levels. First, we create a probabilistic model with detailed behavior of the system (called detailed model), and apply simulation on the detailed model. Next, based on the simulation results, we create a probabilistic model in an abstract level (called simplified model). Then, we verify qualitative properties using the probabilistic model checking techniques. This prevents from state-explosion. We evaluate the validity of our approach by comparing to simulation results of NS-2 using a case study of a video data streaming system. The experiments show that the result of the proposed approach is very close to that of NS-2 simulation. The result encourages the approach is useful for the performance analysis on various domain. © 2011 The Institute of Electronics.},
author_keywords={Model checking;  Probabilistic automaton;  QoS;  Simulation},
keywords={Quality of service;  Real time systems, Information processing systems;  Mathematical analysis;  Probabilistic automata;  Probabilistic model checking;  Probabilistic modeling;  Qualitative properties;  Real time distributed systems;  Simulation, Model checking},
publisher={Institute of Electronics, Information and Communication, Engineers, IEICE},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cazalas2010112,
author={Cazalas, J. and Guha, R.},
title={GEDS: GPU execution of continuous queries on spatio-temporal data streams},
journal={Proceedings - IEEE/IFIP International Conference on Embedded and Ubiquitous Computing, EUC 2010},
year={2010},
pages={112-119},
doi={10.1109/EUC.2010.26},
art_number={5703506},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951784004&doi=10.1109%2fEUC.2010.26&partnerID=40&md5=35ccdc0918d5c50629ba7899e21bff1f},
abstract={Much research exists for the efficient processing of spatio-temporal data streams. However, all methods ultimately rely on an ill-equipped processor [22], namely a CPU, to evaluate concurrent, continuous spatio-temporal queries over these data streams. This paper presents GEDS, a scalable, Graphics Processing Unit (GPU)-based framework for the evaluation of continuous spatio-temporal queries over spatio-temporal data streams. GEDS employs the computation sharing and parallel processing paradigms to deliver scalability in the evaluation of continuous spatio-temporal queries. The GEDS framework utilizes the parallel processing capability of the GPU, a stream processor by trade, to handle the computation required in this application. Experimental evaluation shows promising performance and shows the scalability and efficacy of GEDS in spatio-temporal data streaming environments. © 2010 IEEE.},
author_keywords={Computation sharing;  Continuous query;  GPU;  Graphical processing unit;  Location-based services;  Mobile database systems;  Parallel processing;  Spatio-temporal data streams},
keywords={Computation sharing;  Continuous query;  GPU;  Graphical processing units;  Location-based services;  Mobile database systems;  Parallel processing;  Spatio-temporal data, Computer graphics equipment;  Data communication systems;  Data reduction;  Database systems;  Human computer interaction;  Industrial research;  Program processors;  Scalability;  Search engines;  Ubiquitous computing, Parallel processing systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kowalski2009243,
author={Kowalski, G. and Hefeeda, M.},
title={Empirical analysis of multi-sender segment transmission algorithms in peer-to-peer streaming},
journal={ISM 2009 - 11th IEEE International Symposium on Multimedia},
year={2009},
pages={243-250},
doi={10.1109/ISM.2009.55},
art_number={5363755},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949500701&doi=10.1109%2fISM.2009.55&partnerID=40&md5=bbce3f0853127b37017df456000e159a},
abstract={We study and analyze segment transmission scheduling algorithms in swarm-based peer-to-peer (P2P) streaming systems. These scheduling algorithms are responsible for coordinating the streaming of video data from multiple senders to a receiver in each streaming session. Although scheduling algorithms directly impact the user-perceived visual quality in streaming sessions, they have not been rigorously analyzed in the literature. In this paper, we first conduct an extensive experimental study to evaluate various scheduling algorithms on many PlanetLab nodes distributed all over the world. We study three important performance metrics: (i) continuity index which captures the smoothness of the video playback, (ii) load balancing index which indicates how the load is spread across sending peers, and (iii) buffering delay required to ensure continuous playback. Our experimental analysis reveals the strengths and weaknesses of each scheduling algorithm, and provides insights for developing better ones in order to improve the overall performance of P2P streaming systems. Then, we propose a new scheduling algorithm called On-time Delivery of VBR streams (ODV). Our experiments show that the proposed scheduling algorithm improves the playback quality by increasing the continuity index, requires smaller buffering delays, and achieves more balanced load distribution across peers. © 2009 IEEE.},
author_keywords={Multisender transmission;  Peer-to-peer streaming;  Segment scheduling},
keywords={Balanced loads;  Buffering delay;  Continuity index;  Empirical analysis;  Experimental analysis;  Experimental studies;  Load-Balancing;  On-time delivery;  P2P streaming;  Peer to peer;  Peer-to-peer streaming;  Performance metrics;  Planetlab nodes;  Segment scheduling;  Transmission scheduling algorithms;  Video data;  Video Playback;  Visual qualities, Banks (bodies of water);  Distributed computer systems;  Peer to peer networks;  Video recording, Scheduling algorithms},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shen2009355,
author={Shen, Z. and Kawashima, H. and Kitagawa, H.},
title={Efficient probabilistic event stream processing with lineage and Kleene-plus},
journal={International Journal of Communication Networks and Distributed Systems},
year={2009},
volume={2},
number={4},
pages={355-374},
doi={10.1504/IJCNDS.2009.026554},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956299079&doi=10.1504%2fIJCNDS.2009.026554&partnerID=40&md5=dd25f103b2688d293c707aa8b2668f62},
abstract={This paper proposes a working framework and a query language to support probabilistic queries for composite event detection over probabilistic event streams. The language allows users to express Kleene closure patterns for complex event detection in the physical world. Our processing method first detects sequence patterns over probabilistic data streams using AIG, a new data structure, which handles active states with a nondeterministic finite automaton (NFA). Our method then computes the probability of each detected sequence pattern based on its lineage. Through the benefit of lineage, the probability of an output event can be directly calculated without taking into account the query plan. An optimised plan can be selected. Finally, we conducted a performance evaluation of our method and compared the results with the original and optimised query plan. The experiment clearly showed that our proposal outperforms straight-forward query plans. Copyright © 2009 Inderscience Enterprises Ltd.},
author_keywords={Data stream;  Event stream processing;  Kleene-plus;  Lineage;  Nondeterministic finite automata;  Probabilistic data management},
keywords={Finite automata;  Information management;  Pipeline processing systems;  Processing;  Query languages;  Sensor networks, Data stream;  Event stream processing;  Kleene-plus;  Lineage;  Nondeterministic finite automaton;  Probabilistic data, Computer hardware description languages},
publisher={Inderscience Publishers},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hoong200851,
author={Hoong, P.K. and Matsuo, H.},
title={PALMS: A reliable and incentive-based P2P live media streaming system},
journal={Lecture Notes in Electrical Engineering},
year={2008},
volume={4 LNEE},
pages={51-66},
doi={10.1007/978-0-387-74938-9_5},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954727919&doi=10.1007%2f978-0-387-74938-9_5&partnerID=40&md5=da322af06872f4cd10bae7920f04aa03},
abstract={In recent years, the peer-to-peer (P2P) approach for media streaming has been studied extensively. In comparison with on-demand media streaming, P2P live media streaming faces a very stringent time constraint. To improve the performance metrics, such as startup delay, source-to-end delay, and playback continuity, we present PALMS - a P2P approach for live media streaming where a node employs gossip-based pull and push protocols to receive and forward media data among connected nodes.We present a simple heuristic mechanism for the pull protocol in the selection of media segments and peers. Besides the pull method, a push method is deployed to increase the streaming quality. We know that the presence of free-riders could degrade the delivered streaming quality. In PALMS, a simple score-based incentive mechanism, similar to BitTorrent's tit-for-tat incentive mechanism, is adopted to discourage the existence of free-riders. We conducted simulations and performance comparisons for PALMS. Experimental results demonstrate that PALMS can deliver better streaming quality and more resilience towards the heterogeneity of network bandwidths as compared to some of the existing protocols. © Springer 2008.},
keywords={Heuristic mechanisms;  Incentive mechanism;  Live media streaming;  Network bandwidth;  On-demand media streaming;  Performance comparison;  Performance metrics;  Time constraints, Communication systems;  Distributed computer systems;  Electrical engineering;  Media streaming, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sun20071733,
author={Sun, X. and Sun, N. and Chen, M.},
title={Optimization of B-NIDS for multicore},
journal={Jisuanji Yanjiu yu Fazhan/Computer Research and Development},
year={2007},
volume={44},
number={10},
pages={1733-1740},
doi={10.1360/crad20071015},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-37449021135&doi=10.1360%2fcrad20071015&partnerID=40&md5=b76089194d242821f13ec7bf83deabe1},
abstract={With the rapid increase of network bandwidth and the growing variety of Internet applications, the backbone network intrusion detection systems (B-NIDS) meet the great requirements of delivering higher performance and enhancing effectiveness according to different features of network streams. Computing is entering a new phase in which CPU improvements are driven by the addition of multiple cores on a single chip, rather than higher frequencies. Parallel processing on these systems is in a primitive stage, and the parallelization of a sequential B-NIDS requires the explicit use and knowledge of underlying thread architecture. In this paper the bottleneck of the thread synchronization using fine-grained lock operations is discovered, and the new synchronization mechanism with no contention for shared structures is proposed based on the characteristics of data flow. Then a pipelining programming model of multithreading system with three contexts is issued, and the differential service for streams is implemented with the multiple weighed queues. In performance evaluation, the optimized system shows much better performance in three aspects of resource utilization, throughput, and response time on 8 core server. The improved system with the proposed synchronization mechanism shows good scalability. The processing capability on tested server can exceed over 1 Gbps traffic flow. Also the multiple weighed queues for service quality introduce little latency, and a kind of probe-based sampling test shows that the response times of prioritized streams are shorter than those of non-prioritized.},
author_keywords={Multicore;  Multithreading programming model;  Network intrusion detection;  Parallel optimization;  Synchronization overhead},
keywords={Bandwidth;  Internet;  Optimization;  Parallel processing systems;  Sampling;  Scalability;  Synchronization, Data flow;  Multicore;  Multithreading programming model;  Parallel optimization, Intrusion detection},
language={Chinese},
document_type={Article},
source={Scopus},
}

@CONFERENCE{DeHeus202131,
author={De Heus, M. and Psarakis, K. and Fragkoulis, M. and Katsifodimos, A.},
title={Distributed transactions on serverless stateful functions},
journal={DEBS 2021 - Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
year={2021},
pages={31-42},
doi={10.1145/3465480.3466920},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110321976&doi=10.1145%2f3465480.3466920&partnerID=40&md5=713dfa94af5fb3e1d454fc6b62562113},
abstract={Serverless computing is currently the fastest-growing cloud services segment. The most prominent serverless offering is Function-as-a-Service (FaaS), where users write functions and the cloud automates deployment, maintenance, and scalability. Although FaaS is a good fit for executing stateless functions, it does not adequately support stateful constructs like microservices and scalable, low-latency cloud applications, mainly because it lacks proper state management support and the ability to perform function-to-function calls. Most importantly, executing transactions across stateful functions remains an open problem. In this paper, we introduce a programming model and implementation for transaction orchestration of stateful serverless functions. Our programming model supports serializable distributed transactions with two-phase commit, as well as relaxed transactional guarantees with Sagas. We design and implement our programming model on Apache Flink StateFun. We choose to build our solution on top of StateFun in order to leverage Flink's exactly-once processing and state management guarantees. We base our evaluation on the YCSB benchmark, which we extended with transactional operations and adapted for the SFaaS programming model. Our experiments show that our transactional orchestration adds 10% overhead to the original system and that Sagas can achieve up to 34% more transactions per second than two-phase commit transactions at a sub-200ms latency. © 2021 Owner/Author.},
author_keywords={FaaS;  sagas;  serverless;  streaming dataflow;  transactions;  two-phase commit},
keywords={Cloud applications;  Design and implements;  Distributed transaction;  Original systems;  Programming models;  State management;  Transaction orchestrations;  Two phase commit, Software architecture},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gencer20213110,
author={Gencer, C. and Topolnik, M. and Ďurina, V. and Demirci, E. and Kahveci, E.B. and Gürbüz, A. and Lukáš, O. and Bartók, J. and Gierlach, G. and Hartman, F. and Yılmaz, U. and Doğan, M. and Mandouh, M. and Fragkoulis, M. and Katsifodimos, A.},
title={Hazelcast jet: Low-latency stream processing at the 99.99th percentile},
journal={Proceedings of the VLDB Endowment},
year={2021},
volume={14},
number={12},
pages={3110-3121},
doi={10.14778/3476311.3476387},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110289668&doi=10.14778%2f3476311.3476387&partnerID=40&md5=8a3ffb9037ef4ef63ed70b15c0dc8221},
abstract={Jet is an open-source, high-performance, distributed stream processor built at Hazelcast during the last five years. Jet was engineered with millisecond latency on the 99.99th percentile as its primary design goal. Originally Jet’s purpose was to be an execution engine that performs complex business logic on top of streams generated by Hazelcast’s In-memory Data Grid (IMDG): a set of in-memory, partitioned and replicated data structures. With time, Jet evolved into a full-fledged, scale-out stream processor that can handle out-of-order streams and provide exactly-once processing guarantees. Jet’s end-to-end latency lies in the order of milliseconds, and its throughput in the order of millions of events per CPU-core. This paper presents the main design decisions we made in order to maximize the performance per CPU-core, alongside lessons learned, and an empirical performance evaluation. © The authors.},
keywords={Grid computing, Business logic;  CPU cores;  Design goal;  Execution engine;  Low latency;  Open-source;  Performance;  Primary design;  Stream processing;  Stream processor, Integrated circuit design},
publisher={VLDB Endowment},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fu2020,
author={Fu, Z. and Tang, Z. and Yang, L. and Li, K. and Li, K.},
title={ImRP: A Predictive Partition Method for Data Skew Alleviation in Spark Streaming Environment},
journal={Parallel Computing},
year={2020},
volume={100},
doi={10.1016/j.parco.2020.102699},
art_number={102699},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092286578&doi=10.1016%2fj.parco.2020.102699&partnerID=40&md5=0f5d5a1400df2d8e355459669b507283},
abstract={Spark Streaming is an extension of the core Spark engine that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. It treats stream as a series of deterministic batches and handles them as regular jobs. However, for a stream job responsible for a batch, data skew (i.e., the imbalance in the amount of data allocated to each reduce task), can degrade the job performance significantly because of load imbalance. In this paper, we propose an improved range partitioner (ImRP) to alleviate the reduce skew for stream jobs in Spark Streaming. Unlike previous work, ImRP does not require any pre-run sampling of input data and generates the data partition scheme based on the intermediate data distribution estimated by the previous batch processing, in which a prediction model EWMA (Exponentially Weighted Moving Average) is adopted. To lighten the data skew, ImRP presents a novel method of calculating the partition borders optimally, and a mechanism of splitting the border key clusters when the semantics of shuffle operators permit. Besides, ImRP considers the integrated partition size and heterogeneity of computing environments when balancing the load among reduce tasks appropriately. We implement ImRP in Spark-3.0 and evaluate its performance on four representative benchmarks: wordCount, sort, pageRank, and LDA. The results show that by mitigating the data skew, ImRP can decrease the execution time of stream jobs substantially compared with some other partition strategies, especially when the skew degree of input batch is serious. © 2020},
author_keywords={Data skew;  Load imbalance;  MapReduce;  Spark Streaming},
keywords={Balancing;  Benchmarking;  Data streams;  Predictive analytics;  Semantics, Computing environments;  Data distribution;  Exponentially weighted moving average;  High throughput;  Job performance;  Partition methods;  Prediction model;  Stream processing, Batch data processing},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khudabukhsh20202594,
author={Khudabukhsh, W.R. and Kar, S. and Alt, B. and Rizk, A. and Koeppl, H.},
title={Generalized Cost-Based Job Scheduling in Very Large Heterogeneous Cluster Systems},
journal={IEEE Transactions on Parallel and Distributed Systems},
year={2020},
volume={31},
number={11},
pages={2594-2604},
doi={10.1109/TPDS.2020.2997771},
art_number={9099971},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086450012&doi=10.1109%2fTPDS.2020.2997771&partnerID=40&md5=2fb556774f6cb808ade3ac28ed170df3},
abstract={We study job assignment in large, heterogeneous resource-sharing clusters of servers with finite buffers. This load balancing problem arises naturally in today's communication and big data systems, such as Amazon Web Services, Network Service Function Chains, and Stream Processing. Arriving jobs are dispatched to a server, following a load balancing policy that optimizes a performance criterion such as job completion time. Our contribution is a randomized Cost-Based Scheduling (CBS) policy in which the job assignment is driven by general cost functions of the server queue lengths. Beyond existing schemes, such as the Join the Shortest Queue (JSQ), the power of dd or the SQ(dd) and the capacity-weighted JSQ, the notion of CBS yields new application-specific policies such as hybrid locally uniform JSQ. As today's data center clusters have thousands of servers, exact analysis of CBS policies is tedious. In this article, we derive a scaling limit when the number of servers grows large, facilitating a comparison of various CBS policies with respect to their transient as well as steady state behavior. A byproduct of our derivations is the relationship between the queue filling proportions and the server buffer sizes, which cannot be obtained from infinite buffer models. Finally, we provide extensive numerical evaluations and discuss several applications including multi-stage systems. © 1990-2012 IEEE.},
author_keywords={Job scheduling;  mean-field limit;  performance evaluation},
keywords={Cost functions;  Data streams;  Electric circuit breakers;  Queueing theory;  Scheduling;  Web services, Cost-based scheduling;  Heterogeneous clusters;  Heterogeneous resources;  Join-the-shortest-queue;  Load balancing policies;  Load balancing problem;  Performance criterion;  Steady-state behaviors, Servers},
publisher={IEEE Computer Society},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Venugopal202051,
author={Venugopal, V.E. and Theobald, M. and Chaychi, S. and Tawakuli, A.},
title={AIR: A light-weight yet high-performance dataflow engine based on asynchronous iterative routing},
journal={Proceedings - Symposium on Computer Architecture and High Performance Computing},
year={2020},
volume={2020-September},
pages={51-58},
doi={10.1109/SBAC-PAD49847.2020.00018},
art_number={9235069},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095863087&doi=10.1109%2fSBAC-PAD49847.2020.00018&partnerID=40&md5=1a9022cf45db1c92fb5ad32bf3c34d1a},
abstract={Distributed Stream Processing Engines (DSPEs) are currently among the most emerging topics in data management, with applications ranging from real-time event monitoring to processing complex dataflow programs and big data analytics. In this paper, we describe the architecture of our AIR engine, which is designed from scratch in C++ using the Message Passing Interface (MPI), pthreads for multithreading, and is directly deployed on top of a common HPC workload manager such as SLURM. AIR implements a light-weight, dynamic sharding protocol (referred to as 'Asynchronous Iterative Routing'), which facilitates a direct and asynchronous communication among all worker nodes and thereby completely avoids any additional communication overhead with a dedicated master node. With its unique design, AIR fills the gap between the prevalent scale-out (but Java-based) architectures like Apache Spark and Flink, on one hand, and recent scale-up (and C++ based) prototypes such as StreamBox and PiCo, on the other hand. Our experiments over various benchmark settings confirm that AIR performs as good as the best scale-up SPEs on a single-node setup, while it outperforms existing scale-out DSPEs in terms of processing latency and sustainable throughput by a factor of up to 15 in a distributed setting. © 2020 IEEE.},
author_keywords={Asynchronous stream processing;  Dataflow processing engine;  Distributed stream data processing;  High sustainable throughput;  Stream processing},
keywords={Advanced Analytics;  Application programs;  C++ (programming language);  Data Analytics;  Data streams;  Distributed parameter control systems;  Engines;  Information management;  Message passing, Asynchronous communication;  Asynchronous iterative;  Communication overheads;  Distributed stream processing;  Emerging topics;  Message passing interface;  Multi-threading;  Workload managers, Multitasking},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kesavan2020206,
author={Kesavan, S.P. and Fujiwara, T. and Li, J.K. and Ross, C. and Mubarak, M. and Carothers, C.D. and Ross, R.B. and Ma, K.-L.},
title={A Visual Analytics Framework for Reviewing Streaming Performance Data},
journal={IEEE Pacific Visualization Symposium},
year={2020},
volume={2020-June},
pages={206-215},
doi={10.1109/PacificVis48177.2020.9280},
art_number={9086217},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085181119&doi=10.1109%2fPacificVis48177.2020.9280&partnerID=40&md5=03d0acb351dd2c8ce32cefccbb4e1f34},
abstract={Understanding and tuning the performance of extreme-scale parallel computing systems demands a streaming approach due to the computational cost of applying offline algorithms to vast amounts of performance log data. Analyzing large streaming data is challenging because the rate of receiving data and limited time to comprehend data make it difficult for the analysts to sufficiently examine the data without missing important changes or patterns. To support streaming data analysis, we introduce a visual analytic framework comprising of three modules: data management, analysis, and interactive visualization. The data management module collects various computing and communication performance metrics from the monitored system using streaming data processing techniques and feeds the data to the other two modules. The analysis module automatically identifies important changes and patterns at the required latency. In particular, we introduce a set of online and progressive analysis methods for not only controlling the computational costs but also helping analysts better follow the critical aspects of the analysis results. Finally, the interactive visualization module provides the analysts with a coherent view of the changes and patterns in the continuously captured performance data. Through a multi-faceted case study on performance analysis of parallel discrete-event simulation, we demonstrate the effectiveness of our framework for identifying bottlenecks and locating outliers. © 2020 IEEE.},
author_keywords={Human-centered computing;  Visual analytics;  Visualization;  Visualization application domains},
keywords={Cost benefit analysis;  Data handling;  Data visualization;  Discrete event simulation;  Visualization, Communication performance;  Computational costs;  Interactive visualizations;  Parallel computing system;  Parallel discrete event simulations;  Performance analysis;  Streaming approach;  Streaming data processing, Information management},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang20201537,
author={Zhang, S. and Wu, Y. and Zhang, F. and He, B.},
title={Towards concurrent stateful stream processing on multicore processors},
journal={Proceedings - International Conference on Data Engineering},
year={2020},
volume={2020-April},
pages={1537-1548},
doi={10.1109/ICDE48307.2020.00136},
art_number={9101749},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085862017&doi=10.1109%2fICDE48307.2020.00136&partnerID=40&md5=856ef2d8fc06fa9c836720cb396a07b4},
abstract={Recent data stream processing systems (DSPSs) can achieve excellent performance when processing large volumes of data under tight latency constraints. However, they sacrifice support for concurrent state access that eases the burden of developing stateful stream applications. Recently, some have proposed managing concurrent state access during stream processing by modeling state accesses as transactions. However, these are realized with locks involving serious contention overhead. The coarse-grained processing paradigm adopted in these proposals magnify contention issues and does not exploit modern multicore architectures to their full potential. This paper introduces TStream, a novel DSPS supporting efficient concurrent state access on multicore processors. Transactional semantics is employed like previous work, but scalability is greatly improved due to two novel designs: 1) dual-mode scheduling, which exposes more parallelism opportunities, 2) dynamic restructuring execution, which aggressively exploits the parallelism opportunities from dual-mode scheduling without centralized lock contentions. To validate our proposal, we evaluate TStream with a benchmark of four applications on a modern multicore machine. Experimental results show that 1) TStream achieves up to 4.8 times higher throughput with similar processing latency compared to the state-of-the-art and 2) unlike prior solutions, TStream is highly tolerant of varying application workloads such as key skewness and multi-partition state accesses. © 2020 IEEE.},
keywords={Benchmarking;  Locks (fasteners);  Parallel processing systems;  Scheduling;  Semantics;  Software architecture, Data stream processing;  Latency constraints;  Lock contentions;  Multi-core machines;  Multi-core processor;  Multicore architectures;  Stream application;  Stream processing, Data streams},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kahveci2020205,
author={Kahveci, B. and Gedik, B.},
title={Joker: Elastic stream processing with organic adaptation},
journal={Journal of Parallel and Distributed Computing},
year={2020},
volume={137},
pages={205-223},
doi={10.1016/j.jpdc.2019.10.012},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076826035&doi=10.1016%2fj.jpdc.2019.10.012&partnerID=40&md5=6ab190475e9aaf5a74d3d28b326ab92b},
abstract={This paper addresses the problem of auto-parallelization of streaming applications. We propose an online parallelization optimization algorithm that adjusts the degree of pipeline and data parallelism in a joint manner. We define an operator development API and a flexible parallel execution model to form a basis for the optimization algorithm. The operator interface unifies the development of different types of operators and makes operator properties visible in order to enable safe optimizations. The parallel execution model splits a data flow graph into regions. A region contains the longest sequence of compatible operators that are amenable to data parallelism as a whole and can be further parallelized with pipeline parallelism. We also develop a stream processing run-time, named Joker, to scale the execution of streaming applications in a safe, transparent, dynamic, and automatic manner. This ability is called organic adaptation. Joker implements the runtime machinery to execute a data flow graph with any parallelization configuration and most importantly change this configuration at run-time with low cost in the presence of partitioned stateful operators, in a way that is transparent to the application developers. Joker continuously monitors the run-time performance, and runs the optimization algorithm to resolve bottlenecks and scale the application by adjusting the degree of pipeline and data parallelism. The experimental evaluation based on micro-benchmarks and real-world applications showcase that our solution accomplishes elasticity by finding an effective parallelization configuration. © 2019 Elsevier Inc.},
author_keywords={Elasticity;  Parallelization;  Stream processing},
keywords={Benchmarking;  Computer architecture;  Data flow analysis;  Data transfer;  Elasticity;  Graphic methods;  Machinery;  Optimization;  Pipelines, Application developers;  Experimental evaluation;  Optimization algorithms;  Parallelizations;  Pipeline parallelisms;  Run-time performance;  Stream processing;  Streaming applications, Data flow graphs},
publisher={Academic Press Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Blamey2020335,
author={Blamey, B. and Hellander, A. and Toor, S.},
title={Apache spark streaming, kafka and harmonicio: a performance benchmark and architecture comparison for enterprise and scientific computing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12093 LNCS},
pages={335-347},
doi={10.1007/978-3-030-49556-5_30},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087001451&doi=10.1007%2f978-3-030-49556-5_30&partnerID=40&md5=b750a3af92a1349cd511517637b7f444},
abstract={Many scientific computing applications generate streams where message sizes exceed one megabyte, in contrast with smaller message sizes in enterprise contexts (order kilobytes, often XML or JSON). Furthermore, the processing cost of messages in scientific computing applications are usually an order of magnitude higher than in typical enterprise applications. Frameworks such as Apache Spark offer high throughput processing of streams with such ‘enterprise’ characteristics, as well as scalability, with high resilience and many other desirable features. Motivated by the development of near real-time image processing pipelines for roboticized microscopy, we evaluate the suitability of Apache Spark for streams more typical of scientific computing applications, those with large message sizes (up to 10 MB), and heavy per-message CPU load, under typical stream integrations. For comparison, we benchmark a P2P stream processing framework, HarmonicIO, developed in-house. Our study reveals a complex interplay of performance trade-offs, revealing the boundaries of good performance for each framework and integration over a wide domain of application loads. Based on these results, we suggest which are likely to offer good performance for a given load. Broadly, the advantages of Spark’s rich features makes its performance sensitive to message size in particular, whereas the simplicity of HarmonicIO offers more robust performance, and better CPU utilization. © Springer Nature Switzerland AG 2020.},
author_keywords={Apache Spark;  Benchmark;  HarmonicIO;  HASTE;  High-throughput microscopy;  HPC;  Stream processing;  XaaS},
keywords={Economic and social effects;  Image processing;  Pipeline processing systems, Architecture comparison;  Desirable features;  Enterprise applications;  Performance trade-off;  Robust performance;  Scientific computing applications;  Stream integration;  Stream processing, Benchmarking},
publisher={Springer},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Maroulis20192624,
author={Maroulis, S. and Zacheilas, N. and Kalogeraki, V.},
title={A Holistic Energy-Efficient Real-Time Scheduler for Mixed Stream and Batch Processing Workloads},
journal={IEEE Transactions on Parallel and Distributed Systems},
year={2019},
volume={30},
number={12},
pages={2624-2635},
doi={10.1109/TPDS.2019.2922606},
art_number={8735782},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075105141&doi=10.1109%2fTPDS.2019.2922606&partnerID=40&md5=d35b765e799bbd59264397fa88c06180},
abstract={In recent years we have experienced a wide adoption of novel distributed processing frameworks such as Apache Spark for handling batch and stream processing big data applications. An important aspect that has not been examined in these systems yet, is the energy consumption during the applications' execution. Reducing the energy consumption of modern datacenters is a necessity, as datacenters contribute over 2 percent of the total US electric usage. However, efficiently scheduling applications in distributed processing systems can be challenging as there is a trade-off between minimizing the datacenter's energy usage and satisfying the application performance requirements. In this work we propose, ExpREsS, a scheduler for orchestrating the execution of Spark applications in a way that enables us to minimize the energy consumption while ensuring that the applications' performance requirements are met. Our approach exploits time-series segmentation for capturing the applications' energy usage and execution times, and then applies a novel DVFS technique to minimize the energy consumption. In order to tackle the limited number of application's profiling runs, we exploit regression techniques to predict the applications' execution times and power consumption. Our detailed experimental evaluation using realistic workloads on our local cluster illustrates the working and benefits of our approach. © 1990-2012 IEEE.},
author_keywords={apache spark;  cluster management;  Distributed systems;  green computing;  scheduling},
keywords={Cluster computing;  Data handling;  Economic and social effects;  Electric power utilization;  Energy efficiency;  Energy utilization;  Green computing;  Scheduling, Application performance;  Cluster management;  Distributed processing frameworks;  Distributed processing systems;  Distributed systems;  Experimental evaluation;  Performance requirements;  Time-series segmentation, Batch data processing},
publisher={IEEE Computer Society},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hanif2019393,
author={Hanif, M. and Yoon, H. and Lee, C.},
title={Benchmarking Tool for Modern Distributed Stream Processing Engines},
journal={International Conference on Information Networking},
year={2019},
volume={2019-January},
pages={393-395},
doi={10.1109/ICOIN.2019.8718106},
art_number={8718106},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066761823&doi=10.1109%2fICOIN.2019.8718106&partnerID=40&md5=21e371cc9f903734ea15a9233dadf0ea},
abstract={There is an upsurge in the usage and adaptation of streaming applications in the recent years by both industry and academia. At the core of these applications is streaming data processing engines that perform resource management and allocation in order to support continuous track of queries over distributed data streams. Several stream processing engines exists to handle these distributed streaming applications. In this paper, we present different challenges of the stream processing systems, in particular to stateful operators and implement Linear Road benchmark to examine the characteristic and performance metrics of the streaming system, in particular Apache Flink. Furthermore, we examine that Apache Flink can be used as a core for an efficient Linear Road application implementation for distributed environments without breaching the SLA requirements of the application. © 2019 IEEE.},
author_keywords={Benchmarking;  Cloud Computing;  Distributed Computing;  SLA;  Streaming},
keywords={Acoustic streaming;  Cloud computing;  Data handling;  Distributed computer systems;  Distributed parameter control systems;  Engines;  Information management;  Roads and streets, Distributed data streams;  Distributed environments;  Distributed stream processing;  Distributed streaming;  Stream processing engines;  Stream processing systems;  Streaming applications;  Streaming data processing, Benchmarking},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{López-Gómez201924,
author={López-Gómez, J. and Fernández Muñoz, J. and del Rio Astorga, D. and Dolz, M.F. and Garcia, J.D.},
title={Exploring stream parallel patterns in distributed MPI environments},
journal={Parallel Computing},
year={2019},
volume={84},
pages={24-36},
doi={10.1016/j.parco.2019.03.004},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063648445&doi=10.1016%2fj.parco.2019.03.004&partnerID=40&md5=e8c86fe7ab4ba961fabf366cd1ed79d5},
abstract={In recent years, the large volumes of stream data and the near real-time requirements of data streaming applications have exacerbated the need for new scalable algorithms and programming interfaces for distributed and shared-memory platforms. To contribute in this direction, this paper presents a new distributed MPI back end for GRPPI, a C++ high-level generic interface of data-intensive and stream processing parallel patterns. This back end, as a new execution policy, supports distributed and hybrid (distributed+shared-memory) parallel executions of the Pipeline and Farm patterns, where the hybrid mode combines the MPI policy with a GRPPI shared-memory one. These patterns internally leverage distributed queues, which can be configured to use two-sided or one-sided MPI primitives to communicate items among nodes. A detailed analysis of the GRPPI MPI execution policy reports considerable benefits from the programmability, flexibility and readability points of view. The experimental evaluation of two different streaming applications with different distributed and shared-memory scenarios reports considerable performance gains with respect to the sequential versions at the expense of negligible GRPPI overheads. © 2019 Elsevier B.V.},
author_keywords={C++ programming;  Distributed patterns;  Generic programming;  Parallel patterns;  Stream processing},
keywords={Memory architecture, C++ programming;  Distributed patterns;  Generic programming;  Parallel patterns;  Stream processing, C++ (programming language)},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zacheilas2019378,
author={Zacheilas, N. and Dedousis, D. and Kalogeraki, V.},
title={Scalable Distributed Top-k Join Queries in Topic-Based Pub/Sub Systems},
journal={Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018},
year={2019},
pages={378-383},
doi={10.1109/BigData.2018.8621949},
art_number={8621949},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062601551&doi=10.1109%2fBigData.2018.8621949&partnerID=40&md5=a9fce240775e9b4170091fcb680aecb5},
abstract={In this paper, we provide a novel approach that enables the execution of top-k join queries over sliding windows in a way that reduces the amount of data that need to be analyzed by the stream processing operators. The main idea is that brokers individually invoke the query on their received messages and forward the top-k results to a stream processing operator that performs the merging of the results and provides to the end-user the final top-k results. Moreover, our system exploits the Bayesian Optimization technique to determine automatically the number of top-k results that should be provided by each broker. Our approach has been developed in the Kappa architecture that exploits topic-based scalable publish/subscribe (pub/sub) systems like Apache Kafka to efficiently forward the high volume of incoming messages to distributed processing systems (i.e., Apache Spark or Apache Flink) that perform the batch and stream analytics operations. Our detailed experimental evaluation on our local cluster illustrates that we can efficiently execute top-k join queries on our system with high accuracy and low latency. © 2018 IEEE.},
author_keywords={Kappa architecture;  pub/sub systems;  stream processing;  top-k joins},
keywords={Big data;  Database systems, Bayesian optimization;  Distributed processing systems;  Experimental evaluation;  Local cluster;  Pub/sub systems;  Publish/subscribe;  Sliding Window;  Stream processing, Computer architecture},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{TurrisidaCosta20193,
author={Turrisi da Costa, V.G. and Santana, E.J. and Lopes, J.F. and Barbon, S., Jr.},
title={Evaluating the Four-Way Performance Trade-Off for Stream Classification},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11484 LNCS},
pages={3-17},
doi={10.1007/978-3-030-19223-5_1},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066116119&doi=10.1007%2f978-3-030-19223-5_1&partnerID=40&md5=9e228205c01b085285a1b805aef1adee},
abstract={Machine Learning (ML) solutions need to deal efficiently with a huge amount of data available, addressing scalability concerns without sacrificing predictive performance. Moreover, this data comes in the form of a continuous and evolving stream imposing new constraints, e.g., limited memory and energy resources. In the same way, energy-aware ML algorithms are gaining relevance due to the power constraints of hardware platforms in several real-life applications, as the Internet of Things (IoT). Many algorithms have been proposed to cope with the mutable nature of data streams, with the Very Fast Decision Tree (VFDT) being one of the most widely used. An adaptation of the VFDT, called Strict VFDT (SVFDT), can significantly reduce memory usage without putting aside the predictive performance and time efficiency. However, the analysis of energy consumption regarding data stream processing of the VFDT and SVFDT is overlooked. In this work, we compare the four-way relationship between predictive performance, memory costs, time efficiency and energy consumption, tuning the hyperparameters of the algorithms to optimise the resources devoted to it. Experiments over 23 benchmark datasets revealed that the SVFDT-I is the most energy-friendly algorithm and greatly reduced memory consumption, being statistically superior to the VFDT. © 2019, Springer Nature Switzerland AG.},
author_keywords={Data stream mining;  Energy efficiency;  Machine Learning},
keywords={Cloud computing;  Data handling;  Data mining;  Decision trees;  Economic and social effects;  Energy resources;  Energy utilization;  Green computing;  Internet of things;  Learning systems;  Machine learning;  Power management;  Trees (mathematics), Data stream mining;  Data stream processing;  Internet of thing (IOT);  Performance trade-off;  Predictive performance;  Real-life applications;  Stream classification;  Very fast decision trees, Energy efficiency},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chauvin2019,
author={Chauvin, E.},
title={Efficiency, economics and compliance: A practical planning tool for optimizing cuttings treatment},
journal={Society of Petroleum Engineers - Abu Dhabi International Petroleum Exhibition and Conference 2018, ADIPEC 2018},
year={2019},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059976367&partnerID=40&md5=5d549b6b0bb1259d99b127a747e68915},
abstract={The expanding focus on total cost of ownership (TCO) in the oilfield is driving the application of global best practices in drilling and completion operations. Measurable benefits can result from a proactive approach to separating, treating, and handling disposal streams that accounts for both short- and long-term effects and is tailored to the specific needs of projects in the Middle East. This paper provides a versatile decision-making tool to help determine the optimal processes for a variety of conditions based on efficiency, economics, and sustainability. The most effective treatment solutions and services seek to address multiple factors, including logistics and expenses, drilling efficiency and performance, remote locations with infrastructure limitations, and strict environmental regulations. Interdependencies between the fluids, equipment, and services will also influence the selection of appropriate separation and treatment processes. A useful decision tree is based on the critical goals of waste volume reduction and maximum recovery of valuable components. Making the right choices in both of these areas will provide effective long-term results and economic benefits. This paper summarizes multiple case histories that demonstrate successful cuttings and waste stream processing, including both land and offshore operations. In some cases, a mobile or well-specific treatment system works best; others rely on a central treatment facility to serve an entire field or multirig operation. The results achieved in each case contribute to a step-by-step planning tool in matrix format that can be used to design the best set of equipment and services for each location. The matrix accounts for many factors, including rig equipment and capacities, proposed drilling and completion fluids, lithology, risk assessment, system maintenance, potential process rates and throughput, mobilization, environmental regulations, and infrastructure requirements. The decision tree presented facilitates the treatment selection process by incorporating useful benchmarks for volumes, process rates, expected base oil/fluid recovery, installation costs/requirements, scalability, longevity, and decommissioning (and/or relocation of the treatment system). It provides a practical starting point for planning an efficient, fit-for-purpose treatment configuration, scaled to match operational needs, reduce total cost of ownership, and meet or exceed existing and future standards. © Copyright 2018, Society of Petroleum Engineers.},
keywords={Costs;  Data mining;  Decision making;  Decision trees;  Drilling fluids;  Economics;  Efficiency;  Environmental regulations;  Gasoline;  Infill drilling;  Lithology;  Offshore oil well production;  Offshore oil wells;  Oil well completion;  Optical variables measurement;  Risk assessment;  Sustainable development, Decision making tool;  Drilling and completion;  Drilling and completion fluids;  Drilling efficiency;  Offshore operations;  Pro-active approach;  Total cost of ownership;  Treatment selection, Drilling equipment},
publisher={Society of Petroleum Engineers},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{VanDongen2018247,
author={Van Dongen, G. and Steurtewagen, B. and Van Den Poel, D.},
title={Latency Measurement of Fine-Grained Operations in Benchmarking Distributed Stream Processing Frameworks},
journal={Proceedings - 2018 IEEE International Congress on Big Data, BigData Congress 2018 - Part of the 2018 IEEE World Congress on Services},
year={2018},
pages={247-250},
doi={10.1109/BigDataCongress.2018.00043},
art_number={8457759},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057767900&doi=10.1109%2fBigDataCongress.2018.00043&partnerID=40&md5=48f5bc04862b6f0e84a100a3ec814a86},
abstract={This paper describes a benchmark for stream processing frameworks allowing accurate latency benchmarking of fine-grained individual stages of a processing pipeline. By determining the latency of distinct common operations in the processing flow instead of the end-to-end latency, we can form guidelines for efficient processing pipeline design. Additionally, we address the issue of defining time in distributed systems by capturing time on one machine and defining the baseline latency. We validate our benchmark for Apache Flink using a processing pipeline comprising common stream processing operations. Our results show that joins are the most time consuming operation in our processing pipeline. The latency incurred by adding a join operation is 4.5 times higher than for a parsing operation, and the latency gradually becomes more dispersed after adding additional stages. © 2018 IEEE.},
author_keywords={benchmark;  big data applications;  distributed stream computing;  Flink;  Kafka},
keywords={Benchmarking;  Big data;  Distributed parameter control systems;  Pipelines, Big data applications;  Distributed stream processing;  Distributed systems;  End to end latencies;  Flink;  Kafka;  Latency measurements;  Stream computing, Pipeline processing systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dolz201839944,
author={Dolz, M.F. and Del Rio Astorga, D. and Fernandez, J. and Daniel Garcia, J. and Carretero, J.},
title={Towards Automatic Parallelization of Stream Processing Applications},
journal={IEEE Access},
year={2018},
volume={6},
pages={39944-39961},
doi={10.1109/ACCESS.2018.2855064},
art_number={8409948},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049796118&doi=10.1109%2fACCESS.2018.2855064&partnerID=40&md5=7ab075c3f269ada2d953f2f6327a09e4},
abstract={Parallelizing and optimizing codes for recent multi-/many-core processors have been recognized to be a complex task. For this reason, strategies to automatically transform sequential codes into parallel and discover optimization opportunities are crucial to relieve the burden to developers. In this paper, we present a compile-time framework to (semi) automatically find parallel patterns (Pipeline and Farm) and transform sequential streaming applications into parallel using GrPPI, a generic parallel pattern interface. This framework uses a novel pipeline stage-balancing technique which provides the code generator module with the necessary information to produce balanced pipelines. The evaluation, using a synthetic video benchmark and a real-world computer vision application, demonstrates that the presented framework is capable of producing parallel and optimized versions of the application. A comparison study under several thread-core oversubscribed conditions reveals that the framework can bring comparable performance results with respect to the Intel TBB programming framework. © 2013 IEEE.},
author_keywords={automatic parallelization;  load-balanced pipeline;  parallel patterns;  Refactoring framework},
keywords={Benchmarking;  Codes (symbols);  Job analysis;  Mathematical programming;  Optimization;  Pipelines;  Tools, Automatic Parallelization;  C++ language;  Concurrent computing;  Load-balanced;  Parallel patterns;  Refactoring framework;  Task analysis, C++ (programming language)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bilal201816,
author={Bilal, M. and Alsibyani, H. and Canini, M.},
title={Mitigating network side channel leakage for stream processing systems in trusted execution environments},
journal={DEBS 2018 - Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
year={2018},
pages={16-27},
doi={10.1145/3210284.3210286},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050519606&doi=10.1145%2f3210284.3210286&partnerID=40&md5=5237c3179854947d7c18e9e92e9ed9cf},
abstract={A crucial concern regarding cloud computing is the confidentiality of sensitive data being processed in the cloud. Trusted Execution Environments (TEEs), such as Intel Software Guard eXtensions (SGX), allow applications to run securely on an untrusted platform. However, using TEEs alone for stream processing is not enough to ensure privacy as network communication patterns may leak information about the data. This paper introduces two techniques - anycast and multicast - for mitigating leakage at inter-stage communications in streaming applications according to a user-selected mitigation level. These techniques aim to achieve network data obliviousness, i.e., communication patterns should not depend on the data. We implement these techniques in an SGX-based stream processing system. We evaluate the latency and throughput overheads, and the data obliviousness using three benchmark applications. The results show that anycast scales better with input load and mitigation level, and provides better data obliviousness than multicast. © 2018 Copyright held by the owner/author(s).},
author_keywords={Intel SGX;  Network data obliviousness;  Stream processing},
keywords={Application programs;  Benchmarking;  Multicasting;  Side channel attack;  Software architecture;  Trusted computing, Benchmark applications;  Intel SGX;  Network communications;  Network data;  Stream processing;  Stream processing systems;  Streaming applications;  Trusted execution environments, Data communication systems},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mahapatra201841,
author={Mahapatra, T. and Gerostathopoulos, I. and Fernández Moreno, F.A. and Prehofer, C.},
title={Designing flink pipelines in IoT mashup tools},
journal={CEUR Workshop Proceedings},
year={2018},
volume={2316},
pages={41-53},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061292923&partnerID=40&md5=941409157ee3dd230eb9b502d8c42342},
abstract={Internet of Things (IoT) applications are generating increasingly large amounts of data because of continuous activity and periodical sensing capabilities. Processing the data generated by IoT applications is necessary to derive important insights-for example, processing data from CO emissions can help municipal authorities apply traffic restrictions in order to improve a city's air quality. State-of-the-art stream-processing platforms, such as Apache Flink, can be used to process large amounts of data streams from different IoT devices. However, it is difficult to both set-up and write applications for these platforms; this is also manifested in the increasing need for data analysts and engineers. A promising solution is to enable domain experts, who are not necessarily programmers, to develop the necessary stream pipelines by providing them with domain-specific graphical tools. We present our proposal for a state-of-the-art mashup tool, originally developed for wiring IoT applications together, to graphically design streaming data pipelines and deploy them as a Flink application. Our prototype and experimental evaluation show that our proposal is feasible and potentially impactful. © held by the author(s). NOBIDS 2018},
author_keywords={Flink pipelines;  Graphical tool;  IoT mashup tools;  Stream analytics},
keywords={Air quality;  Big data;  Data handling;  Pipelines, Experimental evaluation;  Graphical tools;  Internet of Things (IOT);  Large amounts of data;  Mashup tools;  Municipal authorities;  Stream analytics;  Traffic restrictions, Internet of things},
publisher={CEUR-WS},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Saroliya2018753,
author={Saroliya, A. and Mishra, U. and Rana, A.},
title={Performance Evaluation and Statistical Analysis of AUR-Chord Algorithm with Default Working of Structured P2P Overlay Network},
journal={Advances in Intelligent Systems and Computing},
year={2018},
volume={583},
pages={753-760},
doi={10.1007/978-981-10-5687-1_67},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036462374&doi=10.1007%2f978-981-10-5687-1_67&partnerID=40&md5=b5cd40992d0a78d10093e3ec1cbc37b8},
abstract={In today’s scenario, where data streaming and file sharing (audio, video, jpeg, etc) are the most performed tasks over the internet, the several characteristic functionalities of peer-to-peer network for resource distribution makes its usage much more than the initially used client–server-based network. Peer-to-peer network requires less time compared to client–server approach for distributing data over the network because at an instance of time any node can behave as client and server in accordance to the requirement, i.e., there exists no predefined behavior/state of a node. The distribution of nodes and their scalable linking increases the churn rate in P2P network which as a result affects various parameters like data availability and data security as the higher churn rate creates an overhead on the data transaction activities. In this paper, statistical analysis and simulation results depict that improved algorithm give a highly efficient response which minimize the incomplete lookup as compared to earlier one. © 2018, Springer Nature Singapore Pte Ltd.},
author_keywords={Chord protocol;  Churn rate;  P2P networks;  Performance evaluation;  Structure overlay network},
keywords={Computation theory;  Distributed computer systems;  Electronic document exchange;  Overlay networks;  Soft computing;  Statistical methods, Analysis and simulation;  Chord protocol;  Churn rates;  Data availability;  Data transaction;  P2P network;  Performance evaluation;  Resource distribution, Peer to peer networks},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jambi2017131,
author={Jambi, S. and Anderson, K.M.},
title={Engineering scalable distributed services for real-time big data analytics},
journal={Proceedings - 3rd IEEE International Conference on Big Data Computing Service and Applications, BigDataService 2017},
year={2017},
pages={131-140},
doi={10.1109/BigDataService.2017.22},
art_number={7944930},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022178102&doi=10.1109%2fBigDataService.2017.22&partnerID=40&md5=bfa3e2aaaad8076687737a54403a1bc8},
abstract={There is high demand for tools that analyze large sets of streaming data in both industrial and academic settings. While existing work has examined a wide range of issues, we focus on query support. In particular, we focus on providing analysts flexibility with respect to the types of queries they can make on large data sets in real time as well as over historical data. We have designed and implemented a lightweight service-based framework-EPIC Real-Time-that manages a set of queries that can be applied to user-initiated data analysis events (such as studying tweets generated during a disaster). Our prototype combines stream processing and batch processing techniques inspired by the Lambda Architecture. We investigate a core set of query types that can answer a wide range of queries asked by analysts who study crisis events. In this paper, we present a prototype implementation of EPIC Real-Time which makes use of event-driven and reactive programming techniques. We also present a performance evaluation on how efficiently the real-time and batch-oriented queries perform, how well these queries meet the needs of our analysts, and provide insight into how EPIC Real-Time performs along a number of dimensions including performance, usability, scalability, and reliability. © 2017 IEEE.},
author_keywords={Crisis informatics;  Lambda architecture;  Query support;  Social media analysis},
keywords={Batch data processing;  Query processing, Crisis informatics;  Distributed service;  Light-weight service;  Processing technique;  Prototype implementations;  Query supports;  Reactive programming;  Social media analysis, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Siow2017245,
author={Siow, E. and Tiropanis, T. and Hall, W.},
title={Ewya: An interoperable fog computing infrastructure with RDF stream processing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10673 LNCS},
pages={245-265},
doi={10.1007/978-3-319-70284-1_20},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033550530&doi=10.1007%2f978-3-319-70284-1_20&partnerID=40&md5=c41451db72c61bdbee48c02d68f6aea3},
abstract={Fog computing is an emerging technology for the Internet of Things (IoT) that aims to support processing on resource-constrained distributed nodes in between the sensors and actuators on the ground and compute clusters in the cloud. Fog Computing benefits from low latency, location awareness, mobility, wide-spread deployment and geographical distribution at the edge of the network. However, there is a need to investigate, optimise for and measure the performance, scalability and interoperability of resource-constrained Fog nodes running real-time applications and queries on streaming IoT data before we can realise these benefits. With Eywa, a novel Fog Computing infrastructure, we (1) formally define and implement a means of distribution and control of query workload with an inverse publish-subscribe and push mechanism, (2) show how data can be integrated and made interoperable through organising data as Linked Data in the Resource Description Format (RDF), (3) test if we can improve RDF Stream Processing query performance and scalability over state-of-the-art engines with our approach to query translation and distribution for a published IoT benchmark on resource-constrained nodes and (4) position Fog Computing within the Internet of the Future. © 2017, Springer International Publishing AG.},
author_keywords={Fog computing;  Internet of Things;  Interoperability;  Linked Data;  Query translation;  Stream processing;  Workload management},
keywords={Benchmarking;  Computer hardware description languages;  Constrained optimization;  Data handling;  Fog;  Geographical distribution;  Interoperability;  Scalability;  Search engines;  Semantic Web, Computing infrastructures;  Internet of thing (IOT);  Linked datum;  Query translations;  Resource constrained nodes;  Resource description formats;  Stream processing;  Workload management, Internet of things},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chakraborty2016,
author={Chakraborty, R. and Majumdar, S.},
title={A priority based resource scheduling technique for multitenant storm clusters},
journal={Proceedings of the 2016 International Symposium on Performance Evaluation of Computer and Telecommunication Systems, SPECTS 2016 - Part of SummerSim 2016 Multiconference},
year={2016},
doi={10.1109/SPECTS.2016.7570513},
art_number={7570513},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991609159&doi=10.1109%2fSPECTS.2016.7570513&partnerID=40&md5=fb77aeed38954a4e4e66f8fa1351b131},
abstract={In this work in progress paper, we present our ongoing effort towards devising a priority based resource scheduling technique and framework for apache storm. Apache Storm is a popular distributed real time stream processing engine which has been widely adopted by key players in the industry including YAHOO and Twitter. An application running in storm is called a topology that is characterized by a Directed Acyclic Graph (DAG). To run multiple of such topologies in a storm cluster, storm provides with default, out of the box scheduler called Isolation Scheduler. Isolation Scheduler assigns resources to topologies based on static resource configuration and does not provide any means to prioritize topologies based on their varying business priority. As a result, performance degradation, even complete starvation of topologies with high business priority is possible when available cluster resources are insufficient. A priority based resource scheduling strategy is proposed in this paper to overcome this problem. A preliminary performance evaluation is performed to demonstrate effectiveness of the proposed scheduler over the default storm Isolation Scheduler. © 2016 The Society for Modeling and Simulation International.},
author_keywords={Apache Storm;  Big Data;  Distributed Computing;  Distributed Stream Processing (DSP);  Event Processing;  Priority Scheduling;  Resource Management},
keywords={Big data;  Directed graphs;  Distributed computer systems;  Distributed parameter control systems;  Information management;  Storms;  Topology, Directed acyclic graph (DAG);  Distributed stream processing;  Event Processing;  Performance degradation;  Priority scheduling;  Real-time streams;  Resource management;  Resource-scheduling, Scheduling},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Coluccio2014,
author={Coluccio, R. and Ghidini, G. and Reale, A. and Levine, D. and Bellavista, P. and Emmons, S.P. and Smith, J.O.},
title={Online stream processing of machine-to-machine communications traffic: A platform comparison},
journal={Proceedings - IEEE Symposium on Computers and Communications},
year={2014},
doi={10.1109/ISCC.2014.6912528},
art_number={6912528},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908177605&doi=10.1109%2fISCC.2014.6912528&partnerID=40&md5=891c45e1e834c37c3c95303c78199ac6},
abstract={In a machine-to-machine (M2M) communications system, the deployed devices relay data from on-board sensors to a back-end application over a wireless network. Since the cellular network provides very good coverage (especially in inhabited areas) and is relatively inexpensive, commercial M2M applications often prefer it to other technologies such as WiFi or satellite links. Unfortunately, having been originally designed with human users in mind, the cellular network provides little support to monitor millions of unattended devices. For this reason, it is extremely important to monitor the underlying signalling traffic to detect misbehaving devices or network problems. In the cellular network used by M2M communications systems, the network elements communicate using the Signalling System #7 (SS7), and a real-life system can generate tens of millions of SS7 messages per hour. This paper reports the results of our practical investigation on the possibility to use distributed stream processing systems (DSPSs) to perform real-time analysis of SS7 traffic in a commercial M2M communications system consisting of hundreds of thousands of devices. Through a thorough experimental evaluation based on the analysis of real-world SS7 traces, we present and compare the implementations of a DSPS-based data analysis application on top of either the well-known Storm DSPS or the Quasit middleware. The results show that, by using DSPS services, we are able to largely meet the real-time processing requirements of our use-case scenario. © 2014 IEEE.},
author_keywords={distributed stream processing;  M2M communications;  Quasit;  SS7;  Storm},
keywords={Automation;  Distributed parameter control systems;  Middleware;  Mobile telecommunication systems;  Petroleum reservoir evaluation;  Real time systems;  Satellite links;  Storms;  Wi-Fi;  Wireless networks, Communications systems;  Distributed stream processing;  Experimental evaluation;  Machine-to-machine (M2M);  Practical investigations;  Quasit;  Realtime processing;  Signalling system #7, Machine-to-machine communication},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kotevski2014410,
author={Kotevski, Z. and Mitrevski, P.},
title={Hybrid fluid modeling approach for performance analysis of P2P live video streaming systems},
journal={Peer-to-Peer Networking and Applications},
year={2014},
volume={7},
number={4},
pages={410-426},
doi={10.1007/s12083-013-0205-7},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905106343&doi=10.1007%2fs12083-013-0205-7&partnerID=40&md5=54700e232b2f6ba794933be4d8674169},
abstract={In this paper a hybrid modeling approach with different modeling formalisms and solution methods is employed in order to analyze the performance of peer to peer live video streaming systems. We conjointly use queuing networks and Fluid Stochastic Petri Nets, developing several performance models to analyze the behavior of rather complex systems. The models account for: network topology, peer churn, scalability, peer average group size, peer upload bandwidth heterogeneity and video buffering, while introducing several features unconsidered in previous performance models, such as: admission control for lower contributing peers, control traffic overhead and internet traffic packet loss. Our analytical and simulation results disclose the optimum number of peers in a neighborhood, the minimum required server upload bandwidth, the optimal buffer size and the influence of control traffic overhead. The analysis reveals the existence of a performance switch-point (i.e. threshold) up to which system scaling is beneficial, whereas performance steeply decreases thereafter. Several degrees of degraded service are introduced to explore performance with arbitrary percentage of lost video frames and provide support for protocols that use scalable video coding techniques. We also find that implementation of admission control does not improve performance and may discourage new peers if waiting times for joining the system increase. © 2013 Springer Science+Business Media New York.},
author_keywords={Discrete-event simulation;  Fluid Stochastic Petri Nets;  Live video streaming;  Modeling;  Peer to Peer;  Performance;  Queuing networks},
keywords={Discrete event simulation;  Distributed computer systems;  Electric network topology;  Models;  Queueing networks;  Stochastic control systems, Fluid Stochastic Petri Nets;  Live video streaming;  Peer to peer;  Performance;  Queuing network, Peer to peer networks},
publisher={Springer New York LLC},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lai2014119,
author={Lai, F. and Hasan, S.S. and Laugesen, A. and Chipara, O.},
title={CSense: A stream-processing toolkit for robust and high-rate mobile sensing applications},
journal={IPSN 2014 - Proceedings of the 13th International Symposium on Information Processing in Sensor Networks (Part of CPS Week)},
year={2014},
pages={119-129},
doi={10.1109/IPSN.2014.6846746},
art_number={6835794},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904625924&doi=10.1109%2fIPSN.2014.6846746&partnerID=40&md5=2d9fd856ec3b42b3e691865955e977bd},
abstract={This paper presents CSense - a stream-processing toolkit for developing robust and high-rate mobile sensing application in Java. CSense addresses the needs of these systems by providing a new programming model that supports flexible application configuration, a high-level concurrency model, memory management, and compiler analyses and optimizations. Our compiler includes a novel flow analysis that optimizes the exchange of data across components from an application-wide perspective. A mobile sensing application benchmark indicates that flow analysis may reduce CPU utilization by as much as 45%. Static analysis is used to detect a range of programming errors including application composition errors, improper use of memory management, and data races. We identify that memory management and concurrency limit the scalability of stream processing systems. We incorporate memory pools, frame conversion optimizations, and custom synchronization primitives to develop a scalable run-time. CSense is evaluated on Galaxy Nexus phones running Android. Empirical results indicate that our run-time achieves 19 times higher steam processing rate compared to a realistic baseline implementation. We demonstrate the versatility of CSense by developing three mobile sensing applications. © 2014 IEEE.},
author_keywords={Dataflow computing;  embedded software;  runtime environment},
keywords={Benchmarking;  Data processing;  Embedded software;  Errors;  Java programming language;  Program compilers;  Sensor networks, Application composition;  Concurrency modeling;  Dataflow;  Flexible applications;  Programming errors;  Runtime environments;  Stream processing systems;  Synchronization primitive, Computer systems programming},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Maamar201281,
author={Maamar, H.R. and Boukerche, A. and Petriu, E.},
title={A multihop supplying partner protocol for 3D streaming systems over thin mobile devices},
journal={DIVANet'12 - Proceedings of the ACM Workshop on Design and Analysis of Intelligent Vehicular Networks and Applications},
year={2012},
pages={81-88},
doi={10.1145/2386958.2386961},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869394582&doi=10.1145%2f2386958.2386961&partnerID=40&md5=960ecd28a5be3f95974a6199accec4c2},
abstract={The recent advances in technology and mobile computing led to the rapid growth of networked 3D streaming applications. Due to the limited network bandwidth of the client server approach, research works are now turning toward mobile ad- hoc networks (MANET)-based streaming. MANET-based streaming is considered challenging due to the limitation of the wireless medium. Moreover, given that the 3D data is distributed among peers, finding a suitable supplying part- ner is not considered an easy task. In this paper, we pro- pose a supplying partner selection technique coupled with a content delivery technique for MANET-based 3D streaming. We refer to our protocol as MULTIPLY. The performance evaluation of MULTIPLY obtained using an extensive set of simulation experiments is then reported. Copyright 2012 ACM.},
author_keywords={3D Streaming;  MANET;  Mobile augmented reality;  Wireless Networks},
keywords={3D data;  3D streaming;  Client server;  Content delivery;  Hoc networks;  MANET;  Mobile augmented reality;  Multihop;  Network bandwidth;  Partner selection;  Performance evaluation;  Rapid growth;  Wireless medium, Augmented reality;  Mobile devices;  Three dimensional computer graphics;  Wireless networks, Mobile ad hoc networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Verstraaten201141,
author={Verstraaten, M. and Grelck, C. and Van Tol, M.W. and Bakker, R. and Jesshope, C.R.},
title={On mapping distributed S-net to the 48-core intel scc processor},
journal={3rd Many-Core Applications Research Community Symposium, MARC 2011},
year={2011},
pages={41-46},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870528148&partnerID=40&md5=bb333f5011e41bdcee3e9539266e52e9},
abstract={Distributed S-NET is a declarative coordination language and component technology primarily aimed at modern multi-core/many-core chip architectures. It builds on the concept of stream processing to structure dynamically evolving networks of communicating asynchronous components. These components themselves are implemented using a conventional language suitable for the application domain. Our goal is to map Distributed S-NET to the Intel SCC processor in order to provide users with a simplified programming environment, yet still allowing them to make use of the advanced features of the SCC architecture. Following a brief introduction to the design principles of S-NET, we sketch out the general ideas of our implementation approach. These mainly concern the use of SCC's message passing buffers for lightweight communication of S-NET records and control data between cores as well as remapping of large data structures through lookup table manipulation. The latter avoids costly memory copy operations that would result from more traditional message passing approaches. Last, but not least, we present prototypical performance measurements for our communication primitives.},
keywords={Asynchronous components;  Chip architecture;  Communication primitives;  Component technologies;  Control data;  Coordination language;  Design Principles;  Evolving networks;  Implementation approach;  Multi core;  Performance measurements;  Programming environment;  Remapping;  Stream processing, Data structures;  Microprocessor chips;  Network architecture, Computer architecture},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kotevski2011215,
author={Kotevski, Z. and Mitrevski, P.},
title={A modeling framework for performance analysis of P2P live video streaming systems},
journal={Communications in Computer and Information Science},
year={2011},
volume={83 CCIS},
pages={215-225},
doi={10.1007/978-3-642-19325-5_22},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952354782&doi=10.1007%2f978-3-642-19325-5_22&partnerID=40&md5=d8888fdd67bc13b0c629088275b17f61},
abstract={Client/server media streaming systems exhibit streaming limitations when number of clients rises and the server can no longer sustain the upload load. At first, IP Multicast was proposed as an alternative solution to this problem but its deployment brought many practical issues in scalability and deployment that prevented it from wider use. Recently, a new promising technique emerged which is cost effective, easy to deploy and can support thousands of simultaneous users. It's a peer to peer network of logically connected clients which form an application level overlay network on top of the physical network. This new paradigm brings numerous advantages, but also a lot of open issues that need to be resolved. This paper exposes the fundamental characteristics of p2p live video streaming systems, gives a survey of p2p video streaming applications and presents a novel modeling framework for performance analysis of such systems as our main goal in future research. © 2011 Springer-Verlag.},
author_keywords={IP video broadcast;  p2p networks;  performance analysis;  Petri nets},
keywords={Application level;  Client/server;  Fundamental characteristics;  IP Multicast;  IP video;  IS costs;  Live video streaming;  Media streaming systems;  Modeling frameworks;  P2P network;  performance analysis;  Physical network;  Practical issues;  Streaming applications, Acoustic streaming;  Graph theory;  Information technology;  Innovation;  Media streaming;  Overlay networks;  Petri nets;  Video streaming;  Videotex, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2009161,
author={Liu, Z. and Tang, A. and Xia, C.H. and Zhang, L.},
title={A decentralized control mechanism for stream processing networks},
journal={Annals of Operations Research},
year={2009},
volume={170},
number={1},
pages={161-182},
doi={10.1007/s10479-008-0434-y},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-65049091479&doi=10.1007%2fs10479-008-0434-y&partnerID=40&md5=1b18cd87ea548531b187af658ddb6b9b},
abstract={Data streaming applications are becoming more and more common due to the rapid development in emerging areas such as sensor networks, multimedia streaming, and on-line data mining, etc. These applications are often running in a decentralized, distributed environment. The requirements for processing large volumes of streaming data at real time have posed many great design challenges. One of the critical issues is to optimize the ongoing resource consumption of multiple, distributed, cooperating processing units. In this paper, we consider a generic model for the general stream data processing systems. We address the resource allocation problem for a collection of processing units so as to maximize the weighted sum of the throughput of different streams. Each processing unit may require multiple input data streams simultaneously and produce one or many valuable output streams. We develop decentralized control mechanisms that maximize the overall system throughput in such data stream processing networks. Performance analysis on the optimality and complexity of these mechanisms are also provided. © 2008 Springer Science+Business Media, LLC.},
author_keywords={Distributed algorithm;  Resource allocation;  Stream processing},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Singh20221585,
author={Singh, P. and Singh, S. and Mishra, P.K. and Garg, R.},
title={A data structure perspective to the RDD-based Apriori algorithm on Spark},
journal={International Journal of Information Technology (Singapore)},
year={2022},
volume={14},
number={3},
pages={1585-1594},
doi={10.1007/s41870-019-00337-3},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091795379&doi=10.1007%2fs41870-019-00337-3&partnerID=40&md5=956b66c53a898d43e928205c926dd223},
abstract={During the recent years, a number of efficient and scalable frequent itemset mining algorithms for big data analytics have been proposed by many researchers. Initially, MapReduce-based frequent itemset mining algorithms on Hadoop cluster were proposed. Although, Hadoop has been developed as a cluster computing system for handling and processing big data, but the performance of Hadoop does not meet the expectation for the iterative algorithms of data mining, due to its high I/O, and writing and then reading intermediate results in the disk. Consequently, Spark has been developed as another cluster computing infrastructure which is much faster than Hadoop due to its in-memory computation. It is highly suitable for iterative algorithms and supports batch, interactive, iterative, and stream processing of data. Many frequent itemset mining algorithms have been re-designed on the Spark, and most of them are Apriori-based. All these Spark-based Apriori algorithms use Hash Tree as the underlying data structure. This paper investigates the efficiency of various data structures for the Spark-based Apriori. Although, the data structure perspective has been investigated previously, but for the MapReduce-based Apriori, and it must be re-investigated in the distributed computing environment of Spark. The considered underlying data structures are Hash Tree, Trie, and Hash Table Trie. The experimental results on the benchmark datasets show that the performance of Spark-based Apriori with Trie and Hash Table Trie are almost similar but both perform many times better than Hash Tree in the distributed computing environment of Spark. © 2019, Bharati Vidyapeeth's Institute of Computer Applications and Management.},
author_keywords={Apriori;  Big data analytics;  Frequent itemset mining;  Parallel and distributed algorithms;  RDD;  Spark},
publisher={Springer Science and Business Media B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Farrokh2022,
author={Farrokh, M. and Hadian, H. and Sharifi, M. and Jafari, A.},
title={SP-ant: An ant colony optimization based operator scheduler for high performance distributed stream processing on heterogeneous clusters},
journal={Expert Systems with Applications},
year={2022},
volume={191},
doi={10.1016/j.eswa.2021.116322},
art_number={116322},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120673518&doi=10.1016%2fj.eswa.2021.116322&partnerID=40&md5=7fe78e68ce85cd80ebc6fb57c3cbe4d9},
abstract={A key feature of distributed stream processing (DSP) systems is the scheduling of operators on clustered computers. In scheduling, the assignment plan of operators to nodes of the cluster, requirements of operators, and the computational power of each worker node must be considered with the goal of finding a tradeoff between the communication latency of operators and the utilization of worker nodes to minimize the overall system response time. To reach this goal is quite challenging, especially in heterogeneous clusters, because there are no accurate estimations about the loads of worker nodes at run time. To address this challenge, we propose a novel stream processing scheduling using ant colony algorithm (SP-Ant). SP-Ant finds the best operator assignment plan considering the inter-node communication latencies of operators by initially collocating highly communicative operators on the same worker nodes using the bin-packing algorithm and iteratively (re-)scheduling only the less communicative operators using the exploration and exploitation phases of the evolutionary ant colony optimization (ACO) algorithm in order to reduce its convergence time. SP-Ant is implemented on the standard Apache Storm. Using several standard benchmark topologies of Storm, it is shown that SP-Ant outperforms the R-Storm and Storm default schedulers by at least 50% in reducing the response time. © 2021 Elsevier Ltd},
author_keywords={Ant colony optimization;  Apache Storm;  Distributed data stream processing;  Heterogeneous clusters;  Scheduling},
keywords={Ant colony optimization;  Artificial intelligence;  Distributed computer systems;  Distributed parameter control systems;  Evolutionary algorithms;  Iterative methods;  Response time (computer systems);  Storms, Apache storm;  Communication latency;  Computational power;  Distributed data stream processing;  Distributed stream processing;  Heterogeneous clusters;  Key feature;  Performance;  Stream processing systems;  Worker nodes, Scheduling},
publisher={Elsevier Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2021671,
author={Zhang, Z. and Liu, Z. and Jiang, Q. and Chen, J. and An, H.},
title={RDMA-Based Apache Storm for High-Performance Stream Data Processing},
journal={International Journal of Parallel Programming},
year={2021},
volume={49},
number={5},
pages={671-684},
doi={10.1007/s10766-021-00696-0},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102993968&doi=10.1007%2fs10766-021-00696-0&partnerID=40&md5=cbaf131f98468accb14fc51ddabd4338},
abstract={Apache Storm is a scalable fault-tolerant distributed real time stream-processing framework widely used in big data applications. For distributed data-sensitive applications, low-latency, high-throughput communication modules have a critical impact on overall system performance. Apache Storm currently uses Netty as its communication component, an asynchronous server/client framework based on TCP/IP protocol stack. The TCP/IP protocol stack has inherent performance flaws due to frequent memory copying and context switching. The Netty component not only limits the performance of the Storm but also increases the CPU load in the IPoIB (IP over InfiniBand) communication mode. In this paper, we introduce two new implementations for Apache Storm communication components with the help of RDMA technology. The performance evaluation on Mellanox QDR Cards (40 Gbps) shows that our implementations can achieve speedup up to 5× compared with IPoIB and 10× with Gigabit Ethernet. Our implementations also significantly reduce the CPU load and increase the throughput of the system. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Apache Storm;  Cloud computing;  Communication optimization;  InfiniBand;  RDMA;  Stream-processing framework},
keywords={Storms;  Transmission control protocol, Big data applications;  Communication components;  Communication mode;  Communication modules;  Context switching;  Real-time streams;  Sensitive application;  Stream data processing, Data streams},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Marić2021148,
author={Marić, J. and Pripužić, K. and Antonić, M.},
title={DEBS grand challenge: Real-time detection of air quality improvement with Apache Flink},
journal={DEBS 2021 - Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
year={2021},
pages={148-153},
doi={10.1145/3465480.3466930},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110393274&doi=10.1145%2f3465480.3466930&partnerID=40&md5=2c435a309bf8cdada1119e163c827f8c},
abstract={The topic of the DEBS Grand Challenge 2021 is to develop a solution for detecting areas in which the air quality index (AQI) improved the most when compared to the previous year. The solution must run two given continuous queries in parallel on the incoming sensor data stream which must return the following: 1) a top 50 cities in terms of AQI improvement with their current AQIs and 2) a histogram of the longest streaks of good AQI. The incoming data is accessed through an API which provides streaming sensor measurements in batches. We present our solution based on Apache Flink, a distributed stream processing framework for the cluster. We opted for Flink since its applications can easily be scaled horizontally and vertically by adding computation nodes or increasing available resources, respectively. Flink allows us to divide the given queries into smaller tasks which can be run concurrently on different nodes in order to reduce the overall processing time and thus improve the performance of our solution. In more detail, the following performance intensive tasks are run in parallel on distributed nodes: 1) retrieving measurement batches, 2) assigning a city to each measurement and 3) calculating air quality index per city. We also discuss the main optimizations we have used to improve the performance and present an experimental evaluation of our solution. © 2021 ACM.},
author_keywords={big data;  fast data;  geospatial data streams;  sensor streams},
keywords={Data streams;  Distributed parameter control systems;  Signal detection;  Software architecture, Air quality improvement;  Air quality indices;  Continuous queries;  Debs grand challenges;  Distributed stream processing;  Experimental evaluation;  Real-time detection;  Sensor measurements, Air quality},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mostafaei2021820,
author={Mostafaei, H. and Afridi, S. and Abawajy, J.H.},
title={SNR: Network-aware geo-distributed stream analytics},
journal={Proceedings - 21st IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGrid 2021},
year={2021},
pages={820-827},
doi={10.1109/CCGrid51090.2021.00100},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114855909&doi=10.1109%2fCCGrid51090.2021.00100&partnerID=40&md5=8a1d365bbc6615da251bbe87b54dda62},
abstract={Emerging applications such as those running on the Internet of Things (IoT) devices produce constant data streams that need to be processed in real-time. Distributed stream processing systems (DSPs), with geographically distributed cluster networks interconnected via wide area network (WAN) links, have recently gained interest in handling these applications. How-ever, these applications have stringent requirements such as low-latency and high bandwidth that must be guaranteed to ensure the quality of service (QoS). These application requirements raise fundamental DSPs resource management and scheduling challenge. In this paper, we formulate the problem of placement of worker nodes on a geo-distributed DSPs cluster network as a multi-criteria decision-making problem and propose an additive weighting-based approach to solve it. The proposed solution finds the trade-off among different network parameters and allows executing the tasks according to the desired performance metrics. We evaluated the proposed approach using the Yahoo! streaming benchmark on a testbed and compare it against mechanisms deployed in Apache Spark, Apache Storm, and Apache Flink. The results of the evaluation show that our approach improves the performance of Spark up to 2.2x-7.2x, Storm up to 1.2x-3.4x, and Flink up to 1.4x-3.3x compared to other approaches, which makes our approach useful for use in practical environments. © 2021 IEEE.},
author_keywords={Geo-distributed analytics;  IoT;  Simple Additive Weighting;  Stream processing;  worker node placement},
keywords={Cluster computing;  Data streams;  Decision making;  Distributed parameter control systems;  Economic and social effects;  Internet of things;  Quality of service;  Scheduling;  Storms, Application requirements;  Distributed clusters;  Distributed stream processing;  Emerging applications;  Internet of thing (IOT);  Multi-criteria decision making problems;  Resource management and scheduling;  Stringent requirement, Wide area networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ndubuaku2019164,
author={Ndubuaku, M. and Anjum, A. and Liotta, A.},
title={Cloud-assisted adaptive stream processing from discriminative representations},
journal={Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
year={2019},
volume={2019-October},
pages={164-169},
doi={10.1109/SMC.2019.8914227},
art_number={8914227},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076775872&doi=10.1109%2fSMC.2019.8914227&partnerID=40&md5=ca4890eef835d4c73bf449bbafc5aac2},
abstract={As the streaming data generated by Internet of Things (IoT) ubiquitous sensors grow in massive scale, extracting interesting information (anomalies) in real-time becomes more challenging. Traditional systems which retrospectively perform all the processing in the cloud do not capture real-time changes in the data. Similarly, real-time solutions which rely on human monitors have the tendency to miss the anomalies due to their rare nature. In recent times, several machine learning techniques have been proposed for stream processing. Approaches based on supervised or semi-supervised learning fail to adapt to changing patterns of the streaming data and the data labelling costs are huge. To address these limitations, we propose a cloud-assisted framework where an intermediary node (edge) is introduced between the end devices and the cloud to assist in stream processing. A model deployed on the edge is designed to learn in an iterative manner to discriminate between similar and dissimilar data representations, making it easier to distinguish the anomalies. In this work, we have proposed an iterative method that combines the capabilities of deep clustering and l2-normalisation to achieve better discriminative representations. Experimental results demonstrate the proposed method achieves robust performance over state-of-the-art discriminative representation algorithms and sets new benchmark accuracy on transformation invariant image dataset. © 2019 IEEE.},
keywords={Benchmarking;  Internet of things;  Iterative methods;  Learning algorithms;  Machine learning;  Supervised learning, Data representations;  Interesting information;  Internet of Things (IOT);  Machine learning techniques;  Real time solution;  Semi- supervised learning;  Traditional systems;  Transformation invariants, Real time systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2019,
author={Wang, Y. and Tari, Z. and Huang, X. and Zomaya, A.Y.},
title={A network-aware and partition-based resource management scheme for data stream processing},
journal={ACM International Conference Proceeding Series},
year={2019},
doi={10.1145/3337821.3337870},
art_number={a20},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071082896&doi=10.1145%2f3337821.3337870&partnerID=40&md5=8babeca934b8af8eba8b043145656b96},
abstract={With the increasing demand for data-driven decision making, there is an urgent need for processing geographically distributed data streams in real-time. The existing scheduling and resource management schemes efficiently optimize stream processing performance with the awareness of resource, quality-of-service, and network traffic. However, the correlation between network delay and inter-operator communication pattern is not well-understood. In this study, we propose a network-aware and partition-based resource management scheme to deal with the ever-changing network condition and data communication in stream processing. The proposed approach applies operator fusion by considering the computational demand of individual operators and the inter-operator communication patterns. It maps the fused operators to the clustered hosts with the weighted shortest processing time heuristic. Meanwhile, we established a 3-dimensional coordinate system for prompt reflection of the network condition, real-time traffic, and resource availability. We evaluated the proposed approach against two benchmarks, and the results demonstrate the efficiency in throughput and resource utilization. We also conducted a case study and implemented a prototype system supported by the proposed approach that aims to utilize the stream processing paradigm for pedestrian behavior analysis. The prototype application estimates walking time for a given path according to the real crowd traffic. The promising evaluation results of processing performance further illustrate the efficiency of the proposed approach. © 2019 ACM.},
author_keywords={Apache Storm;  Data Stream Processing;  Pedestrian Behavior},
keywords={Data streams;  Decision making;  Efficiency;  Natural resources management;  Quality of service;  Resource allocation;  Scheduling, Computational demands;  Data stream processing;  Distributed data streams;  Pedestrian behavior;  Processing performance;  Resource management schemes;  Resource utilizations;  Weighted shortest processing time, Information management},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schelter201961,
author={Schelter, S. and Celebi, U. and Dunning, T.},
title={Efficient incremental cooccurrence analysis for item-based collaborative filtering},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={61-72},
doi={10.1145/3335783.3335784},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071259588&doi=10.1145%2f3335783.3335784&partnerID=40&md5=19f2ebcbee747c4006b1804946bd75f2},
abstract={Recommender systems are ubiquitous in the modern internet, where they help users find items they might like. A widely deployed recommendation approach is item-based collaborative filtering. This approach relies on analyzing large item cooccurrence matrices that denote how many users interacted with a pair of items. The potentially quadratic number of items to compare poses a scalability bottleneck in analyzing such item cooccurrences. Additionally, this problem intensifies in real world use cases with incrementally growing datasets, especially when the recommendation model is regularly recomputed from scratch. We highlight the connection between the growing cost of item-based recommendation and densification processes in common interaction datasets. Based on our findings, we propose an efficient incremental algorithm for item-based collaborative filtering based on cooccurrence analysis. This approach restricts the number of interactions to consider from ‘power users’ and ‘ubiquitous items’ to guarantee a provably constant amount of work per user-item interaction to process. We discuss efficient implementations of our algorithm on a single machine as well as on a distributed stream processing engine, and present an extensive experimental evaluation. Our results confirm the asymptotic benefits of the incremental approach. Furthermore, we find that our implementation is an order of magnitude faster than existing open source recommender libraries on many datasets, and at the same time scales to high dimensional datasets which these existing recommenders fail to process. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
keywords={Distributed parameter control systems;  Management information systems;  Petroleum reservoir evaluation;  Recommender systems, Co-occurrence analysis;  Densification process;  Distributed stream processing;  Efficient implementation;  Experimental evaluation;  High dimensional datasets;  Incremental algorithm;  Item-based collaborative filtering, Collaborative filtering},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Minallah2019387,
author={Minallah, N. and Shah, S.S.H. and Said, N. and Khan, W. and Nayab, A. and Shinwari, Z.},
title={Performance Comparison of Chunk/Peer Scheduling Algorithms of Peer-to-Peer Streaming Systems},
journal={Proceedings - 2nd International Conference on Multimedia Information Processing and Retrieval, MIPR 2019},
year={2019},
pages={387-388},
doi={10.1109/MIPR.2019.00077},
art_number={8695326},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065608831&doi=10.1109%2fMIPR.2019.00077&partnerID=40&md5=d7f56fbab1b332923d283a7c40f9ee36},
abstract={In a P2P system, the media stream is divided into small data units known as chunks. Each peer in a P2P system has to take two important decisions at a given time i.e. how the chunks are to be shared? (chunk scheduling) and with which peer are they to be shared with (peer scheduling). Scheduling plays an important role in evaluating performance of P2P systems. This paper compares the performance of different combinations of chunk/peer schedulers in terms of chunk diffusion delay, average chunk distribution delay and max chunk distribution delay. The results obtained under the specified experimental setup show that when Deadline Based chunk scheduling(DLc) is combined with different peer scheduling algorithms, the best results are obtained by its combination with Chunk Earliest Free Peer Scheduler (CEFp). For a constant peer scheduler CEFp combined with different chunk schedulers, the best results are obtained by combining it with Lastest Blind Chunk Scheduler (LBc). Finally, with varying neighborhood size, the best results are obtained by the combination of DLC/CAFp. © 2019 IEEE.},
author_keywords={Chunk Scheduling;  Chunkisations;  Peer Scheduling;  Peer to Peer Streaming Systems},
keywords={Distributed computer systems;  Scheduling;  Scheduling algorithms, Chunkisations;  Diffusion delays;  Neighborhood size;  P2P system;  Peer-to-peer streaming;  Performance comparison;  Small data, Peer to peer networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Weibbach201965,
author={Weibbach, M.},
title={Live traffic data analysis using stream processing},
journal={Proceedings - 11th IEEE/ACM International Conference on Utility and Cloud Computing Companion, UCC Companion 2018},
year={2019},
pages={65-70},
doi={10.1109/UCC-Companion.2018.00036},
art_number={8605759},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061820844&doi=10.1109%2fUCC-Companion.2018.00036&partnerID=40&md5=1046fd306bfafbc4db09fd508d5120de},
abstract={The increasing digitalization in the traffic infrastructure offers a great potential to optimize traffic flows, to save costs, and to improve the CO2 balance. Achieving this requires the use of scalable, high-performance software environments that process live traffic data with minimal latency. However, there are no standard solutions that work out of the box. Instead, technology stacks of complex components must be assembled, configured, and deployed from a large and heterogeneous set of available building blocks. Since there are no guidelines on which particular software to use in which configuration for specific use cases, it is extremely difficult to build such complex architectures from scratch. Nevertheless, many application areas, including traffic data analysis, have domain-specific requirements, which makes it possible to close these gaps on the basis of further research. Following this idea, we analyze how typical applications of traffic data analysis can be implemented using stream processing technologies in order to find reusable solutions that can be used as blueprints for the design of applications with similar requirements. Therefore, a number of typical use cases will be analyzed, implemented and benchmarked on the basis of various stream processing architectures. This way, specific levers are to be found to systematically increase the performance. Our first results show significant performance differences between different software solutions and architectures. © 2018 IEEE.},
author_keywords={Benchmarking;  Big data;  Live traffic analysis;  Mobile crowdsensing;  Stream processing;  Streaming analytics},
keywords={Benchmarking;  Big data;  Cloud computing;  Computer software reusability;  Data handling;  Information analysis, Complex architectures;  Mobile crowdsensing;  Software environments;  Stream processing;  Traffic analysis;  Traffic data analysis;  Traffic infrastructure;  Typical application, Computer architecture},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rodrigo2019264,
author={Rodrigo, A. and Dayarathna, M. and Jayasena, S.},
title={Privacy preserving elastic stream processing with clouds using homomorphic encryption},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11447 LNCS},
pages={264-280},
doi={10.1007/978-3-030-18579-4_16},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065482060&doi=10.1007%2f978-3-030-18579-4_16&partnerID=40&md5=e939f562f90d8be542b66a260e7089e6},
abstract={Prevalence of the Infrastructure as a Service (IaaS) clouds has enabled organizations to elastically scale their stream processing applications to public clouds. However, current approaches for elastic stream processing do not consider the potential security vulnerabilities in cloud environments. In this paper we describe the design and implementation of an Elastic Switching Mechanism for data stream processing which is based on Homomorphic Encryption (HomoESM). The HomoESM not only does elastically scale data stream processing applications into public clouds but also preserves the privacy of such applications. Using a real world test setup, which includes an email filter benchmark and a web server access log processor benchmark (EDGAR) we demonstrate the effectiveness of our approach. Multiple experiments on Amazon EC2 indicate that the proposed approach for Homomorphic encryption provides significant results which is 10% to 17% improvement of average latency in the case of email filter benchmark and EDGAR benchmarks respectively. Furthermore, EDGAR add/subtract operations and comparison operations showed 6.13% and 26.17% average latency improvements respectively. These promising results pave the way for real world deployments of privacy preserving elastic stream processing in the cloud. © Springer Nature Switzerland AG 2019.},
author_keywords={Cloud computing;  Compressed event processing;  Data compression;  Elastic data stream processing;  IaaS;  System sizing and capacity planning},
keywords={Cloud computing;  Cryptography;  Data compression;  Data handling;  Data mining;  Database systems;  Electronic mail, Capacity planning;  Data stream processing;  Design and implementations;  Event Processing;  Ho-momorphic encryptions;  IaaS;  Real world deployment;  Security vulnerabilities, Infrastructure as a service (IaaS)},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li20182193,
author={Li, L.-N. and Wei, X.-H. and Li, X. and Wang, X.-W.},
title={Burstiness-Aware Elastic Resource Allocation in Stream Data Processing [流数据处理中负载突发感知的弹性资源分配]},
journal={Jisuanji Xuebao/Chinese Journal of Computers},
year={2018},
volume={41},
number={10},
pages={2193-2208},
doi={10.11897/SP.J.1016.2018.02193},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055449062&doi=10.11897%2fSP.J.1016.2018.02193&partnerID=40&md5=9fc70cbdecb42164b4e0b42a7b72d8c7},
abstract={In distributed parallel data stream processing, facing the real-time-changing and bursting stream data, fixed resource allocation will cause waste of resources or reduce the quality of service. To achieve desirable performances without resource wastes, the scalable and elastic resource allocation is a critical problem to be solved, which allows applications to change dynamically their resource configuration in response to data load fluctuations at run-time. However, most elastic resource allocation policies only adjust their resources when the performance of nodes does not match the data load. The adjustment delay caused by the immediate resource allocation, and the unpredictable data load reduce the performance of the elastic allocation policy. In some work, the data load prediction is introduced into the elastic resource allocation, but in the data prediction model, the future data is predicted based on the historical data, which is not applicable to the bursty stream data. Moreover, the unnecessary resource adjustment bump increases the system overhead. This paper focuses on the adjustment delay and the adjustment jump in the elastic resource allocation. For the above problems, the main challenge lies in the prediction of burst load and the cooperation among nodes. Therefore, this paper firstly constructs a data load correlation model to predict the data load of nodes accurately. This model considers the correlation between nodes of the stream application, that is the node will experience the sudden load change, once its upstream nodes have carried out resource adjustments, then uses the queuing theory to predict the data load of the node in the next window, according to the states of its neighborhood upstream nodes in the current window, which includes the data load, the buffer occupy, the processing ability and the consumption ratio. Furthermore, a bi-directional control mechanism is designed to support the cooperative resource allocation between the upstream and downstream nodes, in which the feed forward control transmits the information of upstream nodes to downstream nodes, and the feedback control transmits the information inversely and selects the appropriate upstream nodes to participate in the resource allocation of the downstream nodes. Based on the data load correlation model and the bi-directional control mechanism, this paper proposes an upstream-downstream cooperative and elastic resource allocation strategy, optimizing for data quality and resource utilization rate quality of service (QoS) requirements and considering the adjustment cost. In this strategy, the downstream node can timely detect the sudden load and load variation trend produced by the upstream nodes, and thus the resource adjustment will be done in advance, and the adjustment bump can also be avoided. At the same time, according to the feedback mechanism, the upstream nodes can dynamically adjust the data processing rate to suppress the load fluctuation of the downstream node, aimed at reducing the possibility of resource adjustment. Experimental results show that, the strategy is effective, compared with two benchmark strategies used in similar scenarios. When the load change is large, the strategy can reduce the average data loss by 85%, and significantly reduce the overhead of system resource adjustment, and improves the resource utilization. © 2018, Science Press. All right reserved.},
author_keywords={Burstiness-aware;  Elastic adjustment;  Resource allocation;  Stream data;  Stream data processing},
keywords={Feedback control;  Forecasting;  Quality control;  Quality of service;  Queueing theory;  Resource allocation, Burstiness;  Elastic adjustment;  Qualityof-service requirement (QoS);  Resource allocation policy;  Resource allocation strategies;  Resource configurations;  Stream data;  Stream data processing, Data reduction},
publisher={Science Press},
language={Chinese},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sajjad201833,
author={Sajjad, H.P. and Liu, Y. and Vlassov, V.},
title={Optimizing windowed aggregation over geo-distributed data streams},
journal={Proceedings - 2018 IEEE International Conference on Edge Computing, EDGE 2018 - Part of the 2018 IEEE World Congress on Services},
year={2018},
pages={33-41},
doi={10.1109/EDGE.2018.00012},
art_number={8473374},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055625599&doi=10.1109%2fEDGE.2018.00012&partnerID=40&md5=b4968d8e427a0f33f505cb1fc356ab55},
abstract={Real-time data analytics is essential since more and more applications require online decision making in a timely manner. However, efficient analysis of geo-distributed data streams is challenging. This is because data needs to be collected from all edge data centers, which aggregate data from local sources, in order to process most of the analytic tasks. Thus, most of the time edge data centers need to transfer data to a central data center over a wide area network, which is expensive. In this paper, we advocate for a coordinated approach of edge data centers in order to handle these analytic tasks efficiently and hence, reducing the communication cost among data centers. We focus on the windowed aggregation of data streams, which has been widely used in stream analytics. In general, aggregation of data streams among edge data centers in the same region reduces the amount of data that needs to be sent over cross-region communication links. Based on state-of-the-art research, we leverage intra-region links and design a low-overhead coordination algorithm that optimizes communication cost for data aggregation. Our algorithm has been evaluated using synthetic and Big Data Benchmark datasets. The evaluation results show that our algorithm reduces the bandwidth cost up to ~6x, as compared to the state-of-the-art solution. © 2018 IEEE.},
author_keywords={Aggregation;  Data analytics;  Geo-distributed;  Stream processing;  WAN analytics},
keywords={Agglomeration;  Cost reduction;  Decision making;  Edge computing;  Wide area networks, Communication cost;  Coordination algorithms;  Data analytics;  Distributed data streams;  Efficient analysis;  Geo-distributed;  On-line decision makings;  Stream processing, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Muñoz2018,
author={Muñoz, J.F. and Dolz, M.F. and Del Rio Astorga, D. and Cepeda, J.P. and Daniel García, J.},
title={Supporting MPI-distributed stream parallel patterns in GrPPI},
journal={ACM International Conference Proceeding Series},
year={2018},
doi={10.1145/3236367.3236380},
art_number={a17},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055425096&doi=10.1145%2f3236367.3236380&partnerID=40&md5=7ab0795ff3f42a66cc261631c1464fae},
abstract={In the recent years, the large volumes of stream data and the near real-time requirements of data streaming applications have exacerbated the need for new scalable algorithms and programming interfaces for distributed and shared-memory platforms. To contribute in this direction, this paper presents a new distributed MPI back end for GrPPI, a C++ high-level generic interface of data-intensive and stream processing parallel patterns. This back end, as a new execution policy, supports the distributed and hybrid (distributed and shared-memory) parallel execution of the Pipeline and Farm patterns, where the hybrid mode combines the MPI policy with a GrPPI shared-memory one. A detailed analysis of the GrPPI MPI execution policy reports considerable benefits from the programma-bility, flexibility and readability points of view. The experimental evaluation on a streaming application with different distributed and shared-memory scenarios reports considerable performance gains with respect to the sequential versions at the expense of negligible GrPPI overheads. © 2018 Association for Computing Machinery.},
author_keywords={C++ Programming;  Distributed Patterns;  Generic Programming;  Parallel Patterns;  Stream Processing},
keywords={Memory architecture, C++ programming;  Distributed patterns;  Generic programming;  Parallel patterns;  Stream processing, C++ (programming language)},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tran2018278,
author={Tran, G.P.C. and Walters, J.P. and Crago, S.P.},
title={Reducing tail latencies while improving resiliency to timing errors for stream processing workloads},
journal={Proceedings - 2018 IEEE International Conference on Services Computing, SCC 2018 - Part of the 2018 IEEE World Congress on Services},
year={2018},
pages={278-281},
doi={10.1109/SCC.2018.00048},
art_number={8456434},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054017794&doi=10.1109%2fSCC.2018.00048&partnerID=40&md5=a47b159aab57336f5de3459451f4ef74},
abstract={Stream processing is an increasingly popular model for online data processing that can be partitioned into streams of elements. It is commonly used in data analytics services, such as processing Twitter tweets. Current stream processing frameworks boast high throughput and low average latency. However, lower tail latencies and better real-Time performance are desirable to stream processing users. In practice, there are issues that can affect the performance of these applications and cause unacceptable violations of real-Time constraints. Some examples of these issues are garbage collection pauses and resource contention. In this paper, we propose applying redundancy in the data processing pipeline to increase the resiliency of stream processing applications to timing errors. This results in better real-Time performance and a reduction in tail latency. We present a methodology and apply this redundancy in a framework based on Twitter's Heron. Then, we then evaluate the effectiveness of this technique against a range of injected timing errors using benchmarks from Intel's Storm Benchmark. Furthermore, we also study the potential effects of duplication when applied at different stages in the topology. Finally, we evaluate the additional overhead that duplicating tuples brings to a stream processing topology. Our results show that redundant tuple processing can effectively reduce the tail latency by up to 63% and that the number of missed deadlines can also be reduced by up to 94% in the best case. Overall we conclude that redundancy through duplicated tuples is indeed a powerful tool for increasing the resiliency to intermittent runtime timing errors. © 2018 IEEE.},
author_keywords={Fault-Tolerance;  Garbage collection;  Real-Time;  Resiliency;  stream processing;  Tail latency},
keywords={Data handling;  Errors;  Fault tolerance;  Interactive computer systems;  Redundancy;  Refuse collection;  Social networking (online);  Timing circuits;  Topology, Garbage collection;  Real time;  Resiliency;  Stream processing;  Tail latency, Pipeline processing systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2018215,
author={Wang, Y. and Tari, Z. and Farahabady, M.R.H. and Zomaya, A.Y.},
title={Model-Based Scheduling for Stream Processing Systems},
journal={Proceedings - 2017 IEEE 19th Intl Conference on High Performance Computing and Communications, HPCC 2017, 2017 IEEE 15th Intl Conference on Smart City, SmartCity 2017 and 2017 IEEE 3rd Intl Conference on Data Science and Systems, DSS 2017},
year={2018},
volume={2018-January},
pages={215-222},
doi={10.1109/HPCC-SmartCity-DSS.2017.28},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047474588&doi=10.1109%2fHPCC-SmartCity-DSS.2017.28&partnerID=40&md5=ae2993463c48b6ed0e8976623c3e88c0},
abstract={Stream processing is emerging to react to the changing business situations of real-time processing. The main aim of this paradigm is to deal with the huge volume of data in the format of information flows originating from distributed devices. This consequently poses challenges to the scheduling problem in cloud data centers regarding the time-varying velocity of data ingesting and processing. In response to the uncertainties and complexities of streaming data, we propose a model-based scheduling scheme for stream processing systems, capturing the system behavior and providing an optimal allocation strategy to adapt to the changing work conditions. The proposed scheduling policy is implemented in Apache Storm, and micro-benchmarks with various shapes (e.g line, star, and diamond) were used in the evaluation. A topology that tracks trending topics on Twitter is also used, where the input is feeding with tweets in real-time. Experimental results show that the proposed solution can perform estimations that are well aligned with the system performance. The proposed scheduling policy achieves an improved performance with regards throughput and latency under varying ingesting rates. © 2017 IEEE.},
author_keywords={Apache Storm;  Resource Allocation/Scheduling;  Streaming Data Processing},
keywords={Data handling;  Scheduling;  Smart city;  Storms, Apache storms;  Business situations;  Distributed devices;  Realtime processing;  Scheduling policies;  Stream processing systems;  Streaming data processing;  Time-varying velocity, Data communication systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Grulich20181030,
author={Grulich, P.M. and Zukunft, O.},
title={Smart stream-based car information systems that scale: an experimental evaluation},
journal={Proceedings - 2017 IEEE International Conference on Internet of Things, IEEE Green Computing and Communications, IEEE Cyber, Physical and Social Computing, IEEE Smart Data, iThings-GreenCom-CPSCom-SmartData 2017},
year={2018},
volume={2018-January},
pages={1030-1037},
doi={10.1109/iThings-GreenCom-CPSCom-SmartData.2017.181},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047430242&doi=10.1109%2fiThings-GreenCom-CPSCom-SmartData.2017.181&partnerID=40&md5=e9c6c43ecf11654576a04bcb45396a72},
abstract={Real-time information from embedded sources is hard to process if the number of sources is high. One typical example of this application area are car information systems where the cyber-physical system of a car connects to arbitrary sources in and outside of the car to deliver value-adding information to the driver of the car. In this paper, we propose a new architecture for such a car information system based on a smart data streaming infrastructure. This architecture has been implemented and we have run several experiments to examine the quality of our proposed solution. The current implementation is based on Spark Streaming, Couchbase and written in Scala. We have deployed our implementation on a distributed system using cloud services. This allows us to perform experiments with a high load as typical for the application scenario. First results from our experiments show that our proposed solution for a smart data based car information system is resilient to typical failures and scales for the described use-cases. © 2017 IEEE.},
keywords={Embedded systems;  Information systems;  Information use;  Internet of things, Application area;  Application scenario;  Distributed systems;  Experimental evaluation;  Number of sources;  Real-time information;  Spark streaming;  Typical failures, Green computing},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yan2018624,
author={Yan, H. and Sun, D. and Gao, S. and Zhou, Z.},
title={Performance Analysis of Storm in a Real-World Big Data Stream Computing Environment},
journal={Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
year={2018},
volume={252},
pages={624-634},
doi={10.1007/978-3-030-00916-8_57},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054828597&doi=10.1007%2f978-3-030-00916-8_57&partnerID=40&md5=f7d6eb0e9d7b2d49e7b47e32b78f26ea},
abstract={As an important distributed real-time computation system, Storm has been widely used in a number of applications such as online machine learning, continuous computation, distributed RPC, and more. Storm is designed to process massive data streams in real time. However, there have been few studies conducted to evaluate the performance characteristics clusters in Storm. In this paper, we analyze the performance of a Storm cluster mainly from two aspects, hardware configuration and parallelism setting. Key factors that affect the throughput and latency of the Storm cluster are identified, and the performance of Storm’s fault-tolerant mechanism is evaluated, which help users use the computation system more efficiently. © 2018, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.},
author_keywords={Big data;  Big data computing;  Performance analysis;  Storm;  Stream computing},
keywords={Distributed computer systems;  Learning systems;  Storms, Fault-tolerant mechanism;  Hardware configurations;  Massive data streams;  On-line machine learning;  Performance analysis;  Performance characteristics;  Real-time computations;  Stream computing, Big data},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tahara2018451,
author={Tahara, A. and Hayashida, Y. and Thu, T.T. and Shibata, Y. and Oguri, K.},
title={Power performance analysis of FPGA-based particle filtering for realtime object tracking},
journal={Advances in Intelligent Systems and Computing},
year={2018},
volume={611},
pages={451-462},
doi={10.1007/978-3-319-61566-0_41},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026328570&doi=10.1007%2f978-3-319-61566-0_41&partnerID=40&md5=33ab1a740d40c46bade3d64de8cc6fae},
abstract={Real-time image processing with a compact FPGA-based architecture plays a key role in dynamic state-space models. This paper presents an energy efficient FPGA acceleration architecture of a particle filter, which is based on stream processing structure with a parallel resampling algorithm. Particle filters solve the state estimation problems with three steps: prediction, likelihood calculation and resampling. By accomplishing the resampling in a valid pixel area of an input image frame, while executing prediction in a synchronization region, our approach achieves real-time object tracking. This paper mainly highlights implementation alternatives using different clock frequencies and resource usages of FPGA. The result shows the comparisons of power consumption for the compact architecture with an accelerated clock frequency (135 MHz) compared to the larger circuit size with clock frequency (27 MHz). Interestingly, the larger architecture with a slower clock frequency shows lower power consumption. © Springer International Publishing AG 2018.},
keywords={Clocks;  Computer architecture;  Distributed computer systems;  Electric power utilization;  Energy efficiency;  Image processing;  Monte Carlo methods;  Signal filtering and prediction;  Target tracking, Compact architecture;  Estimation problem;  FPGA-based architectures;  Lower-power consumption;  Real-time image processing;  Real-time object tracking;  Resampling algorithms;  State - space models, Field programmable gate arrays (FPGA)},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xie2017366,
author={Xie, B. and Sun, G. and Ma, G.},
title={Docker based overlay network performance evaluation in large scale streaming system},
journal={Proceedings of 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference, IMCEC 2016},
year={2017},
pages={366-369},
doi={10.1109/IMCEC.2016.7867235},
art_number={7867235},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016759928&doi=10.1109%2fIMCEC.2016.7867235&partnerID=40&md5=2b43e9e4a54d015860a283fd9760fed9},
abstract={Docker, the most popular container technique today, has accepted by more and more people and companies. It achieves agility and controls for Development and IT Operations teams to build, ship, and run any app, anywhere. Taking Docker's advantages, enterprises are able to leverage big data analytics more efficiently. In this paper, we use two network modes in Docker when building one Spark based streaming application. To get better achievement we design one experimental system to investigate performance between host and overlay network mode from different aspects. Through our working, we find host mode is faster overlay mode around 8%. At the same time, the overlay mode is more stable. We introduce coefficient of variation to see the difference of transfer latency among every package and overlay mode is better than host mode more than 50%. Therefore, overlay network mode is more suitable for the business with high quality requirement of network jitter like video conference. On the other hand, when need to get higher transfer latency, host mode is better choice. © 2016 IEEE.},
author_keywords={Big data;  Docker;  Overlay network},
keywords={Information management;  Overlay networks, Coefficient of variation;  Data analytics;  Docker;  Experimental system;  Large-scale streaming;  Network performance evaluation;  Streaming applications;  Transfer latency, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Roodaki2017,
author={Roodaki, H. and Shirmohammadi, S.},
title={Scalable multiview video coding for immersive video streaming systems},
journal={VCIP 2016 - 30th Anniversary of Visual Communication and Image Processing},
year={2017},
doi={10.1109/VCIP.2016.7805454},
art_number={7805454},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011105135&doi=10.1109%2fVCIP.2016.7805454&partnerID=40&md5=74d369e6a6179a478baf46c6a126e8a2},
abstract={Immersive video places the user inside the video scene, allowing the user to control the direction of the view. To achieve this, the view of every direction must be recorded using either a panoramic camera or multiple cameras placed at different positions with different angels. The size of the captured video can be quite large due to multiple video streams, one from each camera. Even with compression standards such as Multiview Video Coding (MVC), the transmission of the whole MVC video is still bandwidth-costly, especially for heterogeneous users whose bandwidths vary. In this paper, we present a new approach for immersive video streaming by using Scalable Multiview Video Coding (SMVC) to create multiple layers of the immersive video, supporting heterogeneous receivers more efficiently. Our method limits the number of views in its base layer, while it uses view scalability and free view-point scalability in the additional layers to synthesize more views at the receiver and provide high quality free view-point viewing to the user. Performance evaluations demonstrate that our method: 1-synthesizes missing views more accurately, as evident subjectively, and 2-achieves an average and maximum gain of 0.75 and 1.4 in Bjontegaard BD-Bitrate scale, respectively, compared to existing work which simply group adjacent views in the same layer. © 2016 IEEE.},
author_keywords={Free view-point scalability;  immersive video;  scalable multiview video coding;  view scalability},
keywords={Bandwidth;  Bandwidth compression;  Cameras;  Codes (symbols);  Communication channels (information theory);  Image coding;  Image processing;  Scalability;  Video signal processing;  Video streaming;  Visual communication, Compression standards;  Heterogeneous users;  Immersive;  Multiple cameras;  Multiple video streams;  Multiview video coding;  Number of views;  Panoramic cameras, Scalable video coding},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ma2017397,
author={Ma, K. and Yu, Z. and Ji, K. and Yang, B.},
title={Stream-based live probabilistic topic computing and matching},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10393 LNCS},
pages={397-406},
doi={10.1007/978-3-319-65482-9_27},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028450409&doi=10.1007%2f978-3-319-65482-9_27&partnerID=40&md5=a0a51657614a9c07c4d0492f50935afd},
abstract={Public opinion monitoring refers to real-time first story detection (FSD) on a particular Internet news event. It play an important part in finding news propagation tendency. Current opinion monitoring methods are related to text matching. However, it has some limitations such as latent and hidden topic discovery and incorrect relevance ranking of matching results on large-scale data. In this paper, we propose one improved solution to live public opinion monitoring: stream-based live probabilistic topic computing and matching. Our method attempts to address the disadvantages such as semantic matching and low efficiency on timely big data. Topic real-time computing with stream processing paradigm and topic matching with query-time document and field boosting are proposed to make substantial improvements. Finally, our experimental evaluation on topic computing and matching using crawled historical Netease news records shows the high effectiveness and efficiency of the proposed approach. © Springer International Publishing AG 2017.},
author_keywords={Mapreduce;  Probabilistic topic model;  Public opinion;  Public sentiment;  Stream computing;  Stream processing;  Topic computing;  Topic matching},
keywords={Big data;  Data mining;  Efficiency;  Parallel architectures;  Semantics;  Social aspects, Map-reduce;  Probabilistic topic models;  Public opinions;  Public sentiments;  Stream computing;  Stream processing;  Topic computing;  Topic matching, Computer architecture},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Karavadara2016,
author={Karavadara, N. and Zolda, M. and Nga Nguyen, V.T. and Knoop, J. and Kirner, R.},
title={Dynamic power management for reactive stream processing on the SCC tiled architecture},
journal={Eurasip Journal on Embedded Systems},
year={2016},
volume={2016},
number={1},
doi={10.1186/s13639-016-0035-9},
art_number={14},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976402647&doi=10.1186%2fs13639-016-0035-9&partnerID=40&md5=aec305a9b869aef17a1c3ebb934286e9},
abstract={Dynamic voltage and frequency scaling (DVFS) is a means to adjust the computing capacity and power consumption of computing systems to the application demands. DVFS is generally useful to provide a compromise between computing demands and power consumption, especially in the areas of resource-constrained computing systems. Many modern processors support some form of DVFS. In this article, we focus on the development of an execution framework that provides lightweight DVFS support for reactive stream processing systems (RSPS). RSPs are a common form of embedded control systems, operating in direct response to inputs from their environment. At the execution framework, we focus on support for many-core scheduling for parallel execution of concurrent programs. We provide a DVFS strategy for RSPs that is simple and lightweight, to be used for dynamic adaptation of the power consumption at runtime. The simplicity of the DVFS strategy became possible by the sole focus on the application domain of RSPs. The presented DVFS strategy does not require specific assumptions about the message arrival rate or the underlying scheduling method. While DVFS is a very active field, in contrast to most existing research, our approach works also for platforms like many-core processors, where the power settings typically cannot be controlled individually for each computational unit. We also support dynamic scheduling with variable workload. While many research results are provided with simulators, in our approach, we present a parallel execution framework with experiments conducted on real hardware, using the single-chip cloud computer many-core processor. The results of our experimental evaluation confirm that our simple DVFS strategy provides potential for significant energy saving on RSPs. © 2016, Karavadara et al.},
author_keywords={DVFS;  Many-core processor;  Power management;  Reactive systems;  Stream-processing;  Tiled architectures},
keywords={Computer hardware;  Concurrency control;  Dynamic frequency scaling;  Electric power utilization;  Embedded systems;  Energy conservation;  Energy management;  Memory architecture;  Parallel processing systems;  Power management;  Scheduling;  Voltage scaling, DVFS;  Many-core processors;  Reactive system;  Stream processing;  Tiled architecture, Computer architecture},
publisher={Springer International Publishing},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Algemili2016226,
author={Algemili, U.},
title={Investigation of Reconfigurable FPGA Design for Processing Big Data Streams},
journal={Proceedings - 2nd IEEE International Conference on Big Data Security on Cloud, IEEE BigDataSecurity 2016, 2nd IEEE International Conference on High Performance and Smart Computing, IEEE HPSC 2016 and IEEE International Conference on Intelligent Data and Security, IEEE IDS 2016},
year={2016},
pages={226-233},
doi={10.1109/BigDataSecurity-HPSC-IDS.2016.75},
art_number={7502294},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979730190&doi=10.1109%2fBigDataSecurity-HPSC-IDS.2016.75&partnerID=40&md5=e4d6df9a4bf9c5f8731d43644b9ce484},
abstract={Big Data situation has placed a tremendous pressure on the existing computational models. The challenges of Big Data call for a new approach to solve both software and hardware problems. Streaming applications is a form of on-demand software distribution. In streaming scenarios, only essential portions of an application's code need to be installed on the system, while the receiver performs the main operations. The necessary code and files are delivered over the network as, and when, they are required. The hardware architecture plays an important role in improving the efficiency of a streaming system. The variance of hardware performance on different HW architectures is quite interesting. Previous work confirms that the CPUs, GPUs, and FPGAs are performing differently on specific applications. The previous efforts of hardware benchmarking show that GPUs outperformed the other platforms in terms of execution time. CPUs outperformed in overall execution combined with transfer time. FPGAs outperformed for fixed algorithms using streaming [1]. Hence, this paper evaluates the performance of streaming applications on a pipelined FPGA design. In the context of real-time processing, it elects one of the Big Data streaming problems that gets a candidate for majority element on-the-fly, that is Moore's Voting Algorithm. The performance analysis of Moore's algorithm on FPGA highlights a noticeable improvement by using a pipelining architecture. © 2016 IEEE.},
author_keywords={Big Data Processing;  FPGA Design;  On-the-fly processing;  Pipeline Design;  Reconfigurable Computing;  Streaming Processing},
keywords={Application programs;  Computer architecture;  Computer hardware;  Data handling;  Design;  Field programmable gate arrays (FPGA);  Hardware;  Integrated circuit design;  Network architecture;  Pipeline processing systems;  Program processors;  Reconfigurable architectures;  Reconfigurable hardware;  Security of data, FPGA design;  On the flies;  Pipeline design;  Reconfigurable computing;  Streaming processing, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Martin2016322,
author={Martin, A. and Brito, A. and Fetzer, C.},
title={Grand challenge: Real-time social network graph analysis using stream mine 3G},
journal={DEBS 2016 - Proceedings of the 10th ACM International Conference on Distributed and Event-Based Systems},
year={2016},
pages={322-329},
doi={10.1145/2933267.2933514},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978698318&doi=10.1145%2f2933267.2933514&partnerID=40&md5=88fef5f14eab595c256282266b2ae18c},
abstract={In this paper, we present our approach for solving the DEBS Grand Challenge 2016 using StreamMine3G, a distributed, highly scalable, elastic and fault tolerant event stream processing (ESP) system. We first provide an overview about StreamMine3G with regards to its programming model and architecture, followed by thorough description of the implementation for the two queries that provide up-to-date information about (i) the top-3 active posts and (ii) the top-k comments with the largest maximum cliques. Novel aspects of our implementation include (i) highly optimized data structures that lower the amount of lookups and traversals, and a (ii) deterministic data partitioning and processing scheme that allows the system to scale without bounds in an elastic fashion while still guaranteeing semantic transparency. In order to better utilize nowadays many-core machines, we furthermore propose a pipelining scheme in addition to data partitioning. Finally, we present a brief performance evaluation of our system. © 2016 ACM.},
author_keywords={Cep;  Complex event processing;  Esp;  Event stream processing;  Fault tolerance;  Migration;  Scalability;  State management},
keywords={Complex networks;  Fault tolerance;  Scalability;  Semantics;  Social networking (online);  Software architecture, Complex event processing;  Debs grand challenges;  Deterministic data;  Event stream processing;  Migration;  Programming models;  Semantic transparencies;  State management, Data handling},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Vaidya2016813,
author={Vaidya, P.S. and Lee, J.J. and Pai, V.S. and Lee, M. and Hur, S.},
title={Symbiote coprocessor unit-A streaming coprocessor for data stream acceleration},
journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
year={2016},
volume={24},
number={3},
pages={813-826},
doi={10.1109/TVLSI.2015.2432063},
art_number={2432063},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945976149&doi=10.1109%2fTVLSI.2015.2432063&partnerID=40&md5=258bba559822b0eaa4b78c2c1e4218c8},
abstract={This paper describes the design and the architecture of symbiote coprocessor unit (SCU)-a programmable streaming coprocessor for a heterogeneous reconfigurable logic-assisted data stream management systems (DSMSs) such as symbiote. The SCU is intended for streaming applications with real-time event and data processing that have stricter deadlines, high-bandwidth, and high-accuracy requirements. To meet these requirements, the SCU exploits unique characteristics of DSMSs, such as single-pass tuple processing, windowed operators, and inherent data level parallelism, using a single-instruction multiple-data very large instruction word (SIMD-VLIW) microarchitecture and a novel inverted distributed register file. In order to better explain the instruction set, design, and functionality of the various units in the SCU, this paper also provides a brief overview of SymQL-a procedural query language that we developed to describe the queries that can be executed on the SCU. Finally, this paper presents the performance of SCU using four queries that represent common data stream processing use-cases, one of them being similar to a query found in the Linear Road Benchmark. Using these queries on SCU simulation, we show that the SCU outperforms a software-only DSMS running on an AMD Opteron 2350 quad-core processor by 1.5-42 times. © 2015 IEEE.},
author_keywords={Coprocessor accelerators;  Data stream management systems (DSMSs);  Heterogeneous computing;  System architectures},
keywords={Benchmarking;  Computer software;  Data communication systems;  Data handling;  Information management;  Microprocessor chips;  Program processors;  Query languages;  Reconfigurable architectures, Data stream management systems;  Data-level parallelism;  Instruction set;  Micro architectures;  Reconfigurable logic;  Single instruction multiple data;  Streaming applications;  Very large instruction words, Very long instruction word architecture},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rokicki2016817,
author={Rokicki, M. and Zerr, S. and Siersdorfer, S.},
title={Just in time: Controlling temporal performance in crowdsourcing competitions},
journal={25th International World Wide Web Conference, WWW 2016},
year={2016},
pages={817-827},
doi={10.1145/2872427.2883075},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040310311&doi=10.1145%2f2872427.2883075&partnerID=40&md5=4fa08edc9ff9ebbe94b2b98a5b046406},
abstract={Many modern data analytics applications in areas such as crisis management, stock trading, and healthcare, rely on components capable of nearly real-Time processing of stream-ing data produced at varying rates. In addition to automatic processing methods, many tasks involved in those applica-tions require further human assessment and analysis. How-ever, current crowdsourcing platforms and systems do not support stream processing with variable loads. In this pa-per, we investigate how incentive mechanisms in competi-tion based crowdsourcing can be employed in such scenar-ios. More specifically, we explore techniques for stimulating workers to dynamically adapt to both anticipated and sud-den changes in data volume and processing demand, and we analyze effects such as data processing throughput, peak-to-Average ratios, and saturation effects. To this end, we study a wide range of incentive schemes and utility func-tions inspired by real world applications. Our large-scale experimental evaluation with more than 900 participants and more than 6200 hours of work spent by crowd work-ers demonstrates that our competition based mechanisms are capable of adjusting the throughput of online workers and lead to substantial on-demand performance boosts.},
author_keywords={Competitions;  Crowdsourcing;  Stream processing},
keywords={Competition;  Crowdsourcing;  World Wide Web, Automatic processing;  Crowdsourcing platforms;  Experimental evaluation;  Incentive mechanism;  Peak to average ratios;  Realtime processing;  Stream processing;  Temporal performance, Data handling},
publisher={International World Wide Web Conferences Steering Committee},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Smirnov2016323,
author={Smirnov, P.A. and Nasonov, D.},
title={Quality-based Workload Scaling for Real-time Streaming Systems},
journal={Procedia Computer Science},
year={2016},
volume={101},
pages={323-332},
doi={10.1016/j.procs.2016.11.038},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008260092&doi=10.1016%2fj.procs.2016.11.038&partnerID=40&md5=698707ad217626d96d1130cd2e04e23e},
abstract={In this paper we propose an idea to scale workload via elastic quality of solution provided by the particular streaming applications. The contribution of this paper consists of quality-based workload scaling model, implementation details for quality assessment mechanism implemented at the top of Apache Storm and experimental evaluation of the proposed model on a synthetic and real-world (medical) examples. © 2016 The Authors. Published by Elsevier B.V.},
author_keywords={apache storm;  big data;  data streaming;  elastic workload;  quality of service;  scaling model},
keywords={Big data;  Quality of service;  Storms, Data streaming;  Elastic quality;  elastic workload;  Experimental evaluation;  Quality assessment;  Real time streaming;  Scaling model;  Streaming applications, Real time systems},
publisher={Elsevier B.V.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen20152371,
author={Chen, W. and Guan, Y.},
title={Distinct element counting in distributed dynamic data streams},
journal={Proceedings - IEEE INFOCOM},
year={2015},
volume={26},
pages={2371-2379},
doi={10.1109/INFOCOM.2015.7218625},
art_number={7218625},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954207942&doi=10.1109%2fINFOCOM.2015.7218625&partnerID=40&md5=15cbba589cfe94cbb36eaaca1a140899},
abstract={We consider a new type of distinct element counting problem in dynamic data streams, where (1) insertions and deletions of an element can appear not only in the same data stream but also in two or more different streams, (2) a deletion of a distinct element cancels out all the previous insertions of this element, and (3) a distinct element can be re-inserted after it has been deleted. Our goal is to count the number of distinct elements that were inserted but have not been deleted in a continuous data stream. We also solve this new type of distinct element counting problem in a distributed setting. This problem is motivated by several network monitoring and attack detection applications where network traffic can be modelled as single or distributed dynamic streams and the number of distinct elements in the data streams, such as unsuccessful TCP connection setup requests, is calculated to be used as an indicator to detect certain network events such as service outage and DDoS attacks. Although there are known tight bounds for distinct element counting in insertion-only data streams, no good bounds are known for it in dynamic data streams, neither for this new type of problem. None of the existing solutions for distinct element counting can solve our problem. In this paper, we will present the first solution to this problem, using a space-bounded data structure with a computation-efficient probabilistic data streaming algorithm to estimate the number of distinct elements in single or distributed dynamic data streams. We have done both theoretical analysis and experimental evaluations, using synthetic and real data traces, of our algorithm to show its effectiveness. © 2015 IEEE.},
keywords={Computational efficiency;  Computer networks;  Data communication systems;  Denial-of-service attack;  Internet protocols, Counting problems;  Distinct elements;  Distributed dynamics;  Experimental evaluation;  Insertions and deletions;  Network Monitoring;  Probabilistic data;  Synthetic and real data, Problem solving},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yang2015146,
author={Yang, D. and Cao, J. and Wu, S. and Wang, J.},
title={Progressive online aggregation in a distributed stream system},
journal={Journal of Systems and Software},
year={2015},
volume={102},
pages={146-157},
doi={10.1016/j.jss.2014.11.027},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923262959&doi=10.1016%2fj.jss.2014.11.027&partnerID=40&md5=1b786579330da1520f13652b6f94597c},
abstract={Interactive query processing aims at generating approximate results with minimum response time. However, it is quite difficult for a batch-oriented processing system to progressively provide cumulatively accurate results in the context of a distributed environment. MapReduce Online extends the MapReduce framework to support online aggregation, but it is hindered by its processing speed in keeping up with ongoing real-time data events. We deploy the online aggregation algorithm over S4, a scalable stream processing system that is inspired by the combined functionalities of MapReduce and Actor model. Our system applies an asynchronous message communication mechanism from actor model to support online aggregation. It can process large scale data stream with high concurrency in a short response time. In this system, we adopt a distributed weighted random sampling algorithm to solve biased distribution between different streams. Furthermore, a multi-level query processing topology is developed to reduce overlapped processing for multiple queries. Our system can provide continuous window aggregation with a confidence interval and error bound. We have implemented our system and conducted plentiful experiments over the TPC-H benchmark. A large number of experiments are carried out to demonstrate that by using our system, high-quality query results can be generated within a short response time and that the approach outperforms MapReduce Online on data streams. © 2014 Elsevier Inc. All rights reserved.},
author_keywords={Actor model;  Online aggregation;  Stream processing},
keywords={Data streams;  Query processing;  Response time (computer systems), Actor models;  Batch-oriented processing;  Distributed environments;  Interactive query processing;  Message communication;  Online aggregations;  Stream processing;  Stream processing systems, Search engines},
publisher={Elsevier Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yan201462,
author={Yan, J. and Tian, C. and Sun, J. and Mao, H.},
title={Improve distributed client lifecycle control in ShadowStream},
journal={International Journal of Web Services Research},
year={2014},
volume={11},
number={4},
pages={62-78},
doi={10.4018/IJWSR.2014100105},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934299335&doi=10.4018%2fIJWSR.2014100105&partnerID=40&md5=e3118df29d382ecd1ff66412f586aaae},
abstract={ShadowStream is a novel Internet live streaming system that integrates performance evaluation as an intrinsic capability. An essential component in ShadowStream is distributed lifecycle control mechanism, which assigns each client a virtual arrival/lifetime to create a particular scenario to evaluate the performance of streaming system. The original design focuses on utilizing stable streaming viewers in physical world to guarantee the accuracy of ShadowStream, which, on the other hand, significantly limits the scale of the experiment. The authors' research develops a novel distributed client lifecycle control to get rid of restrictions caused by the limited number of stable viewers in live-testing streaming networks. The core idea of their research is to match the desired experimental scenario with real viewers' behavior in physical world. The result demonstrates that with the authors' methodology, the scale of experiments can be doubled. Copyright © 2014, IGI Global.},
author_keywords={Lifetime;  Live streaming;  PCE;  ShadowStream;  User behavior},
keywords={Life cycle;  Video streaming, Lifetime;  Live streaming;  PCE;  ShadowStream;  User behaviors, Behavioral research},
publisher={IGI Global},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sui20141889,
author={Sui, Z. and Harvey, N. and Pallickara, S.},
title={On the distributed orchestration of stochastic discrete event simulations},
journal={Concurrency and Computation: Practice and Experience},
year={2014},
volume={26},
number={11},
pages={1889-1907},
doi={10.1002/cpe.3121},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904511235&doi=10.1002%2fcpe.3121&partnerID=40&md5=b507aa9a5138150e34f88af38a3f0ad0},
abstract={Discrete event simulations are a powerful technique for modeling stochastic systems with multiple components where interactions between these components are governed by the probability distribution functions associated with them. Complex discrete event simulations are often computationally intensive with long completion times. This paper describes our solution to the problem of orchestrating the execution of a stochastic, discrete event simulation where computational hot spots evolve spatially over time. Our performance benchmarks report on our ability to balance computational loads in these settings. Copyright © 2013 John Wiley & Sons, Ltd. Copyright © 2013 John Wiley & Sons, Ltd.},
author_keywords={discrete event simulations;  distributed systems;  epidemiological simulations;  stream processing systems},
keywords={Benchmarking;  Distributed database systems;  Distribution functions;  Stochastic systems, Completion time;  Computational loads;  Distributed systems;  Epidemiological simulations;  Hot spot;  Multiple components;  Stream processing systems, Discrete event simulation},
publisher={John Wiley and Sons Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Saleh2014700,
author={Saleh, O. and Sattler, K.-U.},
title={On efficient processing of linked stream data},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2014},
volume={8841},
pages={700-717},
doi={10.1007/978-3-662-45563-0_43},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909991967&doi=10.1007%2f978-3-662-45563-0_43&partnerID=40&md5=0aaaa8d7805d07aae5a557922fd3b1e9},
abstract={Today, many application areas require continuous processing of data streams in an efficient manner and real-time fashion. Processing these continuous flows of data, integrating dynamic data with other data sources, and providing the required semantics lead to real challenges. Thus, Linked Stream Data (LSD) has been proposed which combines two concepts: Linked Open Data and Data Stream Processing (DSP). Recently, several LSD engines have been developed, including C-SPARQL and CQELS, which are based on SPARQL extensions for continuous query processing. However, this SPARQL-centric view makes it difficult to express complex processing pipelines. In this paper, we propose a LSD engine based on a more general stream processing approach. Instead of a variant of SPARQL, our engine provides a dataflow specification language called Pipe Flow which is compiled into native code. Pipe Flow supports native stream processing operators (e.g., window, aggregates, and joins), complex event processing as well as RDF data transformation operators such as tuplifier and triplifier to efficiently support LSD queries and provide a higher degree of expressiveness. We discuss the main concepts addressing the challenges of LSD processing and describe the usage of these concepts for processing queries from LSBench and SRBench. We show the effectiveness of our system in terms of query execution times through a comparison with existing systems as well as through a detailed performance analysis of our system implementation. © Springer-Verlag Berlin Heidelberg 2014.},
keywords={Data handling;  Engines;  Metadata;  Open Data;  Pipe flow;  Semantic Web;  Semantics;  Specification languages, Complex event processing;  Complex processing;  Continuous processing;  Continuous query processing;  Data stream processing;  Dataflow specifications;  Performance analysis;  System implementation, Pipeline processing systems},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{McCurdy2014115,
author={McCurdy, C. and Marin, G. and Vetter, J.S.},
title={Characterizing the impact of prefetching on scientific application performance},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2014},
volume={8551},
pages={115-135},
doi={10.1007/978-3-319-10214-6_6},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908680680&doi=10.1007%2f978-3-319-10214-6_6&partnerID=40&md5=d19b9ccc8140a1ecec0c968548c6b300},
abstract={In order to better understand the impact of hardware and software data prefetching on scientific application performance, this paper introduces two analysis techniques, one micro-architecture-centric and the other application-centric.We use these techniques to analyze representative full-scale production applications from five important Exascale target areas. We find that despite a great diversity in prefetching effectiveness across and even within applications, there is a strong correlation between regions where prefetching is most needed, due to high levels of memory traffic, and where it is most effective. We also observe that the application-centric analysis can explain many of the differences in prefetching effectiveness observed across the studied applications. © Springer International Publishing Switzerland 2014.},
author_keywords={Data streaming;  Performance evaluation;  Prefetching},
keywords={Application programs;  Computer architecture, Analysis techniques;  Application-centric;  Data streaming;  Full-scale production;  Hardware and software;  Performance evaluations;  Prefetching;  Scientific applications, Benchmarking},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Carbone2013153,
author={Carbone, P. and Vandikas, K. and Zaloshnja, F.},
title={Towards highly available complex event processing deployments in the cloud},
journal={International Conference on Next Generation Mobile Applications, Services, and Technologies},
year={2013},
pages={153-158},
doi={10.1109/NGMAST.2013.35},
art_number={6658116},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892755705&doi=10.1109%2fNGMAST.2013.35&partnerID=40&md5=0400c84f5d17af078c368769947cb7fc},
abstract={Recent advances in distributed computing have made it possible to achieve high availability on traditional systems and thus serve them as reliable services. For several offline computational applications, such as fine grained batch processing, their parallel nature in addition to weak consistency requirements allowed a more trivial transition. On the other hand, on-line processing systems such as Complex Event Processing (CEP) still maintain a monolithic architecture, being able to offer high expressiveness and vertical scalability at the expense of low distribution. Despite attempts to design dedicated distributed CEP systems there is potential for existing systems to benefit from a sustainable cloud deployment. In this work we address the main challenges of providing such a CEP service with a focus on reliability, since it is the most crucial aspect of that transition. Our approach targets low average detection latency and sustain-ability by leveraging event delegation mechanisms present on existing stream execution platforms. It also introduces redundancy and transactional logging to provide improved fault tolerance and partial recovery. Our performance analysis illustrates the benefits of our approach and shows acceptable performance costs for on-line CEP exhibited by the fault tolerance mechanisms we introduced. © 2013 IEEE.},
author_keywords={CDR;  Complex event processing;  Distributed stream processing;  Fault tolerance;  Fraud detection;  SIP},
keywords={CDR;  Complex event processing;  Distributed stream processing;  Fraud detection;  SIP, Batch data processing;  Distributed computer systems;  Distributed parameter control systems;  Fault tolerance, Mobile computing},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Papadaki2013100,
author={Papadaki, C. and Kalogeraki, V.},
title={Mobistream: Live multimedia streaming in mobile devices},
journal={MMEDIA 2013 - 5th International Conferences on Advances in Multimedia},
year={2013},
pages={100-108},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905841763&partnerID=40&md5=b0edff7ec5ee5a1a2dd17e976c4e0474},
abstract={In recent years, many techniques have been proposed so as to enable resource-constrained devices to consume or deliver live multimedia streams. The majority of the existing techniques use distributed multimedia services and powerful servers to handle streams on behalf of clients. This is due to the fact that, multimedia streaming, when smartphones act as both clients and servers, can generate many challenges due to the heterogeneity of the multimedia streaming protocols, the media formats and codecs supported by today's smartphones. In addition, multimedia processing is resource consuming and, in many cases, unsuitable for a plethora of resource-constrained devices. To overcome these challenges, we present MobiStream a device-to-device multimedia streaming system for resource-constrained devices that achieves efficient handling of live multimedia streams. The design of MobiStream architecture provides solutions to several issues including resource constraints, streaming among heterogeneous operating systems and platforms, generation, synchronization and presentation of multimedia streams. We have developed the MobiStream prototype system on Java 2 SE and Android platforms; this paper presents the implementation details and the experimental evaluation of our system.},
author_keywords={Android platform;  Live multimedia streaming;  Resource-constrained devices;  Streaming protocol},
keywords={Media streaming;  Mobile devices;  Multimedia services;  Multimedia systems;  Smartphones, Android platforms;  Distributed multimedia services;  Experimental evaluation;  Multimedia processing;  Multimedia streaming;  Multimedia streaming systems;  Resourceconstrained devices;  STreaming protocols, Android (operating system)},
publisher={International Academy, Research and Industry Association, IARIA},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stapenhurst201364,
author={Stapenhurst, R. and Lu, J. and Agrafiotis, D.},
title={Performance evaluation of objective video quality metrics on mixed spatiotemporal resolution content},
journal={2013 IEEE International Conference on Image Processing, ICIP 2013 - Proceedings},
year={2013},
pages={64-68},
doi={10.1109/ICIP.2013.6738014},
art_number={6738014},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897681490&doi=10.1109%2fICIP.2013.6738014&partnerID=40&md5=8c142ed63c1b0f4568a57f363f81e847},
abstract={In this paper we present a study of objective video quality metric performance on test sequences which have undergone spatiotemporal resolution scaling prior to and after compression. The bit rates and resolutions tested have been chosen to typify consumer 'home theatre' wireless streaming scenarios. The aim of the study is to identify a suitable video quality metric for use in a video streaming system that meets changing bandwidth constraints through adaptation of spatiotemporal resolution in addition to quantisation parameters. We first collect subjective quality scores and then assess correlation of objective metrics with this data. Of the metrics tested, we conclude that MOVIE is the most capable of accurate subjective score prediction under changing spatiotemporal resolution conditions. © 2013 IEEE.},
author_keywords={objective quality metric;  spatiotemporal resolution scaling;  video quality assessment},
keywords={Image coding, Bandwidth constraint;  Objective metrics;  Objective qualities;  Objective video quality metrics;  Spatio-temporal resolution;  Subjective quality;  Video quality assessment;  Video quality metric, Image quality},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Alias2012391,
author={Alias, M.S. and Karuppiah, E.K. and Kit, C.P. and Tahir, S.M.},
title={Real-time multiple video streams processing on PC-based FPGA platform},
journal={Proceedings of 2nd International Conference on Computer Science and Network Technology, ICCSNT 2012},
year={2012},
pages={391-395},
doi={10.1109/ICCSNT.2012.6525962},
art_number={6525962},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880241345&doi=10.1109%2fICCSNT.2012.6525962&partnerID=40&md5=0fa023d76546234263400fc7c9eb9b37},
abstract={Programmable embedded media server based video surveillance system is prevalent as it has the capability to accommodate large network of cameras. The implementation of a wide range of programmable security analytics application in PC-based platform demands high processing bandwidth and high-end hardware specification requirements. This is due to insufficient compute resource by multicore CPU in performing some of the computationally intensive analytics operations at acceptable data rates. This paper presents a platform design considerations for programmable video analytics solving compute intensive video data processing leading to cost-efficient embedded platform solution. Our experimental analysis covers PC-based processing pipeline analysis for CPU resources to be used as design input for processing data offloading between multicore CPU and FPGA. The result identifies the processing bottleneck in PC-based media server with video analytics processing component consumes 81% of the total CPU processing cycles. This leads to the exploration of the PCI Express capability in FPGA for system solution as a transfer medium between these computing units. © 2012 IEEE.},
author_keywords={FPGA;  PC-based;  PCI Express;  Video Analytics},
keywords={Experimental analysis;  Hardware specifications;  Multiple video streams;  PC-based;  PCI Express;  Processing bandwidths;  Video analytics;  Video surveillance systems, Computer peripheral equipment;  Computer science;  Data handling;  Field programmable gate arrays (FPGA);  Security systems;  Video streaming, Pipeline processing systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sutinen2011700,
author={Sutinen, T. and Rivas, H.},
title={Multi-interface extension to a scalable video streaming architecture},
journal={Journal of Communications},
year={2011},
volume={6},
number={9},
pages={700-710},
doi={10.4304/jcm.6.9.700-710},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84255170007&doi=10.4304%2fjcm.6.9.700-710&partnerID=40&md5=5d8503fb6ae4716d001c45d7368071ee},
abstract={Video service adaptation capabilities are essential for the efficient utilization of the network resources in heterogeneous multi-access environments and ensuring sufficient perceived service quality for the end users. Multihoming is seen as one important enabler for high-quality and reliable video streaming services and it is expected to be supported extensively in the future Internet. Already today, terminal devices are equipped with multiple network interfaces and able to use them simultaneously. In the future, mobile multimedia services such as video streaming can maximize the user experienced quality by utilizing the available network resources, concurrently. However, intelligent decision-making as well as dynamic mapping of traffic to network interfaces are required for optimal operation. In this paper, we introduce an overall architecture for adaptive video streaming exploiting the novel scalable video coding technology. We also propose a multi-interface streaming extension to the architecture to allow dynamic and concurrent usage of the available access networks in the video service delivery. We present a prototype implementation of the proposed multi-interface video streaming system and test its operation in the presence of network impairments in an experimental evaluation. © 2011 ACADEMY PUBLISHER.},
author_keywords={Adaptation;  Heterogeneous networks;  Multi-homing;  SVC;  Video QoS},
keywords={Access network;  Adaptation;  Adaptive video streaming;  Dynamic mapping;  End users;  Experimental evaluation;  Future internet;  High quality;  Intelligent decision-making;  Mobile multimedia services;  Multi-homing;  Multiaccess;  Multiple networks;  Network impairments;  Network resource;  Optimal operation;  Prototype implementations;  Scalable video coding;  Scalable video streaming;  Service Quality;  SVC;  Terminal devices;  Video QoS;  Video services;  Video streaming services, Heterogeneous networks;  Image coding;  Multimedia services;  Multimedia systems;  Quality of service;  Video streaming;  Videotex, Network architecture},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sutinen2011,
author={Sutinen, T. and Rivas, H.},
title={Cross-layer assisted network interface selection for multi-interface video streaming},
journal={Proceedings - International Conference on Computer Communications and Networks, ICCCN},
year={2011},
doi={10.1109/ICCCN.2011.6005823},
art_number={6005823},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053013000&doi=10.1109%2fICCCN.2011.6005823&partnerID=40&md5=539e128263530bc948e9a2ecf39f1785},
abstract={Multi-homing is expected to be an important enabler of the future Internet. Already today, terminal devices are equipped with multiple network interfaces and able to use them simultaneously. In the future, mobile multimedia services such as video streaming can maximize the user experienced quality by utilizing the available network resources, concurrently. However, intelligent decision-making as well as dynamic mapping of traffic to network interfaces are required for optimal operation. In this paper, we propose a dynamic cross-layer communication assisted network interface selection solution for a multi-interface streaming system designed for scalable video streams. We also present a prototype implementation and verify its operation in an experimental evaluation. © 2011 IEEE.},
author_keywords={Heterogeneous networks;  multi-homing;  SVC;  video QoS},
keywords={Cross-layer;  Dynamic mapping;  Experimental evaluation;  Future internet;  Intelligent decision-making;  Mobile multimedia services;  Multi-homing;  Multiple networks;  Network interface;  Network resource;  Optimal operation;  Prototype implementations;  Scalable video;  SVC;  Terminal devices;  Video QoS, Heterogeneous networks;  Interfaces (computer);  Multimedia services;  Multimedia systems;  Systems analysis;  Telecommunication networks;  Video streaming;  Videotex, Network layers},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Evans2010,
author={Evans, N.S. and GauthierDickey, C. and Grothoff, C. and Grothoff, K. and Keene, J. and Rutherford, M.J.},
title={Simplifying parallel and distributed simulation with the DUP system},
journal={Spring Simulation Multiconference 2010, SpringSim'10},
year={2010},
doi={10.1145/1878537.1878698},
art_number={154},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650603722&doi=10.1145%2f1878537.1878698&partnerID=40&md5=47933bae7fd0771ac4a99ebf49e3d931},
abstract={This paper presents how the DUP System, a straightforward POSIX-compatible framework that enables programming-language-agnostic parallel and distributed stream processing, can be used to facilitate parallel and distributed simulations. Specifically, we describe two ways of using DUP to utilize available resources for efficient simulation: (1) a straightforward technique for parallelizing multiple runs of an existing simulation program with minimal changes, and (2) FiDES, a Discrete-Event Simulation (DES) framework built atop DUP that provides a simple, yet powerful, means of implementing a parallel and/or distributed DES. We then describe a toolset for profiling, debugging and visualization that aids the development of DUP simulations. To support these claims, we present various performance benchmarks that collectively demonstrate how DUP and FiDES can make high-performance simulation accessible to everyone. © 2010 SCS.},
keywords={Efficient simulation;  High-performance simulation;  Parallel and distributed simulation;  Parallelizing;  Simulation program;  Straightforward techniques;  Stream processing;  Toolsets, Computer hardware description languages;  Distributed parameter control systems;  Visualization, Distributed computer systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao201023,
author={Gao, C. and Huo, Y. and Su, Y. and Wu, J. and Ma, Y.},
title={Su-PeerCast: A P2P live streaming system with Super-Node based on PeerCast},
journal={IET Conference Publications},
year={2010},
volume={2010},
number={571 CP},
pages={23-27},
doi={10.1049/cp.2010.0713},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751631567&doi=10.1049%2fcp.2010.0713&partnerID=40&md5=e4e6e8e92963a1cd0db71727e53f09f0},
abstract={The amount of client a live streaming system can serve by unicast is limited by the bandwidth requirement. Theoretically, IP-multicast is an efficient solution for that situation, but it suffers from poor deployment. Therefore, another solution, called Application Layer Multicast (ALM), is being increasingly recognized as a available alternative. However, this solution also has certain shortcomings. In this paper, we firstly introduce a live streaming system-PeerCast based on application layer multicast, and besides that we indicate its existing problems under practical deployment in the large-scale network. Secondly, in order to improve the PeerCast system performance, we append a Super-Node layer, which can divide the live system into different domains, and re-design the play process and heartbeat detection mechanism for our new live system. Finally, the re-designed system performance evaluation is presented.},
author_keywords={Large-scale;  P2P;  Streaming;  Super-Node;  Tracker},
keywords={Large-scale;  P2P;  Streaming;  Super-Node;  Tracker, Internet;  Internet protocols;  Multicasting;  Video streaming, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu2009331,
author={Wu, C. and He, X. and Wan, S. and Cao, Q. and Xie, C.},
title={Hotspot prediction and cache in distributed stream-processing storage systems},
journal={2009 IEEE 28th International Performance Computing and Communications Conference, IPCCC 2009},
year={2009},
pages={331-340},
doi={10.1109/PCCC.2009.5403810},
art_number={5403810},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951173057&doi=10.1109%2fPCCC.2009.5403810&partnerID=40&md5=a13fdd3331e61a66b8463e804d30dce2},
abstract={Storage performance is critical in today's distributed stream-processing systems. One approach to improve the performance is to use hotspot attribute in object-based storage systems. This paper discusses hotspot classification and identification, and then presents an Object Hotspot Prediction Model (OHPM) to dynamically predict hotspots. Based on this model, we discuss an efficient hotspot caching strategy to improve the performance. To demonstrate the effectiveness of our proposed approach, we have developed a prototype of Hotspot Attribute-managed Storage System (HASS) by extending Object-based Storage Device (OSD) file system and iSCSI protocols. Experimental results show that the HASS improves the throughput by up to 62% and reduces the disk I/O by as much as 25% in our VoD tests by integrating our object hotspot prediction and cache approaches. © 2009 IEEE.},
author_keywords={Hotspot;  Object-based storage;  Performance evaluation;  Stream-processing;  Zipf-like distribution},
keywords={Caching strategy;  Disk I/O;  File systems;  Hot spot;  Hotspots;  Object-based storage;  Object-based storage devices;  Prediction model;  Processing systems;  Storage performance;  Storage systems;  Zipf-like distribution, Hydraulics;  Mathematical models, Forecasting},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Giacomazzi2008287,
author={Giacomazzi, P. and Poli, A.},
title={Performance analysis of peer-to-peer video streaming systems with tree and forest topology},
journal={Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
year={2008},
pages={287-294},
doi={10.1109/ICPADS.2008.12},
art_number={4724331},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-60649120789&doi=10.1109%2fICPADS.2008.12&partnerID=40&md5=0545057db7b5f5374f61a136ecbaed05},
abstract={Peer-to-peer networks are an increasingly popular solution for the distribution of media content to a large number of users, with limited investments for network infrastructures. The distribution of a real time video stream imposes strict performance requirements such as small playback delays and few frame losses. In this paper, we focus on peer-to-peer video streaming systems with tree or forest content distribution structure and we provide a sensitivity analysis to investigate the impact of three critical parameters - rejoin time, average permanence time of peers and playback threshold - over the quality of the video stream received by users. The study, carried out through simulation, considers a general peer-to-peer video streaming reference model with tree/forest topology. © 2008 IEEE.},
keywords={Content distributions;  Critical parameters;  Frame loss;  Media contents;  Network infrastructures;  Peer-to-peer networks;  Peer-to-peer video streaming;  Performance analysis;  Performance requirements;  Playback delays;  Real-time video streams;  Reference models;  Video streams, Acoustic streaming;  Distributed computer systems;  Sensitivity analysis;  Videotex, Video streaming},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Krishnamurthy2003655,
author={Krishnamurthy, R. and Schwan, K. and West, R. and Rosu, M.-C.},
title={On network CoProcessors for scalable, predictable media services},
journal={IEEE Transactions on Parallel and Distributed Systems},
year={2003},
volume={14},
number={7},
pages={655-670},
doi={10.1109/TPDS.2003.1214318},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042062352&doi=10.1109%2fTPDS.2003.1214318&partnerID=40&md5=8779dcf0214bec76c1d6a9246c6b50c3},
abstract={This paper presents the embedded realization and experimental evaluation of a media stream scheduler on Network Interface (NI) CoProcessor boards. When using media frames as scheduling units, the scheduler is able to operate in real-time on streams traversing the Coprocessor, resulting in its ability to stream video to remote clients at real-time rates. The contributions of this paper are its detailed evaluation of the effects of placing application or kernel-level functionality, like packet scheduling on NIs, rather than the host machines to which they are attached. The main benefits of such placement are 1) that traffic is eliminated from the host bus and memory subsystem, thereby allowing increased host CPU utilization for other tasks, and 2) that NI-based scheduling is immune to host-CPU loading, unlike host-based media schedulers that are easily affected even by transient load conditions. An outcome of this work is a proposed cluster architecture for building scalable media servers by distributing schedulers and media stream producers across the multiple NIs used by a single server and by clustering a number of such servers using commodity network hardware and software.},
author_keywords={Cluster machines;  Data streaming;  Embedded systems;  Multimedia services;  Operating systems;  Packet scheduling;  Quality of service;  Real-time systems},
keywords={Computer software;  Embedded systems;  Quality of service;  Real time systems;  Servers, Packet scheduling, Multimedia systems},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Livaja2022282,
author={Livaja, I. and Pripužić, K. and Sovilj, S. and Vuković, M.},
title={A distributed geospatial publish/subscribe system on Apache Spark},
journal={Future Generation Computer Systems},
year={2022},
volume={132},
pages={282-298},
doi={10.1016/j.future.2022.02.013},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126095789&doi=10.1016%2fj.future.2022.02.013&partnerID=40&md5=3df2a6fff60a3e371d2744719e15bee2},
abstract={Publish/subscribe is a messaging pattern where message producers, called publishers, publish messages which they want to be distributed to message consumers, called subscribers. Subscribers are required to subscribe to messages of interest in advance to be able to receive them upon the publishing. In this paper, we discuss a special type of publish/subscribe systems, namely geospatial publish/subscribe systems (GeoPS systems), in which both published messages (i.e., publications) and subscriptions include a geospatial object. Such an object is used to express both the location information of a publication and the location of interest of a subscription. We argue that there is great potential for using GeoPS systems for the Internet of Things and Sensor Web applications. However, existing GeoPS systems are not applicable for this purpose since they are centralized and cannot cope with multiple highly frequent incoming geospatial data streams containing publications. To overcome this limitation, we present a distributed GeoPS system in the cluster which efficiently matches incoming publications in real-time with a set of stored subscriptions. Additionally, we propose four different (distributed) replication and partitioning strategies for managing subscriptions in our distributed GeoPS system. Finally, we present results of an extensive experimental evaluation in which we compare the throughput, latency and memory consumption of these strategies. These results clearly show that they are both efficient and scalable to larger clusters. The comparison with centralized state-of-the-art approaches shows that the additional processing overhead of our distributed strategies introduced by the Apache Spark is almost negligible. © 2022 Elsevier B.V.},
author_keywords={Big data;  Data replication;  Data stream processing;  Geospatial data;  Partitioning},
keywords={Data handling;  Publishing, Centralised;  Data replication;  Data streams processing;  Geo-spatial;  Geo-spatial data;  Geo-spatial objects;  Location information;  Partitioning;  Publish/subscribe;  Publish/Subscribe system, Big data},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Heinrich202285,
author={Heinrich, R. and Luthra, M. and Kornmayer, H. and Binnig, C.},
title={Zero-shot cost models for distributed stream processing},
journal={DEBS 2022 - Proceedings of the 16th ACM International Conference on Distributed and Event-Based Systems},
year={2022},
pages={85-90},
doi={10.1145/3524860.3539639},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135440808&doi=10.1145%2f3524860.3539639&partnerID=40&md5=6efc78cb228e66e6c5bb5d3b5f8c3ee4},
abstract={This paper proposes a learned cost estimation model for Distributed Stream Processing Systems (DSPS) with an aim to provide accurate cost predictions of executing queries. A major premise of this work is that the proposed learned model can generalize to the dynamics of streaming workloads out-of-the-box. This means a model once trained can accurately predict performance metrics such as latency and throughput even if the characteristics of the data and workload or the deployment of operators to hardware changes at runtime. That way the model can be used to solve tasks such as optimizing the placement of operators to minimize the end-to-end latency of a streaming query or maximize its throughput even under varying conditions. Our evaluation on a well-known DSPS, Apache Storm, shows that the model can predict accurately for unseen workloads and queries while generalizing across real-world benchmarks. © 2022 ACM.},
author_keywords={Cost Models;  Stream processing;  Zero-shot learning},
keywords={Cost estimating;  Distributed parameter control systems;  Forecasting, Condition;  Cost estimation models;  Cost models;  Cost prediction;  Distributed stream processing;  End to end latencies;  Performance metrices;  Runtimes;  Stream processing;  Stream processing systems, Zero-shot learning},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lv2022249,
author={Lv, C. and Niu, C. and Gu, R. and Jiang, X. and Wang, Z. and Liu, B. and Wu, Z. and Yao, Q. and Huang, C. and Huang, P. and Huang, T. and Shu, H. and Song, J. and Zou, B. and Lan, P. and Xu, G. and Wu, F. and Tang, S. and Wu, F. and Chen, G.},
title={Walle: An End-to-End, General-Purpose, and Large-Scale Production System for Device-Cloud Collaborative Machine Learning},
journal={Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2022},
year={2022},
pages={249-265},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137151521&partnerID=40&md5=c24a73dcdce18e0ab844d8a450e42725},
abstract={To break the bottlenecks of mainstream cloud-based machine learning (ML) paradigm, we adopt device-cloud collaborative ML and build the first end-to-end and general-purpose system, called Walle, as the foundation. Walle consists of a deployment platform, distributing ML tasks to billion-scale devices in time; a data pipeline, efficiently preparing task input; and a compute container, providing a cross-platform and high-performance execution environment, while facilitating daily task iteration. Specifically, the compute container is based on Mobile Neural Network (MNN), a tensor compute engine along with the data processing and model execution libraries, which are exposed through a refined Python thread-level virtual machine (VM) to support diverse ML tasks and concurrent task execution. The core of MNN is the novel mechanisms of operator decomposition and semi-auto search, sharply reducing the workload in manually optimizing hundreds of operators for tens of hardware backends and further quickly identifying the best backend with runtime optimization for a computation graph. The data pipeline introduces an on-device stream processing framework to enable processing user behavior data at source. The deployment platform releases ML tasks with an efficient push-then-pull method and supports multi-granularity deployment policies. We evaluate Walle in practical e-commerce application scenarios to demonstrate its effectiveness, efficiency, and scalability. Extensive micro-benchmarks also highlight the superior performance of MNN and the Python thread-level VM. Walle has been in large-scale production use in Alibaba, while MNN has been open source with a broad impact in the community. © 2022 by The USENIX Association. All rights reserved.},
keywords={Behavioral research;  Benchmarking;  Cloud computing;  Containers;  Data handling;  Digital libraries;  High level languages;  Iterative methods;  Pipeline processing systems;  Pipelines;  Python;  Virtual machine, Cloud-based;  Data pipelines;  End to end;  Large scale productions;  Learning paradigms;  Learning tasks;  Machine-learning;  Neural-networks;  Performance;  Production system, Machine learning},
publisher={USENIX Association},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Garcia2022,
author={Garcia, A.M. and Griebler, D. and Schepke, C. and Fernandes, L.G.},
title={SPBench: a framework for creating benchmarks of stream processing applications},
journal={Computing},
year={2022},
doi={10.1007/s00607-021-01025-6},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122703479&doi=10.1007%2fs00607-021-01025-6&partnerID=40&md5=89cd3c9c6304e30e75d1c8ac4d498f54},
abstract={In a fast-changing data-driven world, real-time data processing systems are becoming ubiquitous in everyday applications. The increasing data we produce, such as audio, video, image, and, text are demanding quickly and efficiently computation. Stream Parallelism allows accelerating this computation for real-time processing. But it is still a challenging task and most reserved for experts. In this paper, we present SPBench, a framework for benchmarking stream processing applications. It aims to support users with a set of real-world stream processing applications, which are made accessible through an Application Programming Interface (API) and executable via Command Line Interface (CLI) to create custom benchmarks. We tested SPBench by implementing parallel benchmarks with Intel Threading Building Blocks (TBB), FastFlow, and SPar. This evaluation provided useful insights and revealed the feasibility of the proposed framework in terms of usage, customization, and performance analysis. SPBench demonstrated to be a high-level, reusable, extensible, and easy of use abstraction to build parallel stream processing benchmarks on multi-core architectures. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.},
author_keywords={Computing workloads;  Parallel computing;  Parallel programming;  Performance analysis;  Stream parallelism},
keywords={Application programming interfaces (API);  Benchmarking;  Computer architecture;  Computer software reusability;  Data handling;  Distributed parameter control systems;  Multicore programming;  Real time systems, Audio videos;  Computing workloads;  Data driven;  Data-processing system;  Parallel com- puting;  Performances analysis;  Processing applications;  Real-time data processing;  Stream parallelism;  Stream processing, Parallel programming},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gütlein202218,
author={Gütlein, M. and Djanatliev, A.},
title={On-demand Simulation of Future Mobility Based on Apache Kafka},
journal={Lecture Notes in Networks and Systems},
year={2022},
volume={306},
pages={18-41},
doi={10.1007/978-3-030-84811-8_2},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113670523&doi=10.1007%2f978-3-030-84811-8_2&partnerID=40&md5=d13b9dd170da50306d7fa82412f4e58d},
abstract={The modeling and simulation community can benefit from building and running (co-)simulation models on-demand. In order to drive innovation further, it should be easy to set up, orchestrate, and execute simulations – even for researchers with no background in computer science. Additionally, open interfaces should be the default to enable a variety of applications. The possibility to connect external components smoothly with a simulation run allows for elaborate experiments. In this work, the details of the architecture and design of a simulation platform enabling evaluations of future mobility scenarios are presented. While the explanations are based on mobility applications, the concept can be generalized and applied to other domains. The novelty lies in the use of a big data stream processing platform called Apache Kafka. All kinds of required communication are handled by the Kafka middleware. Even the coupling of different simulators is realized on top of it. As a foundation, existing works regarding Modeling and Simulation as a Service are presented. The approach is then described by its architecture, its interfaces, and the life-cycle of a simulation run. Further information is given on the distribution of simulations and the topic-based clock synchronization mechanism. A performance evaluation shows that the approach is on one level with state-of-the art co-simulation middlewares. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Apache Kafka;  Distributed;  Mobility;  Modeling and Simulation as a Service;  Parallel;  Simulation},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Raza20215,
author={Raza, M. and Tahir, J. and Doblander, C. and Mayer, R. and Jacobsen, H.-A.},
title={Benchmarking Apache Kafka under network faults},
journal={Middleware 2021 Demos and Posters - Proceedings of the 2021 International Middleware Conference Demos and Posters},
year={2021},
pages={5-7},
doi={10.1145/3491086.3492470},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121433831&doi=10.1145%2f3491086.3492470&partnerID=40&md5=09ddc2983abe9572d46615510561483a},
abstract={Network faults are often transient and hence hard to detect and difficult to resolve. Our study conducts an analysis of Kafka's network fault tolerance capabilities, one of the widely used distributed stream processing system (DSPS). Across different Kafka configurations, we observed that Kafka is fault-tolerant towards network faults to some degree, and we report observations of its shortcomings. We also define a network fault-tolerance benchmark on which other DSPSs can be evaluated. © 2021 ACM.},
author_keywords={chaos testing;  Kafka;  Kafka streams;  monitoring},
keywords={Distributed parameter control systems, Chaos testing;  Distributed stream processing;  Fault-tolerance capability;  Fault-tolerant;  Kafkum;  Kafkum stream;  Network fault tolerances;  Network faults;  Stream processing systems, Fault tolerance},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Daghistani2021_62,
author={Daghistani, A. and Aref, W.G. and Ghafoor, A. and Mahmood, A.R.},
title={SWARM: Adaptive load balancing in distributed streaming systems for big spatial data},
journal={ACM Transactions on Spatial Algorithms and Systems},
year={2021},
volume={7},
number={3},
doi={10.1145/3460013},
art_number={3460013},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122631817&doi=10.1145%2f3460013&partnerID=40&md5=6bf53fd6352f8cec163c142d6d7654d7},
abstract={The proliferation of GPS-enabled devices has led to the development of numerous location-based services. These services need to process massive amounts of streamed spatial data in real-time. The current scale of spatial data cannot be handled using centralized systems. This has led to the development of distributed spatial streaming systems. Existing systems are using static spatial partitioning to distribute the workload. In contrast, the real-time streamed spatial data follows non-uniform spatial distributions that are continuously changing over time. Distributed spatial streaming systems need to react to the changes in the distribution of spatial data and queries. This article introduces SWARM, a lightweight adaptivity protocol that continuously monitors the data and query workloads across the distributed processes of the spatial data streaming system and redistributes and rebalances the workloads as soon as performance bottlenecks get detected. SWARM is able to handle multiple query-execution and data-persistence models. A distributed streaming system can directly use SWARM to adaptively rebalance the system's workload among its machines with minimal changes to the original code of the underlying spatial application. Extensive experimental evaluation using real and synthetic datasets illustrate that, on average, SWARM achieves 2 improvement in throughput over a static grid partitioning that is determined based on observing a limited history of the data and query workloads. Moreover, SWARM reduces execution latency on average 4 compared with the other technique. © 2021 Association for Computing Machinery.},
author_keywords={Cluster utilization;  Distributed streaming systems;  Load balancing;  Spatial continuous queries;  Spatial stream processing},
keywords={Resource allocation;  Search engines;  Spatial distribution;  Telecommunication services, Cluster utilization;  Continuous queries;  Distributed streaming;  Distributed streaming system;  Load-Balancing;  Spatial continuous query;  Spatial data;  Spatial stream processing;  Stream processing;  Streaming systems, Location based services, adaptive management;  experimental study;  spatial data;  spatial distribution;  streaming potential},
publisher={Association for Computing Machinery},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jean-Pierre2021,
author={Jean-Pierre, G. and Akbarihaghighat, H. and Zhao, T. and Berger, A. and Nafsin, N. and Nasir, F.B. and Bravo, H. and Li, J. and Nasiri, A. and Nowak, M.},
title={Development of a Data Analytics Platform for an Electrical/Water Microgrid},
journal={Proceedings of the 2021 IEEE 12th International Symposium on Power Electronics for Distributed Generation Systems, PEDG 2021},
year={2021},
doi={10.1109/PEDG51384.2021.9494250},
art_number={9494250},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112357222&doi=10.1109%2fPEDG51384.2021.9494250&partnerID=40&md5=7e8e4470983e784bca4917b35dadf711},
abstract={Data enabled systems can offer multiple improvements over traditional systems, including higher efficiency, higher reliability, and lower maintenance cost. There has been a large growth in the development of data enabled systems in various industrial sectors, such as energy, manufacturing, and water distribution systems. In order to achieve a data enabled system, it is paramount to develop an analytical platform by collecting data, and formulating and monitoring key performance indicators (KPIs). This paper presents a multilayer structured communication and data analytic framework to collect real-time, high-fidelity data for a full scale electrical microgrid and water system testbed. The system has deployed various electrical and water sensors, communication interfaces, data streaming libraries, cloud programming and storage, data dashboards, and an HMI. Actual water and electrical test systems were built to test this replicable platform. © 2021 IEEE.},
author_keywords={Cloud storage;  data analytic;  HMI;  KPI;  microgrid},
keywords={Benchmarking;  Data Analytics;  Digital storage;  Distributed power generation;  Electric power system interconnection;  Microgrids;  Power electronics;  Water distribution systems, Communication interface;  Electrical tests;  Higher efficiency;  Industrial sector;  Key performance indicators;  Lower maintenance costs;  Structured communication;  Traditional systems, Data communication systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fang20211861,
author={Fang, J. and Zhang, R. and Zhao, Y. and Zheng, K. and Zhou, X. and Zhou, A.},
title={A-DSP: An adaptive join algorithm for dynamic data stream on cloud system},
journal={IEEE Transactions on Knowledge and Data Engineering},
year={2021},
volume={33},
number={5},
pages={1861-1876},
doi={10.1109/TKDE.2019.2947055},
art_number={08868214},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103966801&doi=10.1109%2fTKDE.2019.2947055&partnerID=40&md5=a8cd11cb083765334890d567ba86dccc},
abstract={The join operations, including both equi and non-equi joins, are essential to the complex data analytics in the big data era. However, they are not inherently supported by existing DSPEs (Distributed Stream Processing Engines). The state-of-the-art join solutions on DSPEs rely on either complicated routing strategies or resource-inefficient processing structures, which are susceptible to dynamic workload, especially when the DSPEs face various join predicate operations and skewed data distribution. In this paper, we propose a new cost-effective stream join framework, named A-DSP (Adaptive Dimensional Space Processing), which enhances the adaptability of real-time join model and minimizes the resource used over the dynamic workloads. Our proposal includes: 1) a join model generation algorithm devised to adaptively switch between different join schemes so as to minimize the number of processing task required; 2) a load-balancing mechanism which maximizes the processing throughput; and 3) a lightweight algorithm designed for cutting down unnecessary migration cost. Extensive experiments are conducted to compare our proposal against state-of-the-art solutions on both benchmark and real-world workloads. The experimental results verify the effectiveness of our method, especially on reducing the operational cost under pay-as-you-go pricing scheme. © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.},
author_keywords={Cost effective;  Distributed stream join;  Theta-join},
keywords={Balancing;  Cost effectiveness;  Costs;  Cutting;  Data Analytics;  Digital signal processing;  Distributed parameter control systems, Dimensional spaces;  Distributed stream processing;  Load-balancing mechanisms;  Migration costs;  Model generation;  Pricing scheme;  Routing strategies;  State of the art, Data streams},
publisher={IEEE Computer Society},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Runsewe2021504,
author={Runsewe, O. and Samaan, N.},
title={Cloud resource scaling for time-bounded and unbounded big data streaming applications},
journal={IEEE Transactions on Cloud Computing},
year={2021},
volume={9},
number={2},
pages={504-517},
doi={10.1109/TCC.2018.2876242},
art_number={8493343},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055059054&doi=10.1109%2fTCC.2018.2876242&partnerID=40&md5=d985226883e7df8a6196069f1fb95702},
abstract={Recent advancements in technology have led to a deluge of big data streams that require real-time analysis with strict latency constraints. A major challenge, however, is determining the amount of resources required by applications processing these streams given their high volume, velocity and variety. The majority of research efforts on resource scaling in the cloud are investigated from the cloud provider's perspective with little consideration for multiple resource bottlenecks. We aim at analyzing the resource scaling problem from an application provider's point of view such that efficient scaling decisions can be made. This paper provides two contributions to the study of resource scaling for big data streaming applications in the cloud. First, we present a Layered Multi-dimensional Hidden Markov Model (LMD-HMM) for managing time-bounded streaming applications. Second, to cater to unbounded streaming applications, we propose a framework based on a Layered Multi-dimensional Hidden Semi-Markov Model (LMD-HSMM). The parameters in our models are evaluated using modified Forward and Backward algorithms. Our detailed experimental evaluation results show that LMD-HMM is very effective with respect to cloud resource prediction for bounded streaming applications running for shorter periods while the LMD-HSMM accurately predicts the resource usage for streaming applications running for longer periods. © 2013 IEEE.},
author_keywords={Big data;  cloud computing;  layered hidden Markov model;  layered hidden semi-Markov model;  resource prediction;  resource scaling;  stream processing},
keywords={Big data;  Cloud computing;  Data reduction;  Data streams;  Interactive computer systems;  Job analysis;  Predictive analytics;  Real time systems;  Storms;  Trellis codes, Hidden semi-Markov modeling;  Layered hidden markov models;  Predictive models;  Resource prediction;  Resource Scaling;  Stream processing;  Task analysis, Hidden Markov models},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ojo2021559,
author={Ojo, O.E. and Oluwatope, A.O. and Ajadi, S.O.},
title={A reliable peer-to-peer streaming protocol in low-capacity networks},
journal={Peer-to-Peer Networking and Applications},
year={2021},
volume={14},
number={2},
pages={559-584},
doi={10.1007/s12083-020-01002-4},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092597224&doi=10.1007%2fs12083-020-01002-4&partnerID=40&md5=d8a6a69c36fff7aab4b88ff6f3471ce5},
abstract={The recent global demand for video streaming applications has paved the way for peer-to-peer streaming system (P2PSS). Strategic scheduling scheme and dynamic overlay topology are essential to maintain quality of service (QoS) and quality of experience (QoE) in P2PSS. The concept of P2PSS was tailored towards relying on active peers’ bandwidth to achieve cheap and scalable means of distribution over the Internet, such that peers with highest bandwidth serve as backbones for others. However, selecting backbone peers in low-capacity network environment is challenging due to insufficient bandwidth and poor infrastructure, thereby resulting in poor QoS and unpleasant user’s QoE. In this paper, we conducted a survey on users’ experiences with live video in selected locations in Nigeria. We designed an adaptive P2P streaming protocol and performed a packet-level simulation in Network Simulator 3(NS-3). Diverse simulation scenarios were set up to evaluate the proposed streaming protocol. Trace files data were analysed to measure end-to-end delay, start-up delay, and throughput. Furthermore, the proposed streaming protocol was benchmarked against selected existing schemes. The evaluation results revealed a 7.4% and 28% reduction in start-up and in end-to-end delays and 9% increase in throughput. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Low-capacity networks;  Peer-to-peer networks;  Quality of experience;  Quality of Service;  Video streaming},
keywords={Bandwidth;  Internet protocols;  Quality of service, Low capacity networks;  Overlay topologies;  Packet level simulation;  Peer-to-peer streaming;  Quality of experience (QoE);  Strategic scheduling;  STreaming protocols;  Video Streaming Applications, Peer to peer networks},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Qi2021,
author={Qi, T. and Rodriguez, M.},
title={A Traffic and Resource Aware Online Storm Scheduler},
journal={ACM International Conference Proceeding Series},
year={2021},
doi={10.1145/3437378.3444365},
art_number={3444365},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100623396&doi=10.1145%2f3437378.3444365&partnerID=40&md5=2437db31ff32048035ef4543ca7fb0e7},
abstract={Streaming applications have become widespread with the advent of big data and IoT. They are latency-sensitive applications that aim to process vast amounts of data in near real time. They are usually modelled as directed graphs and their deployment and orchestration in a cluster of nodes are managed by distributed stream processing systems. These systems are responsible for placing the graph components within the cluster nodes, which determines the application's communication overhead and ultimately affects performance metrics such as latency and throughput. This work presents an adaptive, heuristic-based, scheduling algorithm for distributed stream processing systems that aims to minimize the latency and maximize the throughput of streaming applications deployed in heterogeneous clusters, while considering the resource constraints of the available nodes. The proposed approach uses a graph partitioning algorithm and real time traffic data monitored from a deployed application to derive a near-optimal operator placement plan that minimizes inter-node communication and balances the overall communication load distribution. We evaluated our approach using three micro-benchmark and two practical applications, and the results demonstrate that our scheduler outperforms the default scheduler of a popular stream processing system and a state-of-the-art algorithm, improving throughput by up to 106% and reducing complete latency by up to 58% for most applications. © 2021 ACM.},
author_keywords={Graph partitioning;  Scheduling;  Storm;  Stream Processing},
keywords={Benchmarking;  Directed graphs;  Distributed parameter control systems;  Graph algorithms;  Scheduling, Communication overheads;  Distributed stream processing;  Graph partitioning algorithms;  Heterogeneous clusters;  Inter-node communication;  Real-time traffic datum;  State-of-the-art algorithms;  Stream processing systems, Clustering algorithms},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{VanDongen2021109413,
author={Van Dongen, G. and Van Den Poel, D.},
title={Influencing Factors in the Scalability of Distributed Stream Processing Jobs},
journal={IEEE Access},
year={2021},
volume={9},
pages={109413-109431},
doi={10.1109/ACCESS.2021.3102645},
art_number={9507502},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112225373&doi=10.1109%2fACCESS.2021.3102645&partnerID=40&md5=b36bb0b4727f45bfd4192b41230a52a2},
abstract={More and more use cases require fast, accurate, and reliable processing of large volumes of data. To do this, a distributed stream processing framework is needed which can distribute the load over several machines. In this work, we study and benchmark the scalability of stream processing jobs in four popular frameworks: Flink, Kafka Streams, Spark Streaming, and Structured Streaming. Besides that, we determine the factors that influence the performance and efficiency of scaling processing jobs with distinct characteristics. We evaluate horizontal, as well as vertical scalability. Our results show how the scaling efficiency is impacted by many factors including the initial cluster layout and direction of scaling, the pipeline design, the framework design, resource allocation, and data characteristics. Finally, we give some recommendations on how practitioners should undertake to scale their clusters. © 2013 IEEE.},
author_keywords={apache flink;  apache kafka;  Apache spark;  benchmarking;  big data;  distributed computing;  kafka streams;  scalability;  stream processing frameworks;  structured streaming},
keywords={Distributed parameter control systems;  Efficiency;  Horizontal wells, Data characteristics;  Distributed stream processing;  Framework designs;  Large volumes;  Pipeline design;  Processing jobs;  Stream processing;  Vertical scalabilities, Scalability},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Farhat2021485,
author={Farhat, O. and Daudjee, K. and Querzoni, L.},
title={Klink: Progress-Aware Scheduling for Streaming Data Systems},
journal={Proceedings of the ACM SIGMOD International Conference on Management of Data},
year={2021},
pages={485-498},
doi={10.1145/3448016.3452794},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108945921&doi=10.1145%2f3448016.3452794&partnerID=40&md5=cc713b1a9d3753cfc34b4b448f31e1b5},
abstract={Modern stream processing engines (SPEs) process large volumes of events propagated at high velocity through multiple queries. To improve performance, existing SPEs generally aim to minimize query output latency by minimizing, in turn, the propagation delay of events in query pipelines. However, for queries containing commonly used blocking operators such as windows, this scheduling approach can be inefficient. Watermarks are events popularly utilized by SPEs to correctly process window operators. Watermarks are injected into the stream to signify that no events preceding their timestamp should be further expected. Through the design and development of Klink, we leverage these watermarks to robustly infer stream progress based on window deadlines and network delay, and to schedule query pipeline execution that reflects stream progress. Klink aims to unblock window operators and to rapidly propagate events to output operators while performing judicious memory management. We integrate Klink into the popular open source SPE Apache Flink and demonstrate that Klink delivers significant performance gains over existing scheduling policies on benchmark workloads for both scale-up and scale-out deployments. © 2021 ACM.},
author_keywords={scheduling;  stream processing;  watermarks;  windows},
keywords={Benchmarking;  Pipelines;  Watermarking, Design and Development;  Improve performance;  Memory management;  Multiple queries;  Performance Gain;  Propagation delays;  Scheduling policies;  Stream processing engines, Scheduling},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sornalakshmi2021,
author={Sornalakshmi, K. and Vadivu, G.},
title={Dynamic Auto Reconfiguration of Operator Placement in Wireless Distributed Stream Processing Systems},
journal={Wireless Personal Communications},
year={2021},
doi={10.1007/s11277-021-08264-y},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101225936&doi=10.1007%2fs11277-021-08264-y&partnerID=40&md5=52361ae3cfa6d9a043f83ccef152fd5f},
abstract={The data is generated at significant speed and volume by devices in real-time. The data generation and the growth of fog and edge computing infrastructure have led to the noteworthy development of the corresponding distributed stream processing systems (DSPS). A DSPS application has Quality of Service (QoS) restrictions in terms of resource cost and time. The physical resources are distributed and heterogeneous. The resource-constrained scheduling problem has considerable implications on the performance of the system and QoS violations. The static deployment of applications in fog or edge scenario has to be monitored continuously for runtime issues, and actions have to be taken accordingly. In this paper, we propose an adaptation capability with reinforcement learning techniques to an existing stream processing framework scheduler. This functionality enables the scheduler to make decisions on its own when the system model or knowledge of the environment is not known upfront. The reinforcement learning methods adapt to the system when the system model for different states is not available. We consider applications whose workload cannot be characterized or predicted. In such applications, predictions of input load are not helpful for online scheduling. The Q-Learning based online scheduler learns to make dynamic scaling decisions at runtime when there is performance degradation. We validated the proposed approach with real-time and benchmark applications on a DSPS cluster. We obtained an average of 6% reduction in the response time and a 15% increase in the throughput when the Q Learning module is employed in the scheduler. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.},
author_keywords={Adaptive scheduling;  Decision making;  Distributed stream processing systems;  Reconfiguration;  Reinforcement learning;  Runtime scheduling;  Scalability},
keywords={Benchmarking;  Data streams;  Distributed parameter control systems;  Quality of service;  Reinforcement learning;  Scheduling, Benchmark applications;  Computing infrastructures;  Distributed stream processing;  Operator placements;  Physical resources;  Reinforcement learning method;  Reinforcement learning techniques;  Resource constrained scheduling, Learning systems},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gilroy20202957,
author={Gilroy, J. and Paronyan, S. and Acoltzi, J. and Fukuda, M.},
title={Agent-Navigable Dynamic Graph Construction and Visualization over Distributed Memory},
journal={Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
year={2020},
pages={2957-2966},
doi={10.1109/BigData50022.2020.9378298},
art_number={9378298},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103856946&doi=10.1109%2fBigData50022.2020.9378298&partnerID=40&md5=060f73eb310136104c7d4d965aa813d2},
abstract={Some graph analyses, such as social network and biological network, need large-scale graph construction and maintenance over distributed memory space. Distributed data-streaming tools, including MapReduce and Spark, restrict some computational freedom of incremental graph modification and run-time graph visualization. Instead, we take an agent-based approach. We construct a graph from a scientific dataset in CSV, tab, and XML formats; dispatch many reactive agents on it; and analyze the graph in the form of their collective group behavior: propagation, flocking, and collision. The key to success is how to automate the run-time construction and visualization of agent-navigable graphs mapped over distributed memory. We implemented this distributed graph-computing support in the multi-agent spatial simulation (MASS) library, coupled with the Cytoscape graph visualization software. This paper presents the MASS implementation techniques and demonstrates its execution performance in comparison to MapReduce and Spark, using two benchmark programs: (1) an incremental construction of a complete graph and (2) a KD tree construction. © 2020 IEEE.},
author_keywords={agent-based modeling;  data analysis;  data visualization;  multi-agent systems;  parallel programming},
keywords={Benchmarking;  Big data;  Data visualization;  Memory architecture;  Multi agent systems;  Software agents;  Visualization, Agent-based approach;  Biological networks;  Execution performance;  Graph modifications;  Graph visualization;  Implementation techniques;  Incremental construction;  Kd-tree constructions, Trees (mathematics)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hoseinyfarahabady2020,
author={Hoseinyfarahabady, M.R. and Taheri, J. and Zomaya, A.Y. and Tari, Z.},
title={A Dynamic Resource Controller for Resolving Quality of Service Issues in Modern Streaming Processing Engines},
journal={2020 IEEE 19th International Symposium on Network Computing and Applications, NCA 2020},
year={2020},
doi={10.1109/NCA51143.2020.9306697},
art_number={9306697},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099730655&doi=10.1109%2fNCA51143.2020.9306697&partnerID=40&md5=baed84857cf5173d08fd15986f1326a8},
abstract={Devising an elastic resource allocation controller of data analytical applications in virtualized data-center has received a great attention recently, mainly due to the fact that even a slight performance improvement can translate to huge monetary savings in practical large-scale execution. Apache Flink is among modern streamed data processing run-times that can provide both low latency and high throughput computation in to execute processing pipelines over high-volume and high-velocity data-items under tight latency constraints. However, a yet to be answered challenge in a large-scale platform with tens of worker nodes is how to resolve the run-time violation in the quality of service (QoS) level in a multi-tenant data streaming platforms, particularly when the amount of workload generated by different users fluctuates. Studies showed that a static resource allocation algorithm (round-robin), which is used by default in Apache Flink, suffer from lack of responsiveness to sudden traffic surges happening unpredictably during the run-time. In this paper, we address the problem of resource management in a Flink platform for ensuring different QoS enforcement levels in a platform with shared computing resources. The proposed solution applies theoretical principals borrowed from close-loop control theory to design a CPU and memory adjustment mechanism with the primary goal to fulfill the different QoS levels requested by submitted applications while the resource interference is considered as the critical performance-limiting factor. The performance evaluation is carried out by comparing the proposed resource allocation mechanism with two static heuristics (round robin and class-based weighted fair queuing) in a 80-core cluster under multiple traffic patterns resembling sudden changes in the incoming workloads of low-priory streaming applications. The experimental results confirm the stability of the proposed controller to regulate the underlying platform resources to smoothly follow the target values (QoS violation rates). Particularly, the proposed solution can achieve higher efficiency compared to the other heuristics by reducing the response-time of high priority applications by 53% while maintaining the enforced QoS levels during the burst traffic periods. © 2020 IEEE.},
author_keywords={Apache Flink Streaming Platform;  Elastic Auto-Tuning;  Performance Modeling of Computer System;  Quality of Services (QoS) Issues},
keywords={Closed loop control systems;  Computation theory;  Controllers;  Data handling;  Economics;  Pipeline processing systems;  Resource allocation, Adjustment mechanisms;  Analytical applications;  Large scale platforms;  Performance limiting factor;  Quality of service issues;  Streaming applications;  Virtualized data centers;  Weighted fair queuing, Quality of service},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ribezzo20204477,
author={Ribezzo, G. and De Cicco, L. and Palmisano, V. and Mascolo, S.},
title={TAPAS-360°: A Tool for the Design and Experimental Evaluation of 360° Video Streaming Systems},
journal={MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia},
year={2020},
pages={4477-4480},
doi={10.1145/3394171.3414541},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106937325&doi=10.1145%2f3394171.3414541&partnerID=40&md5=efd18dd90f1307063470770c0a7ea5d6},
abstract={Video streaming platforms are required to innovate their delivery pipeline to allow new and more immersive video content to be supported. In particular, Omnidirectional videos enable the user to explore a 360° scene by moving their heads using Head Mounted Display devices. Viewport adaptive streaming allows changing dynamically the quality of the video falling in the user's field of view. In this paper, we present TAPAS-360°, an open-source tool that enables designing and experimenting all the components required to build omnidirectional video streaming systems. The tool can be used by researchers focusing on the design of viewport-adaptive algorithms and also to produce video streams to be employed for subjective and objective Quality of Experience evaluations. © 2020 ACM.},
author_keywords={360-degree video;  adaptive video streaming;  dash},
keywords={Adaptive algorithms;  Helmet mounted displays;  Open systems;  Quality control;  Quality of service, Adaptive streaming;  Experimental evaluation;  Field of views;  Head mounted displays;  Immersive;  Objective qualities;  Open source tools;  Video contents, Video streaming},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bang2020322,
author={Bang, J. and Choi, M.-J.},
title={Docker environment based apache storm and spark benchmark test},
journal={APNOMS 2020 - 2020 21st Asia-Pacific Network Operations and Management Symposium: Towards Service and Networking Intelligence for Humanity},
year={2020},
pages={322-325},
doi={10.23919/APNOMS50412.2020.9237049},
art_number={9237049},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096952473&doi=10.23919%2fAPNOMS50412.2020.9237049&partnerID=40&md5=5387047288b21e2448d29b4eb62540ab},
abstract={With the development of various technologies such as high-speed Internet and SNS dissemination, there have been many fields that require processing of big data generated in real time. Accordingly, real-time streaming data processing technology has been developed, and representative platforms include Apache Storm, Apache Spark, and Hadoop. These processing technologies provide scalability to configure distributed systems using multiple servers because they vary in performance, such as throughput and processing speed, depending on the server environment, but the more the number of servers, the more difficult it is to manage. To solve this problem, a problem can be solved by using a docker, a kind of virtualization system that provides ease of expansion. However, there is a place to maintain a native environment without using Docker due to the problem that performance may be reduced, which is a disadvantage of all virtualization systems. In this paper, we build Apache Storm and Apache Spark, which are real-time data processing systems in Docker and Native environments and conduct performance measurements through experiments processing JSON-format data to verify how much performance decreases in Docker environments. © 2020 KICS.},
author_keywords={Apache Spark;  Apache Storm;  Benchmark Test;  Distributed Stream Processing Platform;  Docker},
keywords={Benchmarking;  Distributed database systems;  Real time systems;  Storms;  Virtualization, Distributed systems;  High speed internet;  Native environment;  Performance measurements;  Processing technologies;  Real time streaming;  Real-time data processing;  Various technologies, Data handling},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Harini20201012,
author={Harini, S. and Ravikumar, A.},
title={Effect of Parallel Workload on Dynamic Voltage Frequency Scaling for Dark Silicon Ameliorating},
journal={Proceedings - International Conference on Smart Electronics and Communication, ICOSEC 2020},
year={2020},
pages={1012-1017},
doi={10.1109/ICOSEC49089.2020.9215262},
art_number={9215262},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094215222&doi=10.1109%2fICOSEC49089.2020.9215262&partnerID=40&md5=67835b176a73e7b8bdd54211a9760fa5},
abstract={Dynamic Voltage and Frequency Scaling (DVFS) approach is proposed to control the power consumption in devices of different types like from a small mobile device to a huge server. This paper aims to study the effect of parallel execution of two benchmark applications, one the Stanford Single Source benchmarks (lightweight parallel code) and, Stanford Parallel Applications for Shared Memory (SPLASH-2) programs (heavy real-Time data streaming parallel applications that run on multi-core clusters) on DVFS for dark silicon ameliorating. Both these benchmarks executed on a gem5 Full System simulator, booting a linaro based linux kernel. To get a holistic big picture of the power and thermal properties of systems that run on DVFS, in addition to the control feature present in gem 5, a power-estimation framework used for the evaluation of the efficiency of various DVFS policies, the MCPAT and Hotspot is also employed. Based on the execution of Stanford workloads DVFS model is analyzed in terms of memory footprint and on other critical processor performance metrics, like running time per core, bus latency, number of read, writes, access time, cache hit or miss. The gem5 is extended with full DVFS support by the addition of a framework in which easy power-model integration can be done.. © 2020 IEEE.},
author_keywords={Area and Timing;  Dynamic Voltage and Frequency Scaling;  Gem5;  McPAT (Multicore Power;  Power consumption;  Shared Memory;  Stanford Parallel Applications for Shared-Memory;  Voltage Regulator},
keywords={Application programs;  Benchmarking;  Cache memory;  Computer operating systems;  Green computing;  Multicore programming;  Silicon;  Voltage scaling, Benchmark applications;  Dynamic voltage and frequency scaling;  Full system simulators;  Parallel application;  Parallel executions;  Parallel workloads;  Processor performance;  Real time data streaming, Dynamic frequency scaling},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang20207156,
author={Zhang, Y. and Yan, J. and Pu, L. and Chen, S.},
title={Dynamic Component Placement and Request Scheduling for IoT Big Data Streaming},
journal={IEEE Internet of Things Journal},
year={2020},
volume={7},
number={8},
pages={7156-7170},
doi={10.1109/JIOT.2020.2982458},
art_number={9044374},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089955275&doi=10.1109%2fJIOT.2020.2982458&partnerID=40&md5=df126f174959013284db6879fb15e5da},
abstract={Internet-of-Things (IoT) big data streaming applications, such as video surveillance and automatic driving, tend to use mobile-edge computing (MEC) infrastructure to enhance their performance and augment their functionalities. Although extensive previous studies have worked on offloading requests to MEC servers, none of them has comprehensively and thoroughly considered the important features of IoT data streaming applications (i.e., component dependency and dynamic arrival) and the infrastructure provisioning (i.e., capacity constraint and colocation interference). In this article, we consider the offloading problem for dynamically arrived IoT data streaming requests on MEC servers in real time. We model it as a delay-sensitive multiuser multiresource online offloading problem respecting component dependency and capacity constraint. The problem is NP-hard with offloading decisions coupling together. To solve it, we decouple the problem into component placement problem and request scheduling problem and propose a two-stage DPGPD algorithm with polynomial time complexity. We show the first stage dynamic programming (DP) algorithm is the optimal solution and the second-stage greedy primal-dual (GPD) algorithm is asymptotic optimal. The simulation results show that our solution is effective yet efficient compared to benchmark solutions. (DP provides the optimal placement layout with 12 × less decision time of Gurobi; and GPD provides the asymptotic optimal scheduling with 5 × less average waiting time compared to least work left (LWL) in heavy workload.) We implement a dedicated prototype and exploit several representative big data streaming applications to evaluate it. Lab-scale experiment shows that our solution can provide over 3 × less total completion time compared to local execution. © 2014 IEEE.},
author_keywords={Big data applications;  dynamic scheduling;  edge computing;  Internet-of-Things (IoT)},
keywords={Automobile drivers;  Big data;  Data reduction;  Dynamic programming;  Network security;  NP-hard;  Polynomial approximation;  Scheduling;  Security systems, Average waiting-time;  Benchmark solutions;  Capacity constraints;  Dynamic programming algorithm;  Internet of Things (IOT);  Polynomial time complexity;  Total completion time;  Video surveillance, Internet of things},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Presser202039,
author={Presser, D. and Siqueira, F. and Rodrigues, L. and Romano, P.},
title={EdgeScaler: Effective elastic scaling for graph stream processing systems},
journal={DEBS 2020 - Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
year={2020},
pages={39-50},
doi={10.1145/3401025.3401734},
art_number={3401734},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089271708&doi=10.1145%2f3401025.3401734&partnerID=40&md5=de3b7c6598bba24d54d4e29dbefe5e90},
abstract={Existing solutions for elastic scaling perform poorly with graph stream processing for two key reasons. First, when the system is scaled, the graph must be dynamically re-partitioned among workers. This requires a partitioning algorithm that is fast and offers good locality, a task that is far from being trivial. Second, existing modelling techniques for distributed graph processing systems only consider hash partitioning, and do not leverage the semantic knowledge used by more efficient partitioners. In this paper we propose EdgeScaler, an elastic scaler for graph stream processing systems that tackles these challenges by employing, in a synergistic way, two innovative techniques: MicroMacroSplitter and AccuLocal. MicroMacroSplitter is a new edge-based graph partitioning strategy that is as fast as simple hash partinioners, while achieving quality comparable to the best state-of-the-art solutions. AccuLocal is a novel performance model that takes the partitioner features into account while avoiding expensive off-line training phases. An extensive experimental evaluation offers insights on the effectiveness of the proposed mechanisms and shows that EdgeScaler is able to significantly outperform existing solutions designed for generic stream processing systems. © 2020 ACM.},
author_keywords={elastic scaling;  graph processing;  stream processing},
keywords={Semantics;  Software architecture, Experimental evaluation;  Graph Partitioning;  Innovative techniques;  Modelling techniques;  Partitioning algorithms;  Semantic knowledge;  Stream processing;  Stream processing systems, Knowledge management},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Weißbach2020138,
author={Weißbach, M. and Hilbert, H. and Springer, T.},
title={Performance analysis of continuous binary data processing using distributed databases within stream processing environments},
journal={CLOSER 2020 - Proceedings of the 10th International Conference on Cloud Computing and Services Science},
year={2020},
pages={138-149},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088373061&partnerID=40&md5=9179d66536f35b80d01d3c8372956230},
abstract={Big data applications must process increasingly large amounts of data within ever shorter time. Often a stream processing engine (SPE) is used to process incoming data with minimal latency. While these engines are designed to process data quickly, they are not made to persist and manage it. Thus, databases are still integrated into streaming architectures, which often becomes a performance bottleneck. To overcome this issue and achieve maximum performance, all system components used must be examined in terms of their throughput and latency, and how well they interact with each other. Several authors have already analyzed the performance of popular distributed database systems. While doing so, we focus on the interaction between the SPEs and the databases, as we assume that stream processing leads to changes in the access patterns to the databases. Moreover, our main focus is on the efficient storing and loading of binary data objects rather than typed data, since in our use cases the actual data analysis is not to be performed by the database, but by the SPE.We've benchmarked common databases within streaming environments to determine which software combination is best suited for these requirements. Our results show that the database performance differs significantly depending on the access pattern used and that different software combinations lead to substantial performance differences. Depending on the access pattern, Cassandra, MongoDB and PostgreSQL achieved the best throughputs, which were mostly the highest when Apache Flink was used. © Copyright 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Benchmarking;  Big Data;  Database Benchmark;  Performance;  Stream Processing},
keywords={Cloud computing;  Data streams;  Engines, Big data applications;  Database performance;  Distributed database;  Large amounts of data;  Performance analysis;  Performance bottlenecks;  Stream processing engines;  Streaming architecture, Distributed database systems},
publisher={SciTePress},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Twaty20193744,
author={Twaty, M. and Ghrab, A. and Skhiri, S.},
title={GraphOpt: A Framework for Automatic Parameters Tuning of Graph Processing Frameworks},
journal={Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
year={2019},
pages={3744-3753},
doi={10.1109/BigData47090.2019.9006320},
art_number={9006320},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081402903&doi=10.1109%2fBigData47090.2019.9006320&partnerID=40&md5=0f3f277a9490f919a6e3c903c9eed4a2},
abstract={Finding the optimal configuration of a black-box system is a difficult problem that requires a lot of time and human labor. Big data processing frameworks are among the increasingly popular systems whose tuning is a complex and time consuming. The challenge of automatically finding the optimal parameters of big data frameworks attracted a lot of research in recent years. Some of the studies focused on optimizing specific frameworks such as distributed stream processing [1], [2], or finding the best cloud configurations [3], while others proposed general services for optimizing any black-box system [4]. In this paper, we introduce a new use case in the domain of automatic parameter tuning: optimizing the parameters of distributed graph processing frameworks. This task is notably difficult given the particular challenges of distributed graph processing that include the graph partitioning and the iterative nature of graph algorithms. To address this challenge, we designed and implemented GraphOpt: an efficient and scalable black-box optimization framework that automatically tunes distributed graph processing frameworks. GraphOpt implements state-of-the-art optimization algorithms and introduces a new hill-climbing-based search algorithm. These algorithms are used to optimize the performance of two major graph processing frameworks: Giraph and GraphX. Extensive experiments were run on GraphOpt using multiple graph benchmarks to evaluate its performance and show that it provides up to 47.8% improvement compared to random search and an average improvement of up to 5.7%. © 2019 IEEE.},
author_keywords={Black-box Optimization;  Distributed Graph Processing;  Parameters Tuning},
keywords={Benchmarking;  Big data;  Data handling;  Distributed parameter control systems;  Graph algorithms;  Iterative methods, Automatic parameter tuning;  Black-box optimization;  Distributed stream processing;  Graph Partitioning;  Graph processing;  Optimal parameter;  Optimization algorithms;  Parameters tuning, Optimization},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Katragadda2019,
author={Katragadda, S. and Gottumukkala, R. and Venna, S. and Lipari, N. and Gaikwad, S. and Pusala, M. and Chen, J. and Borst, C.W. and Raghavan, V. and Bayoumi, M.},
title={Vastream: A visual analytics system for fast data streams},
journal={ACM International Conference Proceeding Series},
year={2019},
doi={10.1145/3332186.3332256},
art_number={3332256},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070950749&doi=10.1145%2f3332186.3332256&partnerID=40&md5=2f42016e9df2e2251e994fc2746812c6},
abstract={Processing high-volume, high-velocity data streams is an important big data problem in many sciences, engineering, and technology domains. There are many open-source distributed stream processing and cloud platforms that offer low-latency stream processing at scale, but the visualization and user-interaction components of these systems are limited to visualizing the outcome of stream processing results. Visual analysis represents a new form of analysis where the user has more control and interactive capabilities either to dynamically change the visualization, analytics or data management processes. VAStream provides an environment for big data stream processing along with interactive visualization capabilities. The system environment consists of hardware and software modules to optimize streaming data workflow (that includes data ingest, pre-processing, analytics, visualization, and collaboration components). The system environment is evaluated for two real-time streaming applications. The real-time event detection using social media streams uses text data arriving from sources such as Twitter to detect emerging events of interest. The real-time river sensor network analysis project uses unsupervised classification methods to classify sensor network streams arriving from the US river network to detect water quality problems. We discuss implementation details and provide performance comparison results of various individual stream processing operations for both stream processing applications. © 2019 Association for Computing Machinery. All rights reserved.},
author_keywords={Big data infrastructure;  Machine learning;  Stream computing;  Visual analytics;  Visualization},
keywords={Big data;  Data visualization;  Distributed parameter control systems;  Flow visualization;  Information management;  Learning systems;  Open source software;  Open systems;  Quality control;  Rivers;  Sensor networks;  Social networking (online);  Visualization;  Water quality, Data infrastructure;  Distributed stream processing;  Interactive visualizations;  Performance comparison;  Stream computing;  Unsupervised classification;  Visual analytics;  Visual analytics systems, Data streams},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mytilinis201973,
author={Mytilinis, I. and Tsoumakos, D. and Koziris, N.},
title={Maintaining wavelet synopses for sliding-window aggregates},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={73-84},
doi={10.1145/3335783.3335793},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071248183&doi=10.1145%2f3335783.3335793&partnerID=40&md5=c5ac533c770f2ce353bebad5954e6f88},
abstract={The IoT era has brought forth a computing paradigm shift from traditional high-end servers to “edge” devices of limited processing and memory capabilities. These devices, together with sensors, regularly produce very high data volumes nowadays. For many real-time applications, storing and indexing an unbounded stream may not be an option. Thus, it is important that we design algorithms and systems that can both work at the edge of the network and be able to answer queries on distributed, streaming data. Moreover, in many streaming scenarios, fresh data tend to be prioritized. A sliding-window model is an important case of stream processing, where only the most recent elements remain active and the rest are discarded. In this work, we study the problem of maintaining basic aggregate statistics over a sliding-window data stream under the constraint of limited memory. As in IoT scenarios the available memory is typically much less than the window size, queries are answered from compact synopses that are maintained in an on-line fashion. For the efficient construction of such synopses, in this work, we propose wavelet-based algorithms that provide deterministic guarantees and produce almost exact results. Our algorithms can work on any kind of numerical data and do not have the positive-numbers constraint of techniques such as the exponential histograms. Our experimental evaluation indicates that, in terms of accuracy and space-efficiency, our solution outperforms the exponential histograms and deterministic waves techniques. © 2019 Association for Computing Machinery.},
keywords={Aggregates;  Data streams;  Graphic methods;  Management information systems;  Query processing, Deterministic guarantee;  Efficient construction;  Experimental evaluation;  Memory capabilities;  Real-time application;  Sliding window models;  Space efficiencies;  Wavelet based algorithm, Internet of things},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fang2019762,
author={Fang, H. and Zhao, B. and Zhang, X.-W. and Yang, X.-X.},
title={A United Framework for Large-Scale Resource Description Framework Stream Processing},
journal={Journal of Computer Science and Technology},
year={2019},
volume={34},
number={4},
pages={762-774},
doi={10.1007/s11390-019-1941-9},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069474257&doi=10.1007%2fs11390-019-1941-9&partnerID=40&md5=9d19962b361920eb0ecf14be8284eb82},
abstract={Resource description framework (RDF) stream is useful to model spatio-temporal data. In this paper, we propose a framework for large-scale RDF stream processing, LRSP, to process general continuous queries over large-scale RDF streams. Firstly, we propose a formalization (named CT-SPARQL) to represent the general continuous queries in a unified, unambiguous way. Secondly, based on our formalization we propose LRSP to process continuous queries in a common white-box way by separating RDF stream processing, query parsing, and query execution. Finally, we implement and evaluate LRSP with those popular continuous query engines on some benchmark datasets and real-world datasets. Due to the architecture of LRSP, many efficient query engines (including centralized and distributed engines) for RDF can be directly employed to process continuous queries. The experimental results show that LRSP has a higher performance, specially, in processing large-scale real-world data. © 2019, Springer Science+Business Media, LLC & Science Press, China.},
author_keywords={continuous query;  large-scale RDF stream processing (LRSP);  resource description framework (RDF) stream;  stream processing;  united framework},
keywords={Engines, Benchmark datasets;  Continuous queries;  Query execution;  Real-world datasets;  Resource description framework;  Spatio-temporal data;  Stream processing;  united framework, Computer hardware description languages},
publisher={Springer New York LLC},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gokalgandhi2019596,
author={Gokalgandhi, B. and Seskar, I.},
title={Distributed Processing for Encoding and Decoding of Binary LDPC codes using MPI},
journal={INFOCOM 2019 - IEEE Conference on Computer Communications Workshops, INFOCOM WKSHPS 2019},
year={2019},
pages={596-601},
doi={10.1109/INFCOMW.2019.8845079},
art_number={8845079},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073247070&doi=10.1109%2fINFCOMW.2019.8845079&partnerID=40&md5=429dc55926c8a6ca5f08f4702e46836d},
abstract={Low Density Parity Check (LDPC) codes are linear error correcting codes used in communication systems for Forward Error Correction (FEC). But, intensive computation is required for encoding and decoding of LDPC codes, making it difficult for practical usage in general purpose software based signal processing systems. In order to accelerate the encoding and decoding of LDPC codes, distributed processing over multiple multi-core CPUs using Message Passing Interface (MPI) is performed. Implementation is done using Stream Processing and Batch Processing mechanisms and the execution time for both implementations is compared w.r.t variation in number of CPUs and number of cores per CPU. Performance evaluation of distributed processing is shown by variation in execution time w.r.t. increase in number of processors (CPU cores). © 2019 IEEE.},
keywords={Batch data processing;  Decoding;  Encoding (symbols);  Error correction;  Forward error correction;  Message passing;  Program processors, Distributed processing;  Encoding and decoding;  General purpose software;  Linear error-correcting codes;  Low-density parity-check (LDPC) codes;  Message passing interface;  Signal processing systems;  Stream processing, Signal encoding},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Al-Sayeh2019117,
author={Al-Sayeh, H. and Sattler, K.-U.},
title={Gray box modeling methodology for runtime prediction of apache spark jobs},
journal={Proceedings - 2019 IEEE 35th International Conference on Data Engineering Workshops, ICDEW 2019},
year={2019},
pages={117-124},
doi={10.1109/ICDEW.2019.00-23},
art_number={8750932},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069168936&doi=10.1109%2fICDEW.2019.00-23&partnerID=40&md5=d44b987518d7a2121212ff9349e69da5},
abstract={Nowadays, many data centers facilitate data processing and acquisition by developing multiple Apache Spark jobs which can be executed in private clouds with various parameters. Each job might take various application parameters which influence its execution time. Some examples of application parameters can be a selected area of interest in spatiotemporal data processing application or a time range of events in a complex event stream processing application. To predict its runtime accurately, these application parameters shall be considered during constructing its runtime model. Runtime prediction of Spark jobs allows us to schedule them efficiently in order to utilize cloud resources, increase system throughput, reduce job latency and meet customers requirements, e.g. deadlines and QoS. Also, the prediction is considered as important advantage when using a pay-as-you-go pricing model. In this paper, we present a gray box modeling methodology for runtime prediction of each individual Apache Spark job in two steps. The first one is building a white box model for predicting the input RDD size of each stage relying on prior knowledge about its behaviour and taking the application parameters into consideration. The second one is extracting a black box runtime model of each task by observing its runtime metrics according to various allocated resources and variant input RDD sizes. The modeling methodology is validated with experimental evaluation on a real-world application, and the results show a high matching accuracy which reached 83-94% of the actual runtime of the tested application. © 2019 IEEE.},
author_keywords={Large scale data processing;  Modelling;  Runtime Prediction},
keywords={Forecasting;  Models;  Technical presentations, Application parameters;  Experimental evaluation;  Large-scale data processing;  Modeling methodology;  Runtimes;  Spatio-temporal data;  System throughput;  White-box models, Data handling},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zalhan2019,
author={Zalhan, P.-G. and Silaghi, G.C. and Buchmann, R.A.},
title={Marrying big data with smart data in sensor stream processing},
journal={Proceedings of the 28th International Conference on Information Systems Development: Information Systems Beyond 2020, ISD 2019},
year={2019},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091296478&partnerID=40&md5=b8a5d352116092c65783af426e1da37a},
abstract={Widespread deployments of spatially distributed sensors are continuously generating data that require advanced analytical processing and interpretation by machines. Devising machine-interpretable descriptions of sensor data is a key issue in building a semantic stream processing engine. This paper proposes a semantic sensor stream processing pipeline using Apache Kafka to publish and subscribe semantic data streams in a scalable way. We use the Kafka Consumer API to annotate the sensor data using the Semantic Sensor Network ontology, then store the annotated output in an RDF triplestore for further reasoning or semantic integration with legacy information systems. We follow a Design Science approach addressing a Smart Airport scenario with geolocated audio sensors to evaluate the viability of the proposed pipeline under various Kafka-based configurations. Our experimental evaluations show that the multi-broker Kafka cluster setup supports read scalability thus facilitating the parallelization of the semantic enrichment of the sensor data. © ISD 2019. All rights reserved.},
author_keywords={Apache Kafka;  Semantic Sensor Network ontology;  Semantic Stream Processing;  Sensor data},
keywords={Advanced Analytics;  Big data;  Information systems;  Information use;  Legacy systems;  Management information systems;  Pipeline processing systems;  Pipelines;  Semantic Web;  Semantics;  Sensor networks, Experimental evaluation;  Legacy information systems;  Parallelizations;  Semantic enrichment;  Semantic integration;  Spatially distributed sensors;  Stream processing;  Stream processing engines, Data streams},
publisher={Association for Information Systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pieters2019133,
author={Pieters, R.P. and Schrijvers, T.},
title={Faster coroutine pipelines: A reconstruction},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11372 LNCS},
pages={133-149},
doi={10.1007/978-3-030-05998-9_9},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059691498&doi=10.1007%2f978-3-030-05998-9_9&partnerID=40&md5=dc330667d8c11d7145627f43468d6737},
abstract={Spivey has recently presented a novel functional representation that supports the efficient composition, or merging, of coroutine pipelines for processing streams of data. This representation was inspired by Shivers and Might’s three-continuation approach and is shown to be equivalent to a simple yet inefficient executable specification. Unfortunately, neither Shivers and Might’s original work nor the equivalence proof sheds much light on the underlying principles allowing the derivation of this efficient representation from its specification. This paper gives the missing insight by reconstructing a systematic derivation in terms of known transformation steps from the simple specification to the efficient representation. This derivation sheds light on the limitations of the representation and on its applicability to other settings. In particular, it has enabled us to obtain a similar representation for pipes featuring two-way communication, similar to the Haskell pipes library. Our benchmarks confirm that this two-way representation retains the same improved performance characteristics. © Springer Nature Switzerland AG 2019.},
author_keywords={Algebra;  Stream processing;  Structured recursion},
keywords={Algebra;  Benchmarking;  Data handling;  Pipelines;  Specifications, Coroutine;  Executable specifications;  Functional representation;  Performance characteristics;  Stream processing;  Structured recursion;  Two way communications;  Underlying principles, Pipeline processing systems},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhou201835,
author={Zhou, Y. and Wang, P. and Wang, W.},
title={A real-time sensor network aggregation computing system},
journal={Proceedings - 5th IEEE International Conference on Cyber Security and Cloud Computing and 4th IEEE International Conference on Edge Computing and Scalable Cloud, CSCloud/EdgeCom 2018},
year={2018},
pages={35-40},
doi={10.1109/CSCloud/EdgeCom.2018.00016},
art_number={8421849},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051526013&doi=10.1109%2fCSCloud%2fEdgeCom.2018.00016&partnerID=40&md5=a7bd04f469377f88e721d73aac43693b},
abstract={With the advent of Cloud Era, real-time streaming data processing applications are used more and more widely. As a concrete example, real-time queries on a large number of sensors are highly demanding and needs to be carefully designed. Sensor monitoring analysts are inexperienced on coding techniques in spite of the rapid development of real-time processing platforms and communities. What's more, the state-of-the-art systems lacks support for multi-query processing and sharing of the original data and results with prior information of the queries. To tackle the challenges, we endeavor to design a light-weight but effective scheme to support multiple queries running on a large number of sensors. We focus on aggregations on sliding windows due to its practical importance, and hence design a query language, SAQL, for the professional analysts to construct their own streaming computational logic. We also propose the whole system, SAQS, which can interpret SAQL into stream processing programs and support the efficient execution of multiple queries in a distributed real-time environment. SAQS can detect the data sharing at different granularity and uses a greedy partition algorithm to balance the load among worker nodes. Our extensive experimental evaluations establish the convenience and efficiency of SAQS. © 2018 IEEE.},
author_keywords={Aggregation;  Apache Storm;  Cloud computing;  Query optimization;  Sensor monitoring;  Sliding window;  Streaming},
keywords={Acoustic streaming;  Agglomeration;  Cloud computing;  Communication channels (information theory);  Computation theory;  Data handling;  Decoding;  Edge computing;  Logic programming;  Query languages;  Search engines;  Sensor networks, Apache storms;  Different granularities;  Experimental evaluation;  Query optimization;  Real-time sensor networks;  Sensor monitoring;  Sliding Window;  State-of-the-art system, Query processing},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hoseinyfarahabady2018554,
author={Hoseinyfarahabady, M.R. and Farhangsadr, N. and Zomaya, A.Y. and Tari, Z. and Khan, S.U.},
title={Elastic CPU cap mechanism for timely dataflow applications},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10860 LNCS},
pages={554-568},
doi={10.1007/978-3-319-93698-7_42},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048951341&doi=10.1007%2f978-3-319-93698-7_42&partnerID=40&md5=f654e28c6c55e7999331dad65a2e05f9},
abstract={Sudden surges in the incoming workload can cause adverse consequences on the run-time performance of data-flow applications. Our work addresses the problem of limiting CPU associated with the elastic scaling of timely data-flow (TDF) applications running in a shared computing environment while each application can possess a different quality of service (QoS) requirement. The key argument here is that an unwise consolidation decision to dynamically scale up/out the computing resources for responding to unexpected workload changes can degrade the performance of some (if not all) collocated applications due to their fierce competition getting the shared resources (such as the last level cache). The proposed solution uses a queue-based model to predict the performance degradation of running data-flow applications together. The problem of CPU cap adjustment is addressed as an optimization problem, where the aim is to reduce the quality of service violation incidents among applications while raising the CPU utilization level of server nodes as well as preventing the formation of bottlenecks due to the fierce competition among collocated applications. The controller uses and efficient dynamic method to find a solution at each round of the controlling epoch. The performance evaluation is carried out by comparing the proposed controller against an enhanced QoS-aware version of round robin strategy which is deployed in many commercial packages. Experimental results confirmed that the proposed solution improves QoS satisfaction by near to 148% on average while it can reduce the latency of processing data records for applications in the highest QoS classes by near to 19% during workload surges. © Springer International Publishing AG, part of Springer Nature 2018.},
author_keywords={Distributed stream processing;  Scheduling and resource allocation algorithms;  Shared resource interference},
keywords={Data flow analysis;  Data handling;  Data transfer;  Distributed parameter control systems;  Routers, Computing environments;  Distributed stream processing;  Optimization problems;  Performance degradation;  Performance evaluations;  Qualityof-service requirement (QoS);  Resource allocation algorithms;  Shared resources, Quality of service},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yang2018141,
author={Yang, S. and Jeong, Y. and Hong, C. and Jun, H. and Burgstaller, B.},
title={Scalability and state: A critical assessment of throughput obtainable on big data streaming frameworks for applications with and without state information},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10659 LNCS},
pages={141-152},
doi={10.1007/978-3-319-75178-8_12},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042491933&doi=10.1007%2f978-3-319-75178-8_12&partnerID=40&md5=dd869a5795ca68dada3ea5efc05cf352},
abstract={Emerging Big Data streaming applications are facing unbounded (infinite) data sets at a scale of millions of events per second. The information captured in a single event, e.g., GPS position information of mobile phone users, loses value (perishes) over time and requires sub-second latency responses. Conventional Cloud-based batch-processing platforms are inadequate to meet these constraints. Existing streaming engines exhibit low throughput and are thus equally ill-suited for emerging Big Data streaming applications. To validate this claim, we evaluated the Yahoo streaming benchmark and our own real-time trend detector on three state-of-the-art streaming engines: Apache Storm, Apache Flink and Spark Streaming. We adapted the Kieker dynamic profiling framework to gather accurate profiling information on the throughput and CPU utilization exhibited by the two benchmarks on the Google Compute Engine. To estimate the performance overhead incurred by current streaming engines, we re-implemented our Java-based trend detector as a multi-threaded, shared-memory application in C++. The achieved throughput of 3.2 million events per second on a stand-alone 2 CPU (44 cores) Intel Xeon E5-2699 v4 server is 44 times higher than the maximum throughput achieved with the Apache Storm version of the trend detector deployed on 30 virtual machines (nodes) in the Cloud. Our experiment suggests vertical scaling as a viable alternative to horizontal scaling, especially if shared state has to be maintained in a streaming application. For reproducibility, we have open-sourced our framework configurations on GitHub [1]. © Springer International Publishing AG, part of Springer Nature 2018.},
keywords={Batch data processing;  Data reduction;  Engines;  Heterojunction bipolar transistors;  Storms;  Throughput, Critical assessment;  Horizontal scaling;  Maximum through-put;  Mobile-phone users;  Position information;  Processing platform;  Profiling informations;  Streaming applications, Big data},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Walulya2018129,
author={Walulya, I. and Nikolakopoulos, Y. and Gulisano, V. and Papatriantafilou, M. and Tsigas, P.},
title={Viper: Communication-layer determinism and scaling in low-latency stream processing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10659 LNCS},
pages={129-140},
doi={10.1007/978-3-319-75178-8_11},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042473351&doi=10.1007%2f978-3-319-75178-8_11&partnerID=40&md5=c36e1e0039b607041d92a6a3c7a82934},
abstract={Stream Processing Engines (SPEs) process continuous streams of data and produce up-to-date results in a real-time fashion, typically through one-at-a-time tuple analysis. When looking into the vital SPE processing properties required from applications, determinism has a strong position besides scalability in throughput and low processing latency. SPEs scale in throughput and latency by relying on shared-nothing parallelism, deploying multiple copies of each operator to which tuples are distributed based on the semantics of the operator. The coordination of the asynchronous analysis of parallel operators required to enforce determinism is then carried out by additional dedicated sorting operators. In this work we shift such costly coordination to the communication layer of the SPE. Specifically, we extend earlier work on shared-memory implementations of deterministic operators and provide a communication module (Viper) which can be integrated in the SPE communication layer. Using Apache Storm and the Linear Road benchmark, we show the benefits that can be achieved by our approach in terms of throughput and energy efficiency of SPEs implementing one-at-a-time analysis. © Springer International Publishing AG, part of Springer Nature 2018.},
author_keywords={Data streaming;  Low-latency shared-nothing and shared-memory parallelism stream processing engines},
keywords={Energy efficiency;  Engines;  Memory architecture;  Semantics, Communication layers;  Communication modules;  Data streaming;  Parallel operators;  Processing properties;  Shared nothing;  Stream processing;  Stream processing engines, Throughput},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chakraborty201721,
author={Chakraborty, R. and Majumdar, S.},
title={Priority based resource scheduling techniques for a resource constrained stream processing system},
journal={BDCAT 2017 - Proceedings of the 4th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
year={2017},
pages={21-31},
doi={10.1145/3148055.3148066},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058316550&doi=10.1145%2f3148055.3148066&partnerID=40&md5=8b016264c73297826adb0a6dcbccf801},
abstract={A multitenant Storm cluster runs multiple stream processing applications and uses the default Isolation Scheduler to schedule them. Isolation Scheduler assigns resources to topologies based on static resource configuration and does not provide any means for prioritizing topologies based on their varying business requirements. Thus, performance degradation, even complete starvation of topologies with high priority is possible when the cluster is resource constrained and comprises an inadequate number of resources. Two priority based resource scheduling techniques are proposed to overcome these problems. A performance analysis based on prototyping and measurements demonstrates the effectiveness of the proposed techniques. © 2017 ACM.},
author_keywords={Big data;  Distributed systems;  Middleware;  Resource allocation;  Resource management;  Resource scheduling;  Stream processing},
keywords={Big data;  Middleware;  Resource allocation;  Scheduling;  Topology, Business requirement;  Distributed systems;  Performance analysis;  Performance degradation;  Resource management;  Resource-scheduling;  Stream processing;  Stream processing systems, Information management},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2017497,
author={Wang, G. and Wada, K. and Yamagiwa, S.},
title={Performance evaluation of parallelizing algorithm using spanning tree for stream-based computing},
journal={Proceedings - 2016 4th International Symposium on Computing and Networking, CANDAR 2016},
year={2017},
pages={497-503},
doi={10.1109/CANDAR.2016.62},
art_number={7818662},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015152091&doi=10.1109%2fCANDAR.2016.62&partnerID=40&md5=a2f144d6dbf3db3750df6db86c0418d8},
abstract={This paper proposes a detailed performance evaluation of an algorithm using spanning tree that automatically exploits the parallelism and determines an execution order of multiple kernel programs in distributed environment. In stream-based computing, efficient parallel execution requires careful scheduling of the invocation of the kernel programs. By mapping a kernel to a node and an I/O stream between kernels to an edge, the entire stream process can be treated as a spanning tree. The spanning tree, which allows feedback and feedforward edges, is effective for expressing dependencies that exist among kernels. In spanning tree, the nodes at the same depth do not have edges between them, and thus can be executed in parallel in the case parent nodes have been already executed. The series of the nodes can be executed in a pipelined manner. Thus, the proposed algorithm can extract both spatial and temporal parallelism. To evaluate the effectiveness of the proposed algorithm, two applications have been developed and parallelized based on the proposed algorithm. The results show that the parallel execution using four nodes of a GPU cluster obtained 3.5 times speedup in 2D-FFT and 3.0 times speedup in LU decomposition, compared to the sequential execution. © 2016 IEEE.},
author_keywords={High performance computing;  Multiple GPUs;  Parallelizing algorithm;  Spanning tree;  Stream computing},
keywords={Program processors, High performance computing;  Multiple GPUs;  Parallelizing;  Spanning tree;  Stream computing, Trees (mathematics)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schäfer2017289,
author={Schäfer, P. and Leser, U.},
title={Benchmarking univariate time series classifiers},
journal={Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
year={2017},
volume={265},
pages={289-298},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045399136&partnerID=40&md5=24b05962b223958918616cd77a04214c},
abstract={Time series are a collection of values sequentially recorded over time. Nowadays, sensors for recording time series are omnipresent as RFID chips, wearables, smart homes, or event-based systems. Time series classification aims at predicting a class label for a time series whose label is unknown. Therefore, a classifier has to train a model using labeled samples. Classification time is a key challenge given new applications like event-based monitoring, real-time decision or streaming systems. This paper is the first benchmark that compares 12 state of the art time series classifiers based on prediction and classification times. We observed that most of the state-of-the-art classifiers require extensive train and classification times, and might not be applicable for these new applications. © 2017 Gesellschaft fur Informatik (GI). All rights reserved.},
author_keywords={Benchmark;  Classification;  Scalability;  Time series},
keywords={Automation;  Benchmarking;  Classification (of information);  Intelligent buildings;  Scalability;  Time series, Classification time;  Event-based monitoring;  Event-based system;  New applications;  Real time decisions;  Streaming systems;  Time series classifications;  Univariate time series, Real time systems},
publisher={Gesellschaft fur Informatik (GI)},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sahni2017399,
author={Sahni, J. and Vidyarthi, D.P.},
title={Scalable online analytics on cloud infrastructures},
journal={Communications in Computer and Information Science},
year={2017},
volume={721},
pages={399-408},
doi={10.1007/978-981-10-5427-3_43},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028323850&doi=10.1007%2f978-981-10-5427-3_43&partnerID=40&md5=394cc1218742cd903e20e659fc14acc2},
abstract={The need for low latency analysis of high velocity real time continuous data streams has led to the emergence of Stream Processing Systems (SPSs). Contemporary SPSs allow a stream processing application to be hosted on Cloud infrastructures and dynamically scaled so as to adapt to the fluctuating data rates. However, the run time scalability incorporated in these SPSs are in their early adaptations and are based on simple local/global threshold based controls. This work studies the issues with the local and global auto scaling techniques that may lead to performance inefficiencies in real time traffic analysis on Cloud platforms and presents an efficient hybrid auto scaling strategy StreamScale which addresses the identified issues. The proposed StreamScale auto-scaling algorithm accounts for the gaps in the local/global scaling approaches and effectively identifies (de)parallelization opportunities in stream processing applications for maintaining QoS at reduced costs. Simulation based experimental evaluation on representative stream application topologies indicate that the proposed StreamScale auto-scaling algorithm exhibits better performance in comparison to both local and global auto-scaling approaches. © Springer Nature Singapore Pte Ltd. 2017.},
author_keywords={Cloud computing;  Internet of Things (IoT);  Online analytics;  Scalability;  Stream Processing Systems (SPS)},
keywords={Cloud computing;  Continuous time systems;  Internet of things;  Online systems;  Platform as a Service (PaaS);  Real time systems;  Scalability, Cloud infrastructures;  Experimental evaluation;  Internet of Things (IOT);  Online analytics;  Real time traffics;  Scaling algorithm;  Stream application;  Stream processing systems, Distributed computer systems},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zacheilas201719,
author={Zacheilas, N. and Kalogeraki, V.},
title={DIsCO: Dynamic data compression in distributed stream processing systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10320 LNCS},
pages={19-33},
doi={10.1007/978-3-319-59665-5_2},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020497365&doi=10.1007%2f978-3-319-59665-5_2&partnerID=40&md5=5349a8259d010fc642bb89cd7fb858e8},
abstract={Supporting high throughput in Distributed Stream Processing Systems (DSPSs) has been an important goal in recent years. Current works either focus on automatically increasing the system resources whenever the current setup is inadequate or apply load shedding techniques discarding some of the incoming data. However, both approaches have significant shortcomings as they require on the fly application reconfiguration where the application needs to be stopped and re-uploaded in the cluster with the new configurations, and can lead to significant information loss. One approach that has not yet been considered for improving the throughput of DSPSs is exploiting compression algorithms to minimize the communication overhead between components especially in cases where we have large-sized data like live CCTV camera reports. This work is the first that provides a novel framework, built on top of Apache Storm, which enables dynamic compression of incoming streaming data. Our approach uses a profiling algorithm to automatically determine the compression algorithm that should be applied and supports both lossless and lossy compression techniques. Furthermore, we propose a novel algorithm for determining when profiling should be applied. Finally, our detailed experimental evaluation with commonly used stream processing applications, indicates a clear improvement on the applications’ throughput when our proposed techniques are applied. © IFIP International Federation for Information Processing 2017.},
keywords={Data compression;  Distributed parameter control systems;  Interoperability;  Throughput, Application reconfiguration;  Communication overheads;  Compression algorithms;  Distributed stream processing;  Dynamic compression;  Experimental evaluation;  Lossy compressions;  Stream processing, Distributed computer systems},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Simmhan201621,
author={Simmhan, Y. and Shukla, A. and Verma, A.},
title={Benchmarking fast-data platforms for the aadhaar biometric database},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={10044},
pages={21-39},
doi={10.1007/978-3-319-49748-8_2},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004098332&doi=10.1007%2f978-3-319-49748-8_2&partnerID=40&md5=0b966439d2cc44557c90980817d56b95},
abstract={Aadhaar is the world’s largest biometric database with a billion records, being compiled as an identity platform to deliver social services to residents of India. Aadhaar processes streams of biometric data as residents are enrolled and updated. Besides ∼1 million enrollments and updates per day, up to 100 million daily biometric authentications are expected during delivery of various public services. These form critical Big Data applications, with large volumes and high velocity of data. Here, we propose a stream processing workload, based on the Aadhaar enrollment and Authentication applications, as a Big Data benchmark for distributed stream processing systems. We describe the application composition, and characterize their task latencies and selectivity, and data rate and size distributions, based on real observations. We also validate this benchmark on Apache Storm using synthetic streams and simulated application logic. This paper offers a unique glimpse into an operational national identity infrastructure, and proposes a benchmark for “fast data” platforms to support such eGovernance applications. © Springer International Publishing AG 2016.},
keywords={Authentication;  Benchmarking;  Biometrics;  Distributed parameter control systems, Application composition;  Application logic;  Big data applications;  Biometric authentication;  Biometric database;  Distributed stream processing;  Public services;  Stream processing, Big data},
publisher={Springer Verlag},
language={English},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Gradvohl2016143,
author={Gradvohl, A.L.S.},
title={Investigating metrics to build a benchmark tool for complex event processing systems},
journal={Proceedings - 2016 4th International Conference on Future Internet of Things and Cloud Workshops, W-FiCloud 2016},
year={2016},
pages={143-147},
doi={10.1109/W-FiCloud.2016.40},
art_number={7592714},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009799667&doi=10.1109%2fW-FiCloud.2016.40&partnerID=40&md5=e4ed9e56e478ce706394b91c1b43a0d8},
abstract={Despite companies' demand for data streams processing systems to handle large volumes of flowing data, we did not find many software to assess these sort of systems. In fact, up to date, there are few papers proposing metrics to evaluate these systems or describing software for benchmarks. Most of the papers focus on metrics such as throughput, latency and memory consumption. However, there are other metrics, which system administrators and users should consider, such as information latency, the correctness of results, adaptability on different workloads and others. Therefore, in this paper, we summarized some key metrics used to assess systems for processing online data streams. In addition, we discuss three benchmark tools found in the literature to assess this type of system. At the end of this paper, we propose a new benchmark tool for complex event processing distributed systems called B2-4CEP, which incorporate the metrics described in this paper. © 2016 IEEE.},
author_keywords={Benchmark;  Complex event processing;  Metrics},
keywords={Benchmarking;  Data communication systems;  Internet of things;  Online systems, Complex event processing;  Data streams processing;  Distributed systems;  Information latency;  Large volumes;  Memory consumption;  Metrics;  System administrators, Data handling},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zaichenkov2016,
author={Zaichenkov, P. and Tveretina, O. and Shafarenko, A. and Gijsbers, B. and Grelck, C.},
title={The cost and benefits of coordination programming: Two case studies in concurrent collections and S-NET},
journal={Parallel Processing Letters},
year={2016},
volume={26},
number={3},
doi={10.1142/S0129626416500110},
art_number={1650011},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988615338&doi=10.1142%2fS0129626416500110&partnerID=40&md5=5bdbf083cb3006258ab6195445283466},
abstract={This is an evaluation study of the expressiveness provided and the performance delivered by the coordination language S-NET in comparison to Intel's Concurrent Collections (CnC). An S-NET application is a network of black-box compute components connected through anonymous data streams, with the standard input and output streams linking the application to the environment. Our case study is based on two applications: a face detection algorithm implemented as a pipeline of feature classifiers and a numerical algorithm from the linear algebra domain, namely Cholesky decomposition. The selected applications are representative and have been selected by Intel researchers as evaluation testbeds for CnC in the past. We implement various versions of both algorithms in S-NET and compare them with equivalent CnC implementations, both with and without tuning, previously published by the CnC community. Our experiments on a large-scale server system demonstrate that S-Net delivers very similar scalability and absolute performance on the studied examples as tuned CnC codes do, even without specific tuning. At the same time, S-Net does achieve a much more complete separation of concerns between compute and coordination layers than CnC even intends to. © 2016 World Scientific Publishing Company.},
author_keywords={coordination programming;  Performance measurement;  stream processing},
keywords={Face recognition;  Linear algebra, Absolute performance;  Cholesky decomposition;  Coordination language;  Coordination programming;  Face detection algorithm;  Performance measurements;  Separation of concerns;  Stream processing, Telecommunication networks},
publisher={World Scientific Publishing Co. Pte Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vögler2016190,
author={Vögler, M. and Schleicher, J.M. and Inzinger, C. and Nickel, B. and Dustdar, S.},
title={Non-intrusive monitoring of stream processing applications},
journal={Proceedings - 2016 IEEE Symposium on Service-Oriented System Engineering, SOSE 2016},
year={2016},
pages={190-199},
doi={10.1109/SOSE.2016.11},
art_number={7473022},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979072512&doi=10.1109%2fSOSE.2016.11&partnerID=40&md5=1297ae3c893d19508f3a2b580848f71d},
abstract={Stream processing applications have emerged as a popular way for implementing high-volume data processing tasks. In contrast to traditional data processing models that persist data to databases and then execute queries on the stored data, stream processing applications continuously execute complex queries on incoming data to produce timely results in reaction to events observed in the processed data. To cope with the request load, components of a stream processing application are usually distributed across multiple machines. In this context, performance monitoring and testing are naturally important for stakeholders to understand as well as analyze the runtime characteristics of deployed applications to identify issues and inform decisions. Existing approaches for monitoring the performance of distributed systems, however, do not provide sufficient support for targeted monitoring of stream processing applications, and require changes to the application code to enable the integration of application-specific monitoring data. In this paper we present MOSAIC, a service oriented framework that allows for in-depth analysis of stream processing applications by non-intrusively adding functionality for acquiring and publishing performance measurements at runtime, to the application. Furthermore, MOSAIC provides a flexible mechanism for integrating different stream processing frameworks, which can be used for executing and monitoring applications independent from a specific operator model. Additionally, our framework provides an extensible approach for gathering and analyzing measurement data. In order to evaluate our solution, we developed a scenario application, which we used for testing and monitoring its performance on different stream processing engines. © 2016 IEEE.},
author_keywords={AOP;  Non-intrusive monitoring;  Service oriented framework;  Stream processing applications},
keywords={Monitoring;  Systems engineering, Data processing models;  Monitoring applications;  Non-intrusive monitoring;  Performance measurements;  Performance monitoring;  Service oriented frameworks;  Stream processing;  Stream processing engines, Data handling},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Maamar20161769,
author={Maamar, H.R. and Boukerche, A. and Petriu, E.},
title={A performance evaluation of mobility management and multihop supplying partner strategies for 3D streaming systems over thin mobile devices},
journal={Concurrency and Computation: Practice and Experience},
year={2016},
volume={28},
number={6},
pages={1769-1795},
doi={10.1002/cpe.3106},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886886234&doi=10.1002%2fcpe.3106&partnerID=40&md5=313268cdf0d405c83c556f60069a1609},
abstract={The recent advances in technology and mobile computing led to the rapid growth of networked 3D streaming applications. The emerging services can involve augmented reality, virtual environment walkthrough, multiplayer gaming just to mention a few. Because of the limited network bandwidth of the client-server approach, research works are now turning toward mobile ad hoc networks-based streaming, where the resources of each peer are used during the streaming service. Peer-to-peer technologies are considering the solution to adapt for scalable applications. Yet, supplying partner selection and 3D data delivery are still significant challenges to face because of the dynamic wireless environment that causes link breakages, high packet loss, an adverse impact on the quality of the 3D media, and a low user satisfaction. In this paper, we propose a supplying partner selection technique coupled with a content delivery technique for peer-to-peer 3D streaming over thin mobile devices. Our proposed protocol, which we refer to as MULTIPLY, considers multihop suppliers in order to alleviate the load on the server and uses the signal strength measurement to analyze the wireless link when sending back the 3D data. Given the high dynamicity of the network due to the mobility of the users, the streaming can be greatly affected. We therefore study the impact of the mobility on MULTIPLY. The performance evaluation of our protocol obtained using an extensive set of simulation experiments is then reported. Copyright © 2013 John Wiley & Sons, Ltd.},
author_keywords={3D streaming;  mobile augmented reality;  wireless networks},
keywords={Augmented reality;  Mobile ad hoc networks;  Wireless networks, 3D streaming;  Mobile augmented reality;  Mobility management;  Multi-player gaming;  Partner selection;  Peer-to-peer technologies;  Signal strength measurements;  Wireless environment, Peer to peer networks},
publisher={John Wiley and Sons Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rattanaopas2016,
author={Rattanaopas, K. and Tandayya, P.},
title={Performance analysis of a multi-tier video on demand service on virtualization platforms},
journal={ICSEC 2015 - 19th International Computer Science and Engineering Conference: Hybrid Cloud Computing: A New Approach for Big Data Era},
year={2016},
doi={10.1109/ICSEC.2015.7401437},
art_number={7401437},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964350719&doi=10.1109%2fICSEC.2015.7401437&partnerID=40&md5=892fdf0ea040befe50eda170dbec5b51},
abstract={Cloud computing technology, especially virtualization is employed in many data centers nowadays. The key concept of virtualization concerns elastic or scalable infrastructure. This concept can be implemented by exploiting multi-tier web applications and hypervisors which are virtual machine management software. Xen hypervisor has presented its Version 4.4 in 2014. In this paper, we present the performance analysis comparison between Xen para-virtualization and KVM full virtualization on the case study of open source video streaming called Cumulusclips, a YouTube-like system. This investigation involves real workload mp4 video streaming on 200 clients' browser, running 3 experiments, including large video files (∼3 GB), small video files (∼120 MB) and random-size video files (the ratio of large and small video files is 25%/75%). The requests size results show that Xen para-virtualization can serve all requests better than KVM full virtualization and use less resource. Xen's performance is dropped when CPU usage is 100% in Experiment 1 (large files). In Experiments 2 (small files) and 3 (random-size files), Xen's CPU usage is under 10%, but KVM's CPU usage is over 50%. The requests size results of Xen and KVM are equal in Experiment 2, but Xen has the maximum throughput about a half (51%) of KVM's. Therefore, we can conclude that Xen para-virtualization has better performance than KVM full virtualization on multi-tier video streaming system. © 2015 IEEE.},
author_keywords={Cloud computing;  KVM;  Multi-tier applications;  performance comparison;  Video Streaming;  virtualization;  Xen},
keywords={Application programs;  Big data;  Cloud computing;  Communication channels (information theory);  Java programming language;  Open source software;  Open systems;  Video on demand;  Video streaming, Cloud computing technologies;  Multi-tier applications;  Performance analysis;  Performance comparison;  Scalable infrastructure;  Video on demand services;  Virtual machine management;  Virtualizations, Virtual reality},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rafailidis20152836,
author={Rafailidis, D. and Antaris, S.},
title={Indexing media storms on Flink},
journal={Proceedings - 2015 IEEE International Conference on Big Data, IEEE Big Data 2015},
year={2015},
pages={2836-2838},
doi={10.1109/BigData.2015.7364094},
art_number={7364094},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963733615&doi=10.1109%2fBigData.2015.7364094&partnerID=40&md5=f00afccea48379ac4f80511e2bb70a76},
abstract={We propose a media storm indexing algorithm using Map-Reduce in our recently proposed CDVC framework. In this study, CDVC is built on Flink, an open-source platform for stream data processing. The question we answer is how to store massive image collections; for instance, with over one million images per second, as well as with varying incoming rate. In our experiments with two benchmark datasets of 80M and 1B image descriptors, we evaluate the proposed algorithm on different indexing workloads, that is, images that come with high volume and different velocity at the scale of 105-106 images per second. Using a limited set of computational nodes, we show that we achieve a significant speed up factor of nine, on average, compared to conventional indexing techniques, in all settings. Finally, we make our source code publicly available. © 2015 IEEE.},
author_keywords={cloud computing;  indexing;  multimedia big data;  Stream processing},
keywords={Algorithms;  Cloud computing;  Data handling;  Indexing (materials working);  Indexing (of information);  Open source software;  Storms, Benchmark datasets;  Computational nodes;  Image descriptors;  Indexing algorithms;  Indexing techniques;  Open source platforms;  Stream data processing;  Stream processing, Big data},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dilawari20131878,
author={Dilawari, A. and Tahir, M.},
title={Optimal flow splitting for multi-path multi-interface wireless data streaming networks},
journal={IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, PIMRC},
year={2013},
pages={1878-1882},
doi={10.1109/PIMRC.2013.6666449},
art_number={6666449},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893324311&doi=10.1109%2fPIMRC.2013.6666449&partnerID=40&md5=313b8c0fe4bee63dc52673c3614ff2b9},
abstract={We propose optimal flow splitting for wireless data streaming networks consisting of nodes that are equipped with more than one wireless communication interface. The objective is to efficiently distribute data traffic among multiple wireless interfaces, leveraging an additional axis of freedom capable of providing improved performance. For that purpose a distributed framework based on convex optimization is developed that achieves optimal resource utilization for multi-interface multi-path network configuration. Although flow splitting can be performed using single wireless interface, our performance evaluation results show an improved performance when employing multiple wireless interfaces. Performance evaluation results reveal that 50% energy saving can be achieved when using optimal flow splitting for dual interface compared to single interface node architecture. In addition, the proposed solution is inherently robust to link outages due to the availability of multiple wireless interfaces. The proposed framework is quite flexible and can be easily extended to integrate any other performance parameters of interest. © 2013 IEEE.},
keywords={Distributed framework;  Evaluation results;  Flow splitting;  Network configuration;  Performance parameters;  Resource utilizations;  Wireless communications;  Wireless interfaces, Convex optimization;  Data reduction;  Wireless telecommunication systems, Optimization},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Allani2013423,
author={Allani, M. and Garbinato, B. and Pietzuch, P.},
title={Hyphen: A hybrid protocol for generic overlay construction in P2P environments},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2013},
pages={423-430},
doi={10.1145/2480362.2480448},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878000141&doi=10.1145%2f2480362.2480448&partnerID=40&md5=591d1ca273ca8fefdeee63f83b1bcc78},
abstract={Overlay networks form the core part of peer-to-peer (P2P) applications such as application-level multicast, content distribution and media streaming. To ease development, middleware solutions and toolkit libraries have been proposed in the past to help with the implementation of overlay networks. Existing solutions, however, are either too generic by only providing low-level communication abstractions, requiring developers to implement algorithms for overlay networks from scratch, or too restrictive by only supporting a particular overlay topology with fixed properties. In this paper, we argue that it is possible to find a middle ground between these two extremes. We describe Hyphen, a middleware for overlay construction and maintenance that supports a range of overlay topologies with custom properties, and show how it can replace topology construction for a variety of application-level multicast systems. Unlike previous efforts, Hyphen can construct and maintain a range of overlay topologies such as trees and forests with specific optimisation goals such as low latency or high bandwidth. By using a gossip-based mechanism to define topologies implicitly, Hyphen can scale to many peers and achieve low construction overhead. Our experimental evaluation with Bullet and Splitstream, two P2P streaming systems, shows that Hyphen can construct a bandwidth-optimised tree for Bullet that achieves a higher streaming rate than the original Bullet implementation, and that it can construct a more reliable forest for Splitstream by taking individual peer reliability into account. Copyright 2013 ACM.},
keywords={Application-level multicast;  Communication abstraction;  Content distribution;  Experimental evaluation;  Overlay construction;  P2p streaming systems;  Peer-to-peer application;  Topology construction, Bandwidth;  Distributed computer systems;  Forestry;  Middleware;  Multicasting;  Overlay networks;  Topology, Peer to peer networks, Algorithms;  Communication;  Computation},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fernandez201311,
author={Fernandez, R.C. and Migliavacca, M. and Kalyvianaki, E. and Pietzuch, P.},
title={Scalable and fault-tolerant stateful stream processing},
journal={OpenAccess Series in Informatics},
year={2013},
volume={35},
pages={11-18},
doi={10.4230/OASIcs.ICCSW.2013.11},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905817712&doi=10.4230%2fOASIcs.ICCSW.2013.11&partnerID=40&md5=a40504f778f55a59ffd3af7f411a7001},
abstract={As users of "big data" applications expect fresh results, we witness a new breed of stream processing systems (SPS) that are designed to scale to large numbers of cloud-hosted machines. Such systems face new challenges: (i) to benefit from the "pay-as-you-go" model of cloud computing, they must scale out on demand, acquiring additional virtual machines (VMs) and parallelising operators when the workload increases; (ii) failures are common with deployments on hundreds of VMs-systems must be fault-tolerant with fast recovery times, yet low per-machine overheads. An open question is how to achieve these two goals when stream queries include stateful operators, which must be scaled out and recovered without affecting query results. Our key idea is to expose internal operator state explicitly to the SPS through a set of state management primitives. Based on them, we describe an integrated approach for dynamic scale out and recovery of stateful operators. Externalised operator state is checkpointed periodically by the SPS and backed up to upstream VMs. The SPS identifies individual operator bottlenecks and automatically scales them out by allocating new VMs and partitioning the checkpointed state. At any point, failed operators are recovered by restoring checkpointed state on a new VM and replaying unprocessed tuples. We evaluate this approach with the Linear Road Benchmark on the Amazon EC2 cloud platform and show that it can scale automatically to a load factor of L=350 with 50 VMs, while recovering quickly from failures.},
author_keywords={Fault tolerance;  Scalability;  Stateful stream processing},
keywords={Big data;  Fault tolerance;  Scalability;  Students, Cloud platforms;  Fault-tolerant;  Integrated approach;  Query results;  State management;  Stream processing;  Stream processing systems;  Virtual machines, Recovery},
publisher={Schloss Dagstuhl- Leibniz-Zentrum fur Informatik GmbH, Dagstuhl Publishing},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Burtsev2013,
author={Burtsev, A. and Mishrikoti, N. and Eide, E. and Ricci, R.},
title={Weir: A streaming language for performance analysis},
journal={Proceedings of the 7th Workshop on Programming Languages and Operating Systems, PLOS 2013 - In Conjunction with the 24th ACM Symposium on Operating Systems Principles, SOSP 2013},
year={2013},
doi={10.1145/2525528.2525537},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897369792&doi=10.1145%2f2525528.2525537&partnerID=40&md5=bed6c8c5700259a8e879b525136e6716},
abstract={For modern software systems, performance analysis can be a challenging task. The software stack can be a complex, multi-layer, multi-component, concurrent, and parallel environment with multiple contexts of execution and multiple sources of performance data. Although much performance data is available, because modern systems incorporate many mature data-collection mechanisms, analysis algorithms suffer from the lack of a unifying programming environment for processing the collected performance data, potentially from multiple sources, in a convenient and script-like manner. This paper presents Weir, a streaming language for systems performance analysis. Weir is based on the insight that performance-analysis algorithms can be naturally expressed as stream-processing pipelines. In Weir, an analysis algorithm is implemented as a graph composed of stages, where each stage operates on a stream of events that represent collected performance measurements. Weir is an imperative streaming language with a syntax designed for the convenient construction of stream pipelines that utilize composable and reusable analysis stages. To demonstrate practical application, this paper presents the authors' experience in using Weir to analyze performance in systems based on the Xen virtualization platform. © 2013 ACM.},
keywords={Algorithms;  Computer hardware description languages;  Computer software reusability;  Hydraulic structures;  Pipeline processing systems;  Pipelines, Analysis algorithms;  Multiple contexts;  Parallel environment;  Performance analysis;  Performance data;  Performance measurements;  Programming environment;  Systems performance analysis, Weirs},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yamamoto2013,
author={Yamamoto, M. and Koizumi, H.},
title={An experimental evaluation of distributed data stream processing using lightweight RDBMS SQLite},
journal={IEEJ Transactions on Electronics, Information and Systems},
year={2013},
volume={133},
number={11},
pages={2125-2132+16},
doi={10.1541/ieejeiss.133.2125},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887246531&doi=10.1541%2fieejeiss.133.2125&partnerID=40&md5=46c0cfda3c4a68db11212a9f9fe60115},
abstract={The data generated at a very high rate by sensors and RFIDs are required to be handled by continuous queries keeping real time response. DSMSs (Data Stream Management systems) are used in several cases of these large scale systems for its purpose. This paper describes the results of experimental evaluation of distributed data stream processing using SQLite which is a lightweight RDBMS as a stream processing engine node. In particular, it is necessary to correspond also to the increasing data rate flexibly in data stream processing. This paper proved that it could respond to the increase in a data rate by adding a processing node dynamically. In that case, it is necessary to perform synchronous processing in two or more processing nodes. Therefore, within the limits of this experiment, it turned out that it is realizable by performing synchronous processing by the side which divides and passes data. © 2013 The Institute of Electrical Engineers of Japan.},
author_keywords={Data stream processing;  Distributed data stream processing;  DSMS},
keywords={Distributed parameter control systems, Continuous queries;  Data stream management systems;  Data stream processing;  Distributed data stream processing;  DSMS;  Experimental evaluation;  Real time response;  Stream processing engines, Data communication systems},
publisher={Institute of Electrical Engineers of Japan},
language={English; Japanese},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2012961,
author={Zhang, M. and Zhao, L. and Fan, X. and Tian, H.},
title={Exploring hybrid stream computing oriented on-chip adaptive memory structure},
journal={Xibei Gongye Daxue Xuebao/Journal of Northwestern Polytechnical University},
year={2012},
volume={30},
number={6},
pages={961-967},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872461091&partnerID=40&md5=1be915b1464cc071bd2e622bd375834f},
abstract={In scientific applications, the computing process is composed of stream computing and general computing. The performance of computing is limited by low parallelism and hard streamlization. We propose what we call DAMS-Cache (DAMS stands for Dynamical Address Mapping Stream), which can be controlled by both the software and hardware that manage an on-chip memory structure that can, we believe, suppress effectively the above-mentioned limitation. Sections 1 through 4 of the full paper explain DAMS-Cache, whose core consists of: (1) the memory system can support both stream computing and scalar data processing in high performance, which supports irregular stream and conditionally-loaded stream computing by hardware; (2) the DAMS Cache explores both coarse-grained producer-consumer locality and fine-grained temporal/spatial locality by using replacement strategy of mixed data structure; (3) in order to avoid address mapping conflict, the dynamically adaptive storage resource assignment strategy and dynamically adaptive address mapping strategy are also presented. Section 5 presents performance analysis of DAMS Cache; it shows preliminarily that DAMS Cache has better adaptability and high performance compared with Scratchpad Memory and traditional Cache.},
author_keywords={Cache memory;  Computer architecture;  Computer hardware;  Computer simulation;  Computer software;  DAMS (dynamical address mapping stream);  Data processing;  Efficiency;  Microprocessor chips;  Multiprocessing systems;  Optimization;  Resource allocation;  Scheduling;  Schematic diagrams},
keywords={Adaptive memory;  Adaptive storage;  Coarse-grained;  Computing process;  Mapping strategy;  Memory systems;  On chip memory;  On chips;  Performance analysis;  Replacement strategy;  Scientific applications;  Scratch pad memory;  Stream computing, Buffer storage;  Computer architecture;  Computer hardware;  Computer hardware description languages;  Computer simulation;  Computer software;  Dams;  Data processing;  Data structures;  Efficiency;  Hardware;  Mapping;  Microprocessor chips;  Multiprocessing systems;  Optimization;  Resource allocation;  Scheduling;  Schematic diagrams, Cache memory},
language={Chinese},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Smeding201211,
author={Smeding, G. and Gössler, G.},
title={A correlation preserving performance analysis for stream processing systems},
journal={10th ACM/IEEE International Conference on Formal Methods and Models for Codesign, MEMOCODE 2012},
year={2012},
pages={11-20},
doi={10.1109/MEMCOD.2012.6292295},
art_number={6292295},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867204014&doi=10.1109%2fMEMCOD.2012.6292295&partnerID=40&md5=67c620fd9830d6555ff05b0b8aba9802},
abstract={For the design of real-time embedded systems, analysis of performance and resource utilization at an early stage is crucial to evaluate design choices. Network Calculus and its variants provide the tools to perform such analyses for distributed systems processing streams of tasks, based on a max-plus algebra. However, the underlying model employed in Network Calculus cannot capture correlations between the availability of different resources and between the arrivals of tasks, leading to overly conservative performance bounds for some frequently used system topologies. We present a model based on timing constraints relative to pairs of streams, endowed with an analysis technique that can handle such correlations. © 2012 IEEE.},
keywords={Analysis techniques;  Distributed systems;  Max-plus algebra;  Model-based OPC;  Network calculus;  Performance analysis;  Performance bounds;  Real-time embedded systems;  Resource utilizations;  Stream processing systems;  System topology;  Timing constraints, Calculations},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2012113,
author={Wang, L. and Zhou, T.H. and Kim, K.A. and Cha, E.J. and Ryu, K.H.},
title={Adaptive approximation-based streaming skylines for similarity search query},
journal={International Journal of Software Engineering and its Applications},
year={2012},
volume={6},
number={2},
pages={113-118},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867916002&partnerID=40&md5=f6b7a8d50f9ea1a93e3ded3937e430aa},
abstract={Actually, large database is not simply considered as a stream database because of streaming data is not only containing huge data volumes, but distributed, continuous, rapid, time varying. Therefore, the general techniques may not suit for streams exactly. Accuracy responses required of approximated answers is more important in stream processing for the similarity search. Therefore, we perform data reduction across synopsis data structure and to batch processing in a particular relevance way on the data stream computation model over sliding windows. Focus on similarity search in streaming environment, D-skyline method proposed in this paper concern useful aggregate as a preprocessing phase instead of original dataset repeatedly processing manner, in order to efficiently optimize both in space usage and error control. Our experimental evaluation would show the detailed effect on approximated analysis by using different kinds of skyline methods, then effectiveness and efficiency of our approach.},
author_keywords={Similarity search;  Skyline;  Stream processing},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cazalas201263,
author={Cazalas, J. and Guha, R.},
title={Performance Modeling of Spatio-Temporal Algorithms Over GEDS Framework},
journal={International Journal of Grid and High Performance Computing},
year={2012},
volume={4},
number={3},
pages={63-84},
doi={10.4018/jghpc.2012070104},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001578744&doi=10.4018%2fjghpc.2012070104&partnerID=40&md5=e5334f5c1daadb99bb6b940be5ae7d9d},
abstract={The efficient processing of spatio-temporal data streams is an area of intense research. However, all methods rely on an unsuitable processor (Govindaraju, 2004), namely a CPU, to evaluate concurrent, continuous spatio-temporal queries over these data streams. This paper presents a performance model of the execution of spatio-temporal queries over the authors' GEDS framework (Cazalas & Guha, 2010). GEDS is a scalable, Graphics Processing Unit (GPU)-based framework, employing computation sharing and parallel processing paradigms to deliver scalability in the evaluation of continuous, spatio-temporal queries over spatio temporal data streams. Experimental evaluation shows the scalability and efficacy of GEDS in spatio-temporal data streaming environments and demonstrates that, despite the costs associated with memory transfers, the parallel processing power provided by GEDS clearly counters and outweighs any associated costs. To move beyond the analysis of specific algorithms over the GEDS framework, the authors developed an abstract performance model, detailing the relationship of the CPU and the GPU. From this model, they are able to extrapolate a list of attributes common to successful GPU-based applications, thereby providing insight into which algorithms and applications are best suited for the GPU and also providing an estimated theoretical speedup for said GPU-based applications. © 2012, IGI Global. All rights reserved.},
author_keywords={Computation Sharing;  Continuous Query;  Graphical Processing Unit (GPU);  kNN;  Location-Based Services;  Mobile Database Systems;  Parallel Processing;  Performance Model;  Range Query;  Spatio-Temporal Data Streams},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Keong20122958,
author={Keong, C.Y. and Hoong, P.K. and Ting, C.-Y.},
title={A push-pull chunk delivery for mesh-based P2P live streaming},
journal={IEICE Transactions on Information and Systems},
year={2012},
volume={E95-D},
number={12},
pages={2958-2959},
doi={10.1587/transinf.E95.D.2958},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870696599&doi=10.1587%2ftransinf.E95.D.2958&partnerID=40&md5=a2c1c893e7f3a3bc31939f8cdbd98ecf},
abstract={In this paper, we propose an adaptive chunk scheduling for mesh-based peer-to-peer live streaming system, a hybrid class of push and pull chunk delivery approach. The proposed rule-based push-pull scheduler simultaneously pull video chunk from lower latency peers to fill up missing chunks and push video chunk adaptively for rapid chunk delivery. We performed comparative simulation study against rarest first pushpull and status-wise push-pull to prove the efficiency of our proposed algorithm. Mesh-push is made possible by effectively exploiting the information through buffer map exchange. The findings of performance evaluation have suggested a better video continuity and achieved lower source to end delay. Copyright © 2012 The Institute of Electronics, Information and Communication Engineers.},
author_keywords={Chunk scheduling;  Hybrid push-pull;  Mesh-based network;  P2P live streaming},
keywords={Distributed computer systems;  Scheduling;  Video streaming, Comparative simulation;  Hybrid push-pull;  Live streaming;  Peer to peer;  Performance evaluation;  Rule based, Peer to peer networks},
publisher={Institute of Electronics, Information and Communication, Engineers, IEICE},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cipriani201152,
author={Cipriani, N. and Schiller, O. and Mitschang, B.},
title={M-TOP: Multi-target operator placement of query graphs for data streams},
journal={ACM International Conference Proceeding Series},
year={2011},
pages={52-60},
doi={10.1145/2076623.2076631},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855337405&doi=10.1145%2f2076623.2076631&partnerID=40&md5=0da094c342d195e272baaea78e27e023},
abstract={Nowadays, many applications processes stream-based data, such as financial market analysis, network intrusion detection, or visualization applications. To process stream-based data in an application-independent manner, distributed stream processing systems emerged. They typically translate a query to an operator graph, place the operators to stream processing nodes, and execute them to process the streamed data. The operator placement is crucial in such systems, as it deeply influences query execution. Often, different stream-based applications require dedicated placement of query graphs according to their specific objectives, e.g. bandwidth not less than 500 MBit/s and costs not more that 1 cost unit. This fact constraints operator placement. Existing approaches do not take into account application-specific objectives, thus not reflecting application-specific placement decisions. As objectives might conflict among each other, operator placement is subject to delicate trade-offs, such as bandwidth maximization is more important than cost reduction. Thus, the challenge is to find a solution which considers the application-specific objectives and their trade-offs. We present M-TOP, an QoS-aware multi-target operator placement framework for data stream systems. Particularly, we propose an operator placement strategy considering application-specific targets consisting of objectives, their respective trade-offs specifications, bottleneck conditions, and ranking schemes to compute a suitable placement. We integrated M-TOP into NexusDS, our distributed data stream processing middleware, and provide an experimental evaluation to show the effectiveness of M-TOP. © 2011 ACM.},
keywords={Data stream;  Datastream Systems;  Distributed data stream processing;  Experimental evaluation;  Financial market;  Multitarget;  Network intrusion detection;  Placement strategy;  Query execution;  Query graph;  Stream processing;  Stream processing systems;  Stream-based applications;  Visualization application, Bandwidth;  Commerce;  Cost reduction;  Data communication systems;  Data visualization;  Distributed parameter control systems;  Intrusion detection;  Middleware;  Visualization, Data handling},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Giacomazzi201082,
author={Giacomazzi, P. and Poli, A.},
title={Performance analysis of mesh-based peer-to-peer video streaming systems},
journal={2010 International Congress on Ultra Modern Telecommunications and Control Systems and Workshops, ICUMT 2010},
year={2010},
pages={82-88},
doi={10.1109/ICUMT.2010.5676654},
art_number={5676654},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951499714&doi=10.1109%2fICUMT.2010.5676654&partnerID=40&md5=d31755934a801460cb3f89264c284fa5},
abstract={Peer-to-peer networks are an increasingly popular solution for the distribution of media content to a large number of users, with limited investments for network infrastructures. In this paper we focus on mesh-based, also referred to as generic, unstructured, or data-driven, peer-to-peer video streaming systems. We carry out a performance analysis of these systems, analyzing their sensitivity to the most critical system parameters. The analysis focuses on the number of neighbors of each peer, the average number of peers in the system, the permanence time of peers, and the amount of buffered video. Differently from existing studies we found that, depending on peers' permanence time, an optimal number of neighbors can be found. Moreover, moving towards a higher number of neighbors, the corresponding increase of signaling traffic causes a decrease of overall performance. ©2010 IEEE.},
author_keywords={Data-driven;  Generic;  Mesh;  Neighbors;  Peer-to-peer;  Performance analysis;  Video streaming},
keywords={Data-driven;  Generic;  Mesh;  Neighbors;  Peer-to-peer;  Performance analysis, Acoustic streaming;  Control systems;  Distributed computer systems;  Video streaming;  Videotex, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nanao201053,
author={Nanao, S. and Masuyama, H. and Kasahara, S. and Takahashi, Y.},
title={Performance analysis of data block synchronization mechanism in coolstreaming},
journal={5th International Conference on Queueing Theory and Network Applications, QTNA 2010 - Proceedings},
year={2010},
pages={53-58},
doi={10.1145/1837856.1837865},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956250532&doi=10.1145%2f1837856.1837865&partnerID=40&md5=89c6df02ad6cff2ba0ab34cc6493322c},
abstract={Coolstreaming is a Peer-to-Peer (P2P) based video streaming system in which a single video stream is decomposed into multiple sub-streams and a client-peer node receives the sub-streams from multiple parent-peer nodes, combining them into the original video stream. The buffer of the client-peer node is composed of a synchronization buffer and a cache buffer. Data blocks are stored in the synchronization buffer in a sub-stream basis, and then forwarded into the cache buffer according to their sequence numbers. Here, data-block synchronization is important to guarantee video quality. In this paper, we consider the performance of data-block synchronization scheme with which data blocks are simultaneously forwarded just after all the data blocks composing a macro data block arrive at the synchronization buffer. We model the synchronization buffer as a multiple-buffer queueing system with homogeneous Poisson arrival processes, deriving the mean forwarding interval. We also consider the frame loss probability for multiple-path video streaming, investigating how the number of sub-streams decreases the frame loss probability. Numerical examples show that increasing the number of sub-streams makes the average forwarding interval large, while the frame loss probability at the bottleneck router is improved. It is also shown that increasing the synchronization buffer decreases the average forwarding interval. © 2010 ACM.},
author_keywords={Coolstreaming;  Live streaming;  P2P;  Queueing analysis},
keywords={Block synchronization;  Bottleneck router;  Coolstreaming;  Data blocks;  Frame loss probability;  Live streaming;  Multiple-path;  Numerical example;  P2P;  Peer nodes;  Peer to peer;  Performance analysis;  Poisson arrival process;  Queueing analysis;  Queueing system;  Sequence number;  Sub-streams;  Video quality;  Video streams, Acoustic streaming;  Distributed computer systems;  Hydraulics;  Operations research;  Peer to peer networks;  Probability;  Synchronization;  Video streaming;  Videotex, Queueing theory},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kitchen20091,
author={Kitchen, C.A. and Guest, M.F.},
title={Chapter 1 The UK HPC Integration Market. Commodity-Based Clusters},
journal={Advances in Computers},
year={2009},
volume={75},
pages={1-111},
doi={10.1016/S0065-2458(08)00801-2},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-64049084442&doi=10.1016%2fS0065-2458%2808%2900801-2&partnerID=40&md5=e8e08553fd205c4044c87163b191604d},
abstract={This chapter considers the ubiquitous position of commodity clusters in providing HPC capabilities to the scientific community, and the many issues faced by organizations when deciding how best to procure, maintain, and maximize the usage of such a resource. With a focus on developments within the UK, we provide an overview of the current high-performance computing (HPC) landscape from both the customer and supplier perspective. Historically HPC provision in the UK has been focused on one or two leading-edge national facilities that have helped the UK to develop and maintain an internationally competitive position in research using HPC. This HPC dynamic has, however, changed dramatically in the period 2005-2008 with the major injection of funding into University HPC sector through the Science Research Investment Fund (SRIF). This sector is now the major provider of HPC resources to the UK research base, with the capability of the sector increased 100-fold. Our primary focus lies on the role of HPC Integrators in supplying resources into this sector, and the challenges faced by the HPC service providers themselves in sustaining and growing these activities. The host sites through partnership with the selected integrator aim to maximize this entire process, from procurement through system installation and subsequent lifetime of the service. We ask whether those integrators based primarily in the UK have the ability to provide the necessary level of expertise required in all phases of the process, from procurement to ongoing support of the resource throughout its lifecycle. We consider how current HPC technology roadmaps might impinge on the role of integrators in responding to the undoubted challenges that lie ahead. Crucial issues when considering potential integrator involvement include both size of the hardware solution, that is, number of cores, number of nodes, and the ongoing robustness of open-source software solutions that might be deployed on these platforms. Informed by developments over the past 24 months associated with the deployment of systems funded under SRIF, we provide an in-depth analysis of the current status and capability of a number of the leading HPC Integrators within the UK. Our primary attention is given to the three major companies who now supply the academic community and hence are well known to us-Streamline Computing, ClusterVision, and OCF. Seven other integrators are also considered, albeit with less rigor. Consideration is also given to several of the Tier-1 suppliers of clusters. In reviewing the status of commodity-based systems in scientific and technical computing, systems representative of those supported by the integrators, we consider how an organization might best decide on the optimum technology to deploy against its intended workload. We outline our cluster performance evaluation work that uses a variety of synthetic and application-based floating-point metrics to inform this question. Our analysis relies on performance measurements of application independent tests (microbenchmarks) and a suite of scientific applications that are in active use on many large-scale systems. The microbenchmarks we used provide information on the performance characteristics of the hardware, specifically memory bandwidth and latency, and intercore/interprocessor communication performance. These measurements have been extensively used to provide insight into application performance, with the scientific applications used being taken from existing workloads within the SRIF HPC sector, representing various scientific domains and program structures-molecular dynamics, computational engineering, and materials simulation to name a few. The chapter concludes with a 10-point summary of important considerations when procuring HPC clusters, particularly those in the mid-to-high-end range. © 2009 Elsevier Inc. All rights reserved.},
language={English},
document_type={Review},
source={Scopus},
}

@ARTICLE{Chen200894,
author={Chen, Y.-Y. and Chai, Y. and Ye, D.-J.},
title={Control and media distribution server for wireless video streaming systems adopting TCP transmission mode},
journal={Tongxin Xuebao/Journal on Communications},
year={2008},
volume={29},
number={6},
pages={94-99},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-47049102612&partnerID=40&md5=4e7e50a816abc0477434f82deac2201f},
abstract={To meet high concurrency requirement of streaming service and overcome the high loss ratio of the wireless channel, a wireless video streaming server system was proposed with hybrid centralized-distributed architecture which adopts TCP transmission mode. Theoretical analysis and experimental evaluation results show that the proposed scheme exhibits reasonable scalability, high concurrency and reliable transmit quality. Therefore, the proposed scheme is able to provide good service for one kind of wireless streaming applications.},
author_keywords={Distributed architecture;  TCP transfer mode;  Video streaming},
keywords={Distributed architectures;  Experimental evaluations;  Loss ratio;  Media distributions;  Server systems;  Streaming applications;  Streaming services;  Transmission modes;  Wireless channels;  Wireless video streaming, Acoustic streaming;  Control systems;  Electric network analysis;  Internet protocols;  Network architecture;  Transmission control protocol;  Videotex, Video streaming},
language={Chinese},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hu2008,
author={Hu, K. and Liu, Y.},
title={Coordinate concurrent processing over distributed real-time multi-data streams},
journal={Huazhong Keji Daxue Xuebao (Ziran Kexue Ban)/Journal of Huazhong University of Science and Technology (Natural Science Edition)},
year={2008},
volume={36},
number={2},
pages={55-57+69},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-41949087722&partnerID=40&md5=a64df066ae71a07e39fdf32597690b16},
abstract={Based on Client/Server model, the transactional issues in distributed real-time multi-data streams processing are first addressed. By using complex event processing, a transactional mechanism ARTs-MDS was designed to support coordinate concurrent processing on multi-data streams. The system is capable of reforming a set of concurrent tasks as a transactional unit in a self-organization way, isolating real operations on physical devices from the logical operations in the tasks by a persistent input/output queue, and achieving an atomic global effect caused by distributed coordinate tasks processing collected local data. Our performance evaluations show that by applying the system, response time on continuous collected data can be significantly enhanced and data lose can be reduced correspondingly.},
author_keywords={Coordinate concurrent execution;  Distributed real-time application;  Multi data streams;  Transactional processing},
keywords={Client server computer systems;  Data acquisition;  Data processing;  Distributed database systems;  Embedded systems;  Real time systems;  Sensors, Client server model;  Complex event processing;  Coordinate concurrent execution;  Distributed real time application;  Multi data streams;  Transactional processing, Computer applications},
language={Chinese},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kang20061736,
author={Kang, S. and Ku, K.I. and Shim, J.M. and Hur, S.J. and Ju, S.H. and Choi, W.},
title={Performance evaluation of software streaming server architecture for massive users},
journal={8th International Conference Advanced Communication Technology, ICACT 2006 - Proceedings},
year={2006},
volume={3},
pages={1736-1741},
art_number={1625928},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750945526&partnerID=40&md5=2539b82b434021683bf40a43aaf7e248},
abstract={In this paper, we propose the streaming server architecture for massive users and verify it by a small scale simulation with virtual users. Software streaming is one of the ASP solution that allows the execution of stream-enabled software, even while the transmission of the program may still be in progress. In this paper, we analyze workload characteristics of the software stream, and extract four requirements that software streaming system have to satisfy. Based on these requirements, we design the software streaming server architecture. The verification of the designed server is performed by virtual user simulation. The result tells us the capacity of each server and helps us to calculate the scale of the system for massive users.},
author_keywords={Application service providing;  Server design;  Software on-demand;  Software streaming;  Virtual user simulation;  Workload characteristics},
keywords={Computer simulation;  Computer software;  Servers;  Systems analysis, Application service providing;  Server design;  Software on-demand;  Software streaming;  Virtual user simulation;  Workload characteristics, Computer architecture},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fortino200325,
author={Fortino, G. and Russo, W. and Zimeo, E.},
title={Reliable multicast protocols for java-based grid middleware platforms},
journal={Proceedings of the IASTED International Conference on Parallel and Distributed Computing and Systems},
year={2003},
volume={15},
number={1},
pages={25-30},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542433072&partnerID=40&md5=596cdb6312a5cb95a823dcd5187f2678},
abstract={Grids are becoming effective infrastructures for high-performance, network-unaware, general purpose, distributed applications. Most of Grid applications demand for large sets of data to be reliably delivered to a wide collection of resources. Even though unicast reliable protocols, such as TCP, can be adopted to implement data transmission toward multiple receivers, multicast protocols are becoming an efficient alternative. However, differently from multimedia distributed systems, which tolerate unreliable data streaming, Grids often require reliable multicast protocols to deliver data without losses and errors. In this paper, we discuss a suite of reliable multicast protocols for providing middleware platforms for hierarchical grid systems with collective communication and we show a preliminary performance evaluation that compares different reliable multicast schemes.},
author_keywords={Distributed systems;  Grid computing;  Performance evaluation;  Reliable multicast protocols},
keywords={Data communication systems;  Distributed computer systems;  Error analysis;  Java programming language;  Multicasting;  Multimedia systems;  Network protocols;  Signal receivers, Grid computing;  Performance evaluation;  Reliable multicast protocols, Middleware},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Purcell1974385,
author={Purcell, C.J.},
title={The control data STAR-100—Performance measurements},
journal={AFIPS Conference Proceedings - 1974 National Computer Conference, AFIPS 1974},
year={1974},
pages={385-387},
doi={10.1145/1500175.1500257},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059211885&doi=10.1145%2f1500175.1500257&partnerID=40&md5=ca7e65f288bd3ffd237ac4c3f1ea4b48},
abstract={The CONTROL DATA STAR-100 (STring-ARray) Computer is a very large, general purpose, high speed computing system. The STAR-100 computer utilizes integrated circuitry, ferrite core memory, 400 hz power and freon cooling in the hardware implementation. The logical design of the computer combines stream processing, virtual addressing, hardware macro instructions, segmented (pipeline) arithmetic units and a 256-word high speed register file to perform arithmetic and logical operation on discrete or structured data elements (Figure 1). © ACM-CONFERENCE. All rights reserved.},
keywords={Computer control systems;  Computer hardware description languages;  Hardware;  Stars, Arithmetic unit;  Hardware implementations;  High speed computing;  Integrated circuitry;  Logical operations;  Performance measurements;  Stream processing;  Structured data, Computer hardware},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Henning2022,
author={Henning, S. and Hasselbring, W.},
title={A configurable method for benchmarking scalability of cloud-native applications},
journal={Empirical Software Engineering},
year={2022},
volume={27},
number={6},
doi={10.1007/s10664-022-10162-1},
art_number={143},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135556720&doi=10.1007%2fs10664-022-10162-1&partnerID=40&md5=526eb23534a18ad26972f6143d7b4e6b},
abstract={Cloud-native applications constitute a recent trend for designing large-scale software systems. However, even though several cloud-native tools and patterns have emerged to support scalability, there is no commonly accepted method to empirically benchmark their scalability. In this study, we present a benchmarking method, allowing researchers and practitioners to conduct empirical scalability evaluations of cloud-native applications, frameworks, and deployment options. Our benchmarking method consists of scalability metrics, measurement methods, and an architecture for a scalability benchmarking tool, particularly suited for cloud-native applications. Following fundamental scalability definitions and established benchmarking best practices, we propose to quantify scalability by performing isolated experiments for different load and resource combinations, which asses whether specified service level objectives (SLOs) are achieved. To balance usability and reproducibility, our benchmarking method provides configuration options, controlling the trade-off between overall execution time and statistical grounding. We perform an extensive experimental evaluation of our method’s configuration options for the special case of event-driven microservices. For this purpose, we use benchmark implementations of the two stream processing frameworks Kafka Streams and Flink and run our experiments in two public clouds and one private cloud. We find that, independent of the cloud platform, it only takes a few repetitions (≤ 5) and short execution times (≤ 5 minutes) to assess whether SLOs are achieved. Combined with our findings from evaluating different search strategies, we conclude that our method allows to benchmark scalability in reasonable time. © 2022, The Author(s).},
author_keywords={Benchmarking;  Cloud-Native;  Performance engineering;  Scalability},
keywords={Application programs;  Economic and social effects;  Scalability, Application deployment;  Application frameworks;  Benchmarking methods;  Cloud-native;  Configuration options;  Large-scale software systems;  Performance engineering;  Recent trends;  Scalability evaluation;  Service level objective, Benchmarking},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mostafaei2022270,
author={Mostafaei, H. and Afridi, S. and Abawajy, J.},
title={Network-aware worker placement for wide-area streaming analytics},
journal={Future Generation Computer Systems},
year={2022},
volume={136},
pages={270-281},
doi={10.1016/j.future.2022.06.009},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132713867&doi=10.1016%2fj.future.2022.06.009&partnerID=40&md5=e90ecb5dd27a2ba4862046710f763577},
abstract={Many organizations leverage Distributed Stream processing systems (DPSs) to get insights from the data generated by different users/devices, e.g., the Internet of Things (IoT) devices or user clicks on a website, on geographically distributed datacenters. The worker nodes in such environments are connected through Wide Area Network (WAN) links with various delays and bandwidth. Therefore, minimizing the execution latency of a task on the worker nodes while using the links with enough bandwidth and lower cost to steer the traffic of the applications is a challenging task. In this paper, we formulate the worker node placement for a geo-distributed DSPs network as a multi-criteria decision-making problem. Then, we propose an additive weighting-based approach to solve it. The users can prioritize the worker node placement according to the network-relevant parameters. We also propose a framework that can be integrated with the current DPSs to execute the tasks. We test our placement approach on three widely used stream processing systems, i.e., Apache Spark, Apache Storm, and Apache Flink, on three custom graphs adopted from the real cloud providers. We run the streaming query of the Yahoo! streaming benchmark on these three DPSs. The experimental results show that our approach improves the performance of Spark up to 2.2x–7.2x, Storm up to 1.2x–3.4x, and Flink up to 1.4x–3.3x compared with other placement approaches, which makes our framework useful for use in practical environments. © 2022 The Author(s)},
author_keywords={Internet of Things (IoT);  Simple additive weighting;  Stream processing;  Wide Area Network (WAN);  Wide-area stream analytics;  Worker node placement},
keywords={Additives;  Bandwidth;  Decision making;  Storms;  Wide area networks, Internet of thing;  Node placement;  Simple additive weighting;  Stream processing;  Stream processing systems;  Wide area network;  Wide-area networks;  Wide-area stream analytic;  Worker node placement;  Worker nodes, Internet of things},
publisher={Elsevier B.V.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Angbera2022256,
author={Angbera, A. and Chan, H.Y.},
title={A NOVEL TRUE-REAL-TIME SPATIOTEMPORAL DATA STREAM PROCESSING FRAMEWORK},
journal={Jordanian Journal of Computers and Information Technology},
year={2022},
volume={8},
number={3},
pages={256-270},
doi={10.5455/jjcit.71-1646838830},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139450691&doi=10.5455%2fjjcit.71-1646838830&partnerID=40&md5=171752a4ff84471880612981695fb554},
abstract={The ability to interpret spatiotemporal data streams in real time is critical for a range of systems. However, processing vast amounts of spatiotemporal data out of several sources, such as online traffic, social platforms, sensor networks and other sources, is a considerable challenge. The major goal of this study is to create a framework for processing and analyzing spatiotemporal data from multiple sources with irregular shapes, so that researchers can focus on data analysis instead of worrying about the data sources' structure. We introduced a novel spatiotemporal data paradigm for true-real-time stream processing, which enables high-speed and low-latency real-time data processing, with these considerations in mind. A comparison of two state-of-the-art real-time process architectures was offered, as well as a full review of the various open-source technologies for real-time data stream processing and their system topologies were also presented. Hence, this study proposed a brand-new framework that integrates Apache Kafka for spatiotemporal data ingestion, Apache Flink for true-real-time processing of spatiotemporal stream data, as well as machine learning for real-time predictions and Apache Cassandra at the storage layer for distributed storage in real time. The proposed framework was compared with others from the literature using the following features: Scalability (Sc), prediction tools (PT), data analytics (DA), multiple event types (MET), data storage (DS), Real-time (Rt) and performance evaluation (PE) stream processing (SP) and our proposed framework provided the ability to handle all of these tasks. © 2022, Scientific Research Support Fund of Jordan. All rights reserved.},
author_keywords={Apache Cassandra;  Apache Flink;  Apache Kafka;  Apache Spark;  Real-time processing;  Spatiotemporal big data;  Stream processing},
publisher={Scientific Research Support Fund of Jordan},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Matteussi2022,
author={Matteussi, K.J. and Dos Anjos, J.C.S. and Leithardt, V.R.Q. and Geyer, C.F.R.},
title={Performance Evaluation Analysis of Spark Streaming Backpressure for Data-Intensive Pipelines},
journal={Sensors},
year={2022},
volume={22},
number={13},
doi={10.3390/s22134756},
art_number={4756},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132417178&doi=10.3390%2fs22134756&partnerID=40&md5=a1a45fa7c8219bc382d06e8afca14370},
abstract={A significant rise in the adoption of streaming applications has changed the decision-making processes in the last decade. This movement has led to the emergence of several Big Data technologies for in-memory processing, such as the systems Apache Storm, Spark, Heron, Samza, Flink, and others. Spark Streaming, a widespread open-source implementation, processes data-intensive applications that often require large amounts of memory. However, Spark Unified Memory Manager cannot properly manage sudden or intensive data surges and their related in-memory caching needs, resulting in performance and throughput degradation, high latency, a large number of garbage collection operations, out-of-memory issues, and data loss. This work presents a comprehensive performance evaluation of Spark Streaming backpressure to investigate the hypothesis that it could support data-intensive pipelines under specific pressure requirements. The results reveal that backpressure is suitable only for small and medium pipelines for stateless and stateful applications. Furthermore, it points out the Spark Streaming limitations that lead to in-memory-based issues for data-intensive pipelines and stateful applications. In addition, the work indicates potential solutions. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={backpressure;  big data;  spark streaming;  stream processing},
keywords={Decision making;  Information management;  Pipelines, Back pressures;  Data intensive;  Data technologies;  Decision-making process;  Implementation process;  Open source implementation;  Performances evaluation;  Spark streaming;  Stream processing;  Streaming applications, Big data, algorithm, Algorithms;  Big Data},
publisher={MDPI},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kritharakis2022145,
author={Kritharakis, E. and Luo, S. and Unnikrishnan, V. and Vombatkere, K.},
title={Detecting trading trends in streaming financial data using Apache Flink},
journal={DEBS 2022 - Proceedings of the 16th ACM International Conference on Distributed and Event-Based Systems},
year={2022},
pages={145-150},
doi={10.1145/3524860.3539647},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135496129&doi=10.1145%2f3524860.3539647&partnerID=40&md5=b9022089b16e5097d749c0da4bb1729c},
abstract={Modern financial analytics rely on high-volume streams of event notifications that report live market fluctuations based on supply and demand. Accurately identifying trends or breakout patterns based on the Exponential Moving Average (EMA) in the development of an instrument's price early on is an important challenge, so as to buy while the price is low and sell before a downtrend begins. This paper aims to solve the above challenge with a distributed, event-streaming solution built using Apache Flink. We present and implement a solution that leverages customized window operators to calculate the EMA and find breakout patterns, using event generation parallelism to facilitate the rapid processing of the input stream uses sinks to collect and output results, and scales easily on a distributed Flink cluster. We empirically test our design on metrics specified by the benchmarking platform for the DEBS 2022 Grand Challenge and observe a throughput of 45 batches per second and an average latency of 120 ms. © 2022 ACM.},
author_keywords={Apache Flink;  Financial Data;  Stream Processing},
keywords={Data handling;  Finance, Apache flink;  Distributed events;  Event notification;  Exponential moving averages;  Financial data;  High volumes;  Input streams;  Market fluctuations;  Rapid processing;  Stream processing, Commerce},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Horchidan2022,
author={Horchidan, S. and Kritharakis, E. and Kalavri, V. and Carbone, P.},
title={Evaluating model serving strategies over streaming data},
journal={Proceedings of the 6th Workshop on Data Management for End-To-End Machine Learning, DEEM 2022 - In conjunction with the 2022 ACM SIGMOD/PODS Conference},
year={2022},
doi={10.1145/3533028.3533308},
art_number={4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133190958&doi=10.1145%2f3533028.3533308&partnerID=40&md5=d1405abb00b419e8ecfddd3d7b11d81c},
abstract={We present the first performance evaluation study of model serving integration tools in stream processing frameworks. Using Apache Flink as a representative stream processing system, we evaluate alternative Deep Learning serving pipelines for image classification. Our performance evaluation considers both the case of embedded use of Machine Learning libraries within stream tasks and that of external serving via Remote Procedure Calls. The results indicate superior throughput and scalability for pipelines that make use of embedded libraries to serve pre-trained models. Whereas, latency can vary across strategies, with external serving even achieving lower latency when network conditions are optimal due to better specialized use of underlying hardware. We discuss our findings and provide further motivating arguments towards research in the area of ML-native data streaming engines in the future. © 2022 Owner/Author.},
author_keywords={data streams;  machine learning inference},
keywords={Computer hardware description languages;  Data streams;  Deep learning;  Pipeline processing systems;  Pipelines, Data stream;  Evaluating models;  Evaluation study;  Integration tools;  Machine learning inference;  Machine-learning;  Performances evaluation;  Stream processing;  Stream processing systems;  Streaming data, Libraries},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2022,
author={Wang, X. and Zhang, C. and Fang, J. and Zhang, R. and Qian, W. and Zhou, A.},
title={A comprehensive study on fault tolerance in stream processing systems},
journal={Frontiers of Computer Science},
year={2022},
volume={16},
number={2},
doi={10.1007/s11704-020-0248-x},
art_number={162603},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115705468&doi=10.1007%2fs11704-020-0248-x&partnerID=40&md5=e6d118ed3a04501fb8625938ca93c940},
abstract={Stream processing has emerged as a useful technology for applications which require continuous and low latency computation on infinite streaming data. Since stream processing systems (SPSs) usually require distributed deployment on clusters of servers in face of large-scale of data, it is especially common to meet with failures of processing nodes or communication networks, but should be handled seriously considering service quality. A failed system may produce wrong results or become unavailable, resulting in a decline in user experience or even significant financial loss. Hence, a large amount of fault tolerance approaches have been proposed for SPSs. These approaches often have their own priorities on specific performance concerns, e.g., runtime overhead and recovery efficiency. Nevertheless, there is a lack of a systematic overview and classification of the state-of-the-art fault tolerance approaches in SPSs, which will become an obstacle for the development of SPSs. Therefore, we investigate the existing achievements and develop a taxonomy of the fault tolerance in SPSs. Furthermore, we propose an evaluation framework tailored for fault tolerance, demonstrate the experimental results on two representative open-sourced SPSs and exposit the possible disadvantages in current designs. Finally, we specify future research directions in this domain. © 2022, Higher Education Press.},
author_keywords={fault tolerance;  performance evaluation;  stream processing},
keywords={Losses, Communications networks;  Distributed deployment;  Large-scales;  Low latency;  Performances evaluation;  Processing nodes;  Stream processing;  Stream processing systems;  Streaming data;  Tolerance approach, Fault tolerance},
publisher={Higher Education Press Limited Company},
language={English},
document_type={Review},
source={Scopus},
}

@ARTICLE{Su2022538,
author={Su, G. and Liu, L. and Zhang, M. and Rosenblum, D.S.},
title={Quantitative Verification for Monitoring Event-Streaming Systems},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={2},
pages={538-550},
doi={10.1109/TSE.2020.2996033},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085761724&doi=10.1109%2fTSE.2020.2996033&partnerID=40&md5=8c9ece984385bccbae7ea1f83bc38e42},
abstract={High-performance data streaming technologies are increasingly adopted in IT companies to support the integration of heterogeneous and possibly distributed applications. Compared with the traditional message queuing middleware, a streaming platform enables the implementation of event-streaming systems (ESS) which include not only complex queues but also pipelines that transform and react to the streams of data. By analysing the centralised data streams, one can evaluate the Quality-of-Service for other systems and components that produce or consume those streams. We consider the exploitation of probabilistic model checking as a performance monitoring technique for ESS systems. Probabilistic model checking is a mature, powerful verification technique with successful application in performance analysis. However, an ESS system may contain quantitative parameters that are determined by event streams observed in a certain period of time. In this paper, we present a novel theoretical framework called QV4M (meaning 'quantitative verification for monitoring') for monitoring ESS systems, which is based on two recent methods of probabilistic model checking. QV4M assumes the parameters in a probabilistic system model as random variables and infers the statistical significance for the probabilistic model checking output. We also present an empirical evaluation of computational time and data cost for QV4M. © 1976-2012 IEEE.},
author_keywords={Discrete-time markov chain;  Event stream;  Parametric model checking;  Performance monitoring;  Probabilistic model checking;  Statistical inference},
keywords={Data streams;  Middleware;  Model checking;  Multimedia systems;  Quality control;  Quality of service, Empirical evaluations;  Performance monitoring;  Probabilistic model checking;  Probabilistic systems;  Quantitative parameters;  Quantitative verification;  Statistical confidence;  Verification techniques, Monitoring},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Löff2022,
author={Löff, J. and Hoffmann, R.B. and Pieper, R. and Griebler, D. and Fernandes, L.G.},
title={DSParLib: A C++ Template Library for Distributed Stream Parallelism},
journal={International Journal of Parallel Programming},
year={2022},
doi={10.1007/s10766-022-00737-2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140980771&doi=10.1007%2fs10766-022-00737-2&partnerID=40&md5=cef976dcc168e83e80ea14a8012c3458},
abstract={Stream processing applications deal with millions of data items continuously generated over time. Often, they must be processed in real-time and scale performance, which requires the use of distributed parallel computing resources. In C/C++, the current state-of-the-art for distributed architectures and High-Performance Computing is Message Passing Interface (MPI). However, exploiting stream parallelism using MPI is complex and error-prone because it exposes many low-level details to the programmer. In this work, we introduce a new parallel programming abstraction for implementing distributed stream parallelism named DSParLib. Our abstraction of MPI simplifies parallel programming by providing a pattern-based and building block-oriented development to inter-connect, model, and parallelize data streams found in modern applications. Experiments conducted with five different stream processing applications and the representative PARSEC Ferret benchmark revealed that DSParLib is efficient and flexible. Also, DSParLib achieved similar or better performance, required less coding, and provided simpler abstractions to express parallelism with respect to handwritten MPI programs. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Distributed systems;  MPI;  Parallel patterns;  Parallel programming;  Stream processing},
keywords={Abstracting;  Benchmarking;  C++ (programming language);  Interface states;  Message passing, C++ template library;  Data items;  Distributed systems;  Message passing interface;  Message-passing;  Parallel patterns;  Performance;  Processing applications;  Real- time;  Stream processing, Parallel programming},
publisher={Springer},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gu2022539,
author={Gu, R. and Yin, H. and Zhong, W. and Yuan, C. and Huang, Y.},
title={Meces: Latency-efficient Rescaling via Prioritized State Migration for Stateful Distributed Stream Processing Systems},
journal={Proceedings of the 2022 USENIX Annual Technical Conference, ATC 2022},
year={2022},
pages={539-556},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140959714&partnerID=40&md5=38ef20b1e52c2a9346ed315d2b2d43c9},
abstract={Stateful distributed stream processing engines (SPEs) usually call for dynamic rescaling due to varying workloads. However, existing state migration approaches suffer from latency spikes, or high resource usage, or major disruptions as they ignore the order of state migration during rescaling. This paper reveals the importance of state migration order to the latency performance in SPEs. Based on that, we propose Meces, an on-the-fly state migration mechanism which prioritizes the state migration of hot keys (those being processed or about to be processed by downstream operator tasks) to achieve smooth rescaling. Meces leverages a fetch-on-demand design which migrates operator states at record-granularity for state consistency. We further devise a hierarchical state data structure and gradual strategy for migration efficiency. Meces is implemented on Apache Flink and evaluated with diversified benchmarks and scenarios. Compared to state-of-the-art approaches, Meces improves stream processing performance in terms of latency and throughput during rescaling by orders of magnitude, with negligible overhead and no disruption to non-rescaling periods. © 2022 USENIX Annual Technical Conference, ATC 2022.All rights reserved.},
keywords={Distributed stream processing;  Down-stream;  Latency performance;  Migration mechanisms;  On demands;  Rescaling;  Resource usage;  State migrations;  Stream processing engines;  Stream processing systems},
publisher={USENIX Association},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang202217,
author={Zhang, M. and Gao, Y. and He, C. and Tan, T.},
title={MicroStream: A Distributed In-memory Caching Service for Data Production},
journal={Proceedings - 2022 IEEE 13th International Conference on Joint Cloud Computing, JCC 2022},
year={2022},
pages={17-22},
doi={10.1109/JCC56315.2022.00010},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140907596&doi=10.1109%2fJCC56315.2022.00010&partnerID=40&md5=14b0bf48d918cd176545f88ce967cd01},
abstract={Data-driven innovation and optimization have become an important direction for the intelligent transformation of enterprises. Data processing tasks have been developed and orchestrated to extract data insights, creating direct or indirect data dependencies between tasks or between tasks and the presentation layer. Traditional ETL (Extract-Transformation-Load) solutions share data through persistent storage, which has certain performance bottlenecks in hybrid cloud and multisource data scenarios. In this paper, we propose MicroStream, a distributed data virtualization and caching middleware service. MicroStream shields the direct access of ETL tasks to the storage layer and converts batch access to the source database into microstream access. ETL jobs share data through the distributed in-memory caching of MicroStream. In resource-constrained scenarios, such a solution significantly improves the performance of data transformation while reducing the extra load that the transformation jobs imply on the source persistent layer. We present a detailed performance evaluation of MicroStream and show that its performance compares favorably with traditional database-oriented solutions. © 2022 IEEE.},
author_keywords={Data Caching;  Distributed System;  ETL;  Stream Processing},
keywords={Data mining;  Digital storage;  Metadata;  Middleware, Caching services;  Data caching;  Data driven;  Data production;  Distributed systems;  Extract-transformation-load;  Memory caching;  Microstream;  Performance;  Stream processing, Presentation Layer},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Verheijde20221314,
author={Verheijde, J. and Karakoidas, V. and Fragkoulis, M. and Katsifodimos, A.},
title={S-QUERY: Opening the Black Box of Internal Stream Processor State},
journal={Proceedings - International Conference on Data Engineering},
year={2022},
volume={2022-May},
pages={1314-1327},
doi={10.1109/ICDE53745.2022.00103},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136446829&doi=10.1109%2fICDE53745.2022.00103&partnerID=40&md5=8d74fadfa57ee37ba591230d4e6f0fbf},
abstract={Distributed streaming dataflow systems have evolved into scalable and fault-tolerant production-grade systems. Their applicability has departed from the mere analysis of streaming windows and complex-event processing, and now includes cloud applications and machine learning inference. Although the advancements in the state management of streaming systems have contributed significantly to their maturity, the internal state of streaming operators has been so far hidden from external applications. However, that internal state can be seen as a materialized view that can be used for analytics, monitoring, and debugging. In this paper we argue that exposing the internal state of streaming systems to outside applications by making it queryable, opens the road for novel use cases. To this end, we introduce S-QUERY: an approach and reference architecture where the state of stream processors can be queried - either live or through snapshots, achieving different isolation levels. We show how this new capability can be implemented in an existing open-source stream processor, and how queryable state can affect the performance of such a system. Our experimental evaluation suggests that the snapshot configuration adds only up to 8ms latency in the 99.99thpercentile and negligible increase in 0-90thpercentiles. © 2022 IEEE.},
keywords={Program debugging, Black boxes;  Cloud applications;  Complex events;  Dataflow;  Distributed streaming;  Event Processing;  Fault-tolerant;  Internal state;  Stream processor;  Streaming systems, Open systems},
publisher={IEEE Computer Society},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Raptis2022,
author={Raptis, T.P. and Passarella, A.},
title={On Efficiently Partitioning a Topic in Apache Kafka},
journal={Proceedings of the 2022 International Conference on Computer, Information and Telecommunication Systems, CITS 2022},
year={2022},
doi={10.1109/CITS55221.2022.9832981},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135977924&doi=10.1109%2fCITS55221.2022.9832981&partnerID=40&md5=f9d839c5c3432a610a7da481c3e16b9a},
abstract={Apache Kafka addresses the general problem of delivering extreme high volume event data to diverse consumers via a publish-subscribe messaging system. It uses partitions to scale a topic across many brokers for producers to write data in parallel, and also to facilitate parallel reading of consumers. Even though Apache Kafka provides some out of the box optimizations, it does not strictly define how each topic shall be efficiently distributed into partitions. The well-formulated fine-Tuning that is needed in order to improve an Apache Kafka cluster performance is still an open research problem. In this paper, we first model the Apache Kafka topic partitioning process for a given topic. Then, given the set of brokers, constraints and application requirements on throughput, OS load, replication latency and unavailability, we formulate the optimization problem of finding how many partitions are needed and show that it is computationally intractable, being an integer program. Furthermore, we propose two simple, yet efficient heuristics to solve the problem: The first tries to minimize and the second to maximize the number of brokers used in the cluster. Finally, we evaluate its performance via largescale simulations, considering as benchmarks some Apache Kafka cluster configuration recommendations provided by Microsoft and Confluent. We demonstrate that, unlike the recommendations, the proposed heuristics respect the hard constraints on replication latency and perform better w.r.t. unavailability time and OS load, using the system resources in a more prudent way. © 2022 IEEE.},
author_keywords={Apache Kafka;  distributed systems;  event-store;  publish-subscribe;  stream processing},
keywords={Application programs;  Integer programming, Apache kafkum;  Distributed systems;  Event-store;  Fine tuning;  High volumes;  Messaging system;  Optimisations;  Publish/subscribe;  Publish/subscribe messaging;  Stream processing, Benchmarking},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen20221427,
author={Chen, M. and Sun, J. and Aida, K. and Figueiredo, R.J. and Ku, Y.-J. and Subratie, K.},
title={Intelligent Live Video Streaming for Object Detection},
journal={2021 IEEE 23rd International Conference on High Performance Computing and Communications, 7th International Conference on Data Science and Systems, 19th International Conference on Smart City and 7th International Conference on Dependability in Sensor, Cloud and Big Data Systems and Applications, HPCC-DSS-SmartCity-DependSys 2021},
year={2022},
pages={1427-1434},
doi={10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00214},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132434165&doi=10.1109%2fHPCC-DSS-SmartCity-DependSys53884.2021.00214&partnerID=40&md5=1ff4f74fd72747e1ec979f4ad655181c},
abstract={These days, sensors and cameras are being deployed on an increasingly large scale. Furthermore, the rapid development of machine learning models for computer vision now presents novel opportunities for the use of artificial intelligence (AI) and Internet of Things (IoT) combinations in various application scenarios. However, challenges remain in supporting low-latency video streaming from distributed mobile IoT devices under dynamic network environments, and overcoming video data quality degradation that results from weather 'noise', which reduces the accuracy of AI-based data analyses such as object detection. In this paper, we propose a live video stream processing system for supporting intelligent services that integrates the following features. First, to cope with dynamic networks and achieve low latency, our approach employs a peer-to-peer (P2P)-based virtual network at the edge and a multi-tiered architecture composed of IoT cameras, edge, and cloud servers. Second, we construct a flexible messaging system for video analysis built upon SINETStream, which is a messaging system that adopts a topic-based pub/sub model. Third, we implement a framework that can remove weather-related (rain, snow, and fog) noise by applying weather classification and adaptive noise removal models that improve the accuracy of video analysis from data collected outdoors. The latency, throughput, and image quality benchmark experiments conducted to validate the feasibility of our proposed system showed that the process resulted in image quality improvements of approximately 30% (on average). © 2021 IEEE.},
author_keywords={Edge Computing;  IoT;  Object Detection;  P2P;  Video Stream},
keywords={Artificial intelligence;  Benchmarking;  Cameras;  Edge computing;  Image enhancement;  Image quality;  Object detection;  Object recognition;  Peer to peer networks;  Video streaming, Edge computing;  Large-scales;  Live video streaming;  Low latency;  Machine learning models;  Messaging system;  Objects detection;  P2P;  Video analysis;  Video stream, Internet of things},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vyas2022465,
author={Vyas, S. and Tyagi, R.K. and Jain, C. and Sahu, S.},
title={Performance Evaluation of Apache Kafka-A Modern Platform for Real Time Data Streaming},
journal={Proceedings of 2nd International Conference on Innovative Practices in Technology and Management, ICIPTM 2022},
year={2022},
pages={465-470},
doi={10.1109/ICIPTM54933.2022.9754154},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129422538&doi=10.1109%2fICIPTM54933.2022.9754154&partnerID=40&md5=8bd7266bb8ea3245063f7b579727fa01},
abstract={Current generation businesses become more demanding on timely availability of data. Many real-time data streaming tools and technologies are capable to meet business expectations. Apache Kafka is one of the capable open-source distributed scalable technology that enables real-time data streaming with good throughput and latency. In traditional batch processing, data is getting processed in groups or batches but in streaming services, data records are handled separately and there is a flow of data processing that is continuous and real-time. Once Data is available at the source, Kafka can detect and stream it in real-time to the target application. After doing the literature survey it was observed that there are insufficient experiments have been done till now with a variety of volumes and with different values of the number of partitions and polling intervals. The purpose of this study is to elaborate on Apache Kafka implementation and evaluate its performance. This study will analyse key performance indicators for the streaming platform and will provide useful insights from it. These insights will help to design optimized applications in Apache Kafka. Based on gaps identified after the literature survey, multiple experiments have been conducted for the producer and consumer API (Application Programming interface). Configuration of Kafka with Apache Zookeeper helped to drive the results which are captured in tabular form for different values of polling intervals, volumes, and partitions. Data for all test runs have been analysed further to drive the conclusions as mentioned in the results section. This study provides valuable insights about the utilization of CPU (Central Processing Unit) and memory for Apache Kafka streaming on changing volumes, also elaborates the impacts on streaming performance when key configurations are getting changed. © 2022 IEEE.},
author_keywords={Apache Kafka;  Latency;  Polling Interval;  Real Time Data Streaming;  Throughput;  Zookeeper},
keywords={Application programming interfaces (API);  Benchmarking;  Digital storage;  Open source software;  Program processors;  Surveys, Apache kafkum;  Current generation;  Latency;  Literature survey;  Performance;  Performances evaluation;  Polling interval;  Real time data streaming;  Real- time;  Zookeeper, Batch data processing},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mebrek2022516,
author={Mebrek, W. and Bouzeghoub, A.},
title={A Multi-agent Based Framework for RDF Stream Processing},
journal={Lecture Notes in Networks and Systems},
year={2022},
volume={449 LNNS},
pages={516-528},
doi={10.1007/978-3-030-99584-3_45},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128637964&doi=10.1007%2f978-3-030-99584-3_45&partnerID=40&md5=9a8b4d6a9ebbf7619381cd91976bf4d2},
abstract={When a large amount of data is generated from multiple, heterogeneous and continuous data streams, the need for continuous processing and on-the-fly consumption of the overwhelming flow of data is crucial. In this context, the W3C RDF Stream Processing (RSP) Community Group has defined a common model for continuous querying RDF Streams, giving rise to a plethora of RSP engines. However, their main limitation is that, depending on the application queries, one RSP engine may be more appropriate than another, or multiple engines are required to address complex queries. In this paper, we propose a multi-agent based framework for distributed continuous processing that gives the opportunity to use several RSP engines in the same framework in order to benefit from their advantages and to offer the possibility to use them at the same time or in a sequence to answer complex queries. A preliminary experimental evaluation with a real-world benchmark shows promising results when compared to an existing work. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lu2022133,
author={Lu, P. and Yue, Y. and Yuan, L. and Zhang, Y.},
title={AutoFlow: Hotspot-Aware, Dynamic Load Balancing for Distributed Stream Processing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2022},
volume={13157 LNCS},
pages={133-151},
doi={10.1007/978-3-030-95391-1_9},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126221784&doi=10.1007%2f978-3-030-95391-1_9&partnerID=40&md5=6817f510bc6cdff2bc1fbeb5e6531b9c},
abstract={Stream applications are widely deployed on the cloud. While modern distributed streaming systems like Flink and Spark Streaming can schedule and execute them efficiently, streaming dataflows are often dynamically changing, which may cause computation imbalance and backpressure. We introduce AutoFlow, an automatic, hotspot-aware dynamic load balance system for streaming dataflows. It incorporates a centralized scheduler that monitors the load balance in the entire dataflow dynamically and implements state migrations correspondingly. The scheduler achieves these two tasks using a simple asynchronous distributed control message mechanism and a hotspot-diminishing algorithm. The timing mechanism supports implicit barriers and a highly efficient state-migration without global barriers or pauses to operators. It also supports a time-window based load-balance measurement and feeds them to the hotspot-diminishing algorithm without user interference. We implemented AutoFlow on top of Ray, an actor-based distributed execution framework. Our evaluation based on various streaming benchmark datasets shows that AutoFlow achieves good load-balance and incurs a low latency overhead in a highly data-skew workload. © 2022, Springer Nature Switzerland AG.},
author_keywords={Big data;  Control message;  Data skewness;  Load balance;  Stream processing},
keywords={Distributed parameter control systems;  Dynamic loads;  Dynamics;  Scheduling, Control messages;  Data skewness;  Dataflow;  Distributed stream processing;  Dynamic load balancing;  Hotspots;  Hotspots-aware;  Load-balance;  State migrations;  Stream processing, Big data},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sharkova202296,
author={Sharkova, D. and Chernokoz, A. and Trofimov, A. and Sokolov, N. and Gorshkova, E. and Kuralenok, I. and Novikov, B.},
title={Adaptive SQL Query Optimization in Distributed Stream Processing: A Preliminary Study},
journal={Communications in Computer and Information Science},
year={2022},
volume={1457 CCIS},
pages={96-109},
doi={10.1007/978-3-030-93849-9_7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124029622&doi=10.1007%2f978-3-030-93849-9_7&partnerID=40&md5=83f47ec43a71c57c5fd1e4d013bb3a68},
abstract={Distributed stream processing is widely adopted for real-time data analysis and management. SQL is becoming a common language for robust streaming analysis due to the introduction of time-varying relations and event time semantics. However, query optimization in state-of-the-art stream processing engines (SPEs) remains limited: runtime adjustments to execution plans are not applied. This fact restricts the optimization capabilities because SPEs lack the statistical data properties before query execution begins. Moreover, streaming queries are often long-lived, and these properties can change over time. Adaptive optimization, used in databases for queries with insufficient or unknown data statistics, can fit the streaming scenario. In this work, we explore the main challenges that SPEs face during the adjustment of adaptive optimization, such as predicting statistical properties of streams and execution graph migration. We demonstrate potential performance gains of our approach within an extension of the NEXMark streaming benchmark and outline our further work. © 2022, Springer Nature Switzerland AG.},
keywords={Benchmarking;  Distributed parameter control systems, Adaptive optimization;  Common languages;  Distributed stream processing;  Queries optimization;  Real time data analysis;  Real time data management;  Robust streaming;  SQL query;  Stream processing engines;  Time varying, Semantics},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tu2022207,
author={Tu, Y.-F. and Chen, H.-D. and Wang, H.-Y. and Yan, Z.-S. and Qin, X.-L. and Chen, B.},
title={Research on Heterogeneous Accelerated Columnar Storage Engine Based on Co- optimization of Software and Hardware for GoldenX [面向GoldenX软硬协同优化的异构加速列式存储引擎研究]},
journal={Jisuanji Xuebao/Chinese Journal of Computers},
year={2022},
volume={45},
number={1},
pages={207-223},
doi={10.11897/SP.J.1016.2022.00207},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122898924&doi=10.11897%2fSP.J.1016.2022.00207&partnerID=40&md5=63a86c904eafcf969eb14c801c12ea75},
abstract={In the era of Internet of Everything, with the emergence of various new computing technologies and emerging application fields, the traditional data processing methods are gradually evolving from a single processing method to a cloud-edge collaborative data processing method. The complex and changeable ecosystem (including multi-cloud and multi-edge) on the cloud and edge sides have brought rapid growth in data scale, complex data locations and load connections. Under this premise, how to efficiently and stably access data and information and how to speed up data query processing has become a key issue that academia and industry need to solve urgently. On the other hand, the new hardware technologies represented by GPU (graphic process unit) / FPGA (field programmable gate array) heterogeneous computing power, NVM (non-volatile memory) storage, and RDMA(remote direct memory access) network, is developing and applying rapidly. This will have a revolutionary impact on the existing software architecture system, and also provide a foundation for the evolution and performance improvement of the database system, and attract the attention from more and more researchers. Therefore, how to use these emerging new hardware technologies to empower the real database system used in the industry has become one of the current research hotspots. First of all, this paper introduces the architecture of ZTE GoldenX database system and the design of its columnar storage engine, and then focuses on the software and hardware co-design and optimization of the GoldenX's columnar storage engine using new hardware features at the computing layers and storage layers. The main work and contributions include: (1) Offload the tasks of compression/decompression and encryption/decryption from the CPU to the FPGA, and use the programmable characteristics of FPGA to design a dedicated MISD (multiple instruction stream single data stream) architecture processor, adopting "software interface level-computing core level-functional module level" three stage pipeline design to improve the efficiency of data stream processing; (2) Customize a vectorized execution engine for GoldenX's column storage to make full use of the new features of CPU/GPU namely SIMD (single instruction multiple data) to optimize the traditional volcano model, reducing the cost of function calls; (3) Optimize the SQL execution engine, dynamically evaluate and utilize GPU computing resources, and use the JIT compilation technology to push statistical and analysis SQL operation tasks with matrix computing characteristics (such as filtering/sorting/aggregation) down to the GPU, so that the ultra-high parallel computing powers of the GPU can be used to improve SQL query and analysis performance. Our experiments show that the software and hardware optimization method proposed in this paper can effectively improve the system performance of GoldenX. Using FPGA for compression/decompression operations can improve read and write performance by 1.2~2.4 times, and after offloading scanning/connection/grouping operations to GPU, their execution time can be reduced by 33%~92%, and after the vectorization engine is turned on, the execution time of SCAN/AGG/JOIN is reduced by 41%~83%. Finally, we conducted a system-level test on the database, among the 22 queries in the TPC-H benchmark test scenario, the system performance after optimization is 2.5~10 times higher than that before optimization. Compared with openGauss with vectorization turned on, the optimized GoldenX reduces the execution time by 17%~78%. © 2022, Science Press. All right reserved.},
author_keywords={Database;  Field programmable gate array;  Graphic process unit;  Online analytical process;  Storage engine},
keywords={Complex networks;  Computer hardware description languages;  Cryptography;  Data handling;  Database systems;  Digital storage;  Engines;  Graphics processing unit;  Integrated circuit design;  Logic gates;  Memory architecture;  Network architecture;  Pipeline processing systems;  Program processors;  Query processing, Co-optimization;  Computing power;  Data processing methods;  Execution engine;  Graphic process units;  Hardware technology;  On-line analytical process;  Performance;  Software and hardwares;  Storage engines, Field programmable gate arrays (FPGA)},
publisher={Science Press},
language={Chinese},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cattermole2021,
author={Cattermole, A. and Dowland, J. and Watson, P.},
title={Run-time adaptation of stream processing spanning the cloud and the edge},
journal={ACM International Conference Proceeding Series},
year={2021},
doi={10.1145/3492323.3495627},
art_number={3495627},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124796678&doi=10.1145%2f3492323.3495627&partnerID=40&md5=cc6d9ec9b88c7cd4859370e3a1a7b9e5},
abstract={Applications that process streams of events generated by sensors are important in a wide range of domains. It is now common to distribute stream processing across edge devices and the cloud. This exploits processing power near the sensors, reducing the load on the cloud and often the required network bandwidth. In this paper we focus on one challenge in distributed stream processing: automatically adapting the partitioning of the processing between the edge and the cloud without a loss of service. An example is when the event arrival rate increases and the edge processor can no longer meet performance requirements. Re-partitioning without loss of service involves moving computations between the edge and the cloud while events are still being processed. In this paper we describe StrIoT - a stream processing system that supports automatic re-partitioning. It is based on a set of functional stream operators, and the paper describes how the run-time system can automatically adapt applications that use them. A key feature is support for the fission and fusion of operators during adaptations. Performance evaluation shows that StrIoT can move parts of a stream processing application between the cloud and edge with only a low, temporary impact on performance. © 2021 ACM.},
keywords={Arrival rates;  Distributed stream processing;  Network bandwidth;  Performance requirements;  Process streams;  Processing power;  Rate increase;  Runtime adaptation;  Stream processing;  Stream processing systems, Distributed parameter control systems},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Matesanz2021,
author={Matesanz, P. and Graen, T. and Fiege, A. and Nolting, M. and Nejdl, W.},
title={Demand-driven data acquisition for large scale fleets},
journal={Sensors},
year={2021},
volume={21},
number={21},
doi={10.3390/s21217190},
art_number={7190},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118183585&doi=10.3390%2fs21217190&partnerID=40&md5=f309b2b81a6913195b257d6e9aa8251e},
abstract={Automakers manage vast fleets of connected vehicles and face an ever-increasing demand for their sensor readings. This demand originates from many stakeholders, each potentially requiring different sensors from different vehicles. Currently, this demand remains largely unfulfilled due to a lack of systems that can handle such diverse demands efficiently. Vehicles are usually passive participants in data acquisition, each continuously reading and transmitting the same static set of sensors. However, in a multi-tenant setup with diverse data demands, each vehicle potentially needs to provide different data instead. We present a system that performs such vehicle-specific minimization of data acquisition by mapping individual data demands to individual vehicles. We collect personal data only after prior consent and fulfill the requirements of the GDPR. Non-personal data can be collected by directly addressing individual vehicles. The system consists of a software component natively integrated with a major automaker’s vehicle platform and a cloud platform brokering access to acquired data. Sensor readings are either provided via near real-time streaming or as recorded trip files that provide specific consistency guarantees. A performance evaluation with over 200,000 simulated vehicles has shown that our system can increase server capacity on-demand and process streaming data within 269 ms on average during peak load. The resulting architecture can be used by other automakers or operators of large sensor networks. Native vehicle integration is not mandatory; the architecture can also be used with retrofitted hardware such as OBD readers. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Big data;  Cloud computing;  Connected vehicles;  Data streaming;  Fault-tolerant systems;  Floating car data;  Sensor-data acquisition},
keywords={Automobiles;  Big data;  Network architecture;  Sensor networks, Cloud-computing;  Connected vehicle;  Data streaming;  Demand-driven;  Fault- tolerant systems;  Floating-car data;  Large-scales;  Sensor readings;  Sensor-data acquisition;  Sensors data, Data acquisition},
publisher={MDPI},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sarathchandra2021,
author={Sarathchandra, M. and Karandana, C. and Heenatigala, W. and Dayarathna, M. and Jayasena, S.},
title={Resource aware scheduler for distributed stream processing in cloud native environments},
journal={Concurrency and Computation: Practice and Experience},
year={2021},
volume={33},
number={20},
doi={10.1002/cpe.6373},
art_number={e6373},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106274048&doi=10.1002%2fcpe.6373&partnerID=40&md5=460198360f7f7dd5026b85fbaac24b21},
abstract={Recently distributed stream processors are increasingly being deployed in cloud computing infrastructures. In this article, we study performance characteristics of distributed stream processing applications in Google Compute Engine which is based on Kubernetes. We identify performance gaps in terms of throughput which appear in such environments when using a round robin (RR) scheduling algorithm. As a solution, we propose resource aware stream processing scheduler called resource aware scheduler for stream processing applications in cloud native environments (RaspaCN). We implement RaspaCN's job scheduler using two-step process. First, we use machine learning to identify the optimal number of worker nodes. Second, we use RR and multiple Knapsack algorithms to produce performance optimal stream processing job schedules. With three application benchmarks called HTTP Log Processor, Nexmark, and Email Processor representing real world stream processing scenarios we evaluate the performance benefits obtained via RaspaCN's scheduling algorithm. RaspaCN could produce percentage increase of average throughput values by at least 37%, 38%, and 10%, respectively, for HTTP Log Processor, Nexmark, and Email Processor benchmarks for fixed input data rates. Furthermore, we conduct experiments with varying input data rates as well and show 7% improved average throughput for HTTP Log Processor. These experiments show the effectiveness of our proposed stream processor job scheduler for producing improved performance. © 2021 John Wiley & Sons Ltd.},
author_keywords={autoscaling;  cloud computing;  event-based systems;  IaaS;  Knapsack;  Kubernetes;  machine learning;  scalability;  software performance engineering;  stream processing},
keywords={Distributed parameter control systems;  Electronic mail;  HTTP;  Input output programs;  Scheduling, Average throughput;  Cloud computing infrastructures;  Distributed stream processing;  Knapsack algorithms;  Native environment;  Performance benefits;  Performance characteristics;  Round robin scheduling algorithms, Benchmarking},
publisher={John Wiley and Sons Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu202149,
author={Liu, T. and Yang, Z. and Sun, Y.},
title={Docker Container Networking Based Apache Storm and Flink Benchmark Test},
journal={2021 22nd Asia-Pacific Network Operations and Management Symposium, APNOMS 2021},
year={2021},
pages={49-52},
doi={10.23919/APNOMS52696.2021.9562644},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118147039&doi=10.23919%2fAPNOMS52696.2021.9562644&partnerID=40&md5=24278c501374c70cbc6d8ac645ea02d0},
abstract={Many distributed stream computing engines have emerged to handle big data, and they can be deployed in cloud environments consisting of native networks or container networks. Most of the benchmark research on stream computing engines are carried out under the native network, and the research on the impact on container network on stream computing engines is currently inadequate. However, the use of container network will inevitably lead to performance degradation, which is the disadvantage of all virtual networks. In this work, we build Apache Storm and Apache Flink, which are Streaming Computation Engines in container network and native network environments and conduct performance measurements through experiments processing textual data to verify how much performance decreases in container network. Experiments show that the throughput in a container network environment is 1%-5% lower and CPU utilization is 11%-18% lower than in a local network environment. © 2021 IEICE.},
author_keywords={Apache Flink;  Apache Storm;  Benchmark Test;  Container Network;  Streaming Computation Engines},
keywords={Benchmarking;  Data handling;  Engines;  Storms, Apache flink;  Apache storm;  Benchmark tests;  Computation engine;  Computing engines;  Container network;  Network environments;  Stream computing;  Streaming computation engine;  Streaming computations, Containers},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{EnekoRuizDeGauna202146,
author={Eneko Ruiz De Gauna, D. and Irigoyen, E. and Cejudo, I. and Arregui, H. and Leskovsky, P. and Otaegui, O.},
title={Video analytics architecture with metadata event-engine for urban safe cities},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={46-52},
doi={10.1145/3477911.3477919},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117950979&doi=10.1145%2f3477911.3477919&partnerID=40&md5=74f3a381a0408e1f8b95a82ccbc2b844},
abstract={Intelligent video analysis from sources such as urban surveillance cameras is a prolific research area today. Multiple types of computer architectures offer a wide range of possibilities when addressing the needs of computer vision technologies. When it comes to real time processing for high level and complex event detections, however, some limitations may arise, such as the computing power in the edge or the cost of sending real time video to the cloud for running advanced algorithms. In this paper, we present a functional architecture of a complete video surveillance solution and we focus on the metadata-processing event engine which takes care of the high-level video processing that is decoupled from a low-level video analysis. The low-level video analysis running in the edge generates and publishes a flow of JSON messages structure containing the details of bounding boxes detected in each frame into an asynchronous messaging service. The metadata event engine is running in a remote cloud, far from the camera locations. We present the performance evaluation of this event engine under different circumstances simulating data coming simultaneously from multiple cameras, in order to study the best strategies when deploying and partitioning distributed processing tasks. © 2021 ACM.},
author_keywords={Cloud computing;  Edge computing;  Metadata;  Stream processing;  Video surveillance},
keywords={Cameras;  Computer architecture;  Edge computing;  Engines;  Monitoring;  Security systems;  Video signal processing, Cloud-computing;  Edge computing;  Intelligent video;  Running-in;  Stream processing;  Surveillance cameras;  Urban surveillance;  Video analysis;  Video analytics;  Video surveillance, Metadata},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Stefani2021,
author={Stefani, L.D. and Terolli, E. and Upfal, E.},
title={Tiered Sampling: An Efficient Method for Counting Sparse Motifs in Massive Graph Streams},
journal={ACM Transactions on Knowledge Discovery from Data},
year={2021},
volume={15},
number={5},
doi={10.1145/3441299},
art_number={3441299},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108945305&doi=10.1145%2f3441299&partnerID=40&md5=9f185300f502ac6441491f0d5cac5559},
abstract={We introduce Tiered Sampling, a novel technique for estimating the count of sparse motifs in massive graphs whose edges are observed in a stream. Our technique requires only a single pass on the data and uses a memory of fixed size M, which can be magnitudes smaller than the number of edges. Our methods address the challenging task of counting sparse motifs - sub-graph patterns - that have a low probability of appearing in a sample of M edges in the graph, which is the maximum amount of data available to the algorithms in each step. To obtain an unbiased and low variance estimate of the count, we partition the available memory into tiers (layers) of reservoir samples. While the base layer is a standard reservoir sample of edges, other layers are reservoir samples of sub-structures of the desired motif. By storing more frequent sub-structures of the motif, we increase the probability of detecting an occurrence of the sparse motif we are counting, thus decreasing the variance and error of the estimate. While we focus on the designing and analysis of algorithms for counting 4-cliques, we present a method which allows generalizing Tiered Sampling to obtain high-quality estimates for the number of occurrence of any sub-graph of interest, while reducing the analysis effort due to specific properties of the pattern of interest. We present a complete analytical analysis and extensive experimental evaluation of our proposed method using both synthetic and real-world data. Our results demonstrate the advantage of our method in obtaining high-quality approximations for the number of 4 and 5-cliques for large graphs using a very limited amount of memory, significantly outperforming the single edge sample approach for counting sparse motifs in large scale graphs. © 2021 ACM.},
author_keywords={Graph motif mining;  reservoir sampling;  stream computing},
keywords={Graph algorithms;  Graph theory;  Quality control, Analysis of algorithms;  Analytical analysis;  Experimental evaluation;  Low probability;  Novel techniques;  Specific properties;  Sub-structures;  Variance estimate, Graph structures},
publisher={Association for Computing Machinery},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cermak2021736,
author={Cermak, M. and Celeda, P.},
title={Stream-Based IP Flow Analysis},
journal={Proceedings of the IM 2021 - 2021 IFIP/IEEE International Symposium on Integrated Network Management},
year={2021},
pages={736-741},
art_number={9464011},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113673996&partnerID=40&md5=7e81072e5a2e28b5e231a277f9113f9e},
abstract={As the complexity of Internet services, transmission speed, and data volume increases, current IP flow monitoring and analysis approaches cease to be sufficient, especially within high-speed and large-scale networks. Although IP flows consist only of selected network traffic features, their processing faces high computational demands, analysis delays, and large storage requirements. To address these challenges, we propose to improve the IP flow monitoring workflow by stream-based collection and analysis of IP flows utilizing a distributed data stream processing. This approach requires changing the paradigm of IP flow data monitoring and analysis, which is the main goal of our research. We analyze distributed stream processing systems, for which we design a novel performance benchmark to determine their suitability for stream-based processing of IP flow data. We define a stream-based workflow of IP flow collection and analysis based on the benchmark results, which we also implement as a publicly available and open-source framework Stream4Flow. Furthermore, we propose new analytical methods that leverage the stream-based IP flow data processing approach and extend network monitoring and threat detection capabilities. © 2021 IFIP.},
author_keywords={IP Flow;  Stream Processing;  Stream4Flow},
keywords={Benchmarking;  Data streams;  Digital storage;  Distributed parameter control systems;  Network management, Computational demands;  Distributed data stream processing;  Distributed stream processing;  IP flow monitoring;  Large-scale network;  Open source frameworks;  Storage requirements;  Stream-based processing, Internet protocols},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Park2021,
author={Park, H. and Kang, S. and Jeong, J.},
title={Development of Boiling Prediction Method in LP-EGR Cooler and Shape Optimization for Suppressing Boiling using Boiling Index},
journal={SAE Technical Papers},
year={2021},
number={2021},
doi={10.4271/2021-01-0228},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104839958&doi=10.4271%2f2021-01-0228&partnerID=40&md5=e3dd16f823ab9327c148aaf2f9889195},
abstract={An EGR system has been significantly used in order to cope with reinforced exhaust gas regulation and enhancement of fuel efficiency. For the well-designed EGR cooler, performance analysis is basically required. Furthermore, boiling prediction of the EGR cooler is especially essential to evaluate durability failure of abnormal operating conditions in DPF. However, due to intrinsic complexity of detailed 3-dimensional heat transfer tubes in the EGR cooler, no precise technique of boiling prediction has been developed. Therefore, this research had been performed in order to fulfill 3 goals: (1) development of 3-dimentional performance prediction technique including boiling occurrence, (2) generation and validation of a new evaluation index for boiling, (3) development of an optimized EGR cooler for suppressing boiling. In order to increase analysis accuracy and reduce analysis efforts at the same time, 3-dimensional single-phase flow analysis was developed. This technique was applied to LP-EGR cooler design of the next generation smart stream engine of HMC. Rig test results were used to validate this newly developed analysis method. Especially, in order to predict global system boiling, a new evaluation boiling index was introduced. It was defined as ratio of boiling heat transfer to total heat transfer and showed good behavior. For shape optimization, the boiling index was evaluated with 12 operating points. A base design showed poor distribution of coolant flow caused local boiling and system boiling when it gets worse. In order to achieve well distributed coolant flow and suppressing boiling, 43 different baffles were investigated using the boiling index. The shape optimized EGR cooler showed 32% reduction of boiling index and slight increment of cooling efficiency compared to base EGR cooler. Additional rig test of the optimized EGR cooler was performed to validate correlation and consistency and demonstrated good feasibility of the new method and the boiling index. © 2021 SAE International. All rights reserved.},
keywords={Coolants;  Cooling systems;  Efficiency;  Forecasting;  Heat transfer, Abnormal operating conditions;  Analysis accuracy;  Boiling heat transfer;  Cooling efficiency;  Performance analysis;  Performance prediction;  Prediction methods;  Single-phase flow, Shape optimization},
publisher={SAE International},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Muthakshi2021,
author={Muthakshi, S. and Mahesh, K.},
title={Container selection processing implementing extensive neural learning in cloud services},
journal={Materials Today: Proceedings},
year={2021},
doi={10.1016/j.matpr.2021.05.628},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137085280&doi=10.1016%2fj.matpr.2021.05.628&partnerID=40&md5=bf5a4299586bdd71e472f5e5a5de0f25},
abstract={The container selection processing performance analyses huge data with minimal resources and the lowest latency. The elastic management applications execute the containers with the specific hierarchy of virtual processing and machine management. The container that has performance degradation implicit provision of least expensive containers with minimal resources helps to increase the performance of containers. The specific data and stream processing prompts scrutinizing of data through the container selection process through different methodologies. To exterminate the bottleneck problem that selects efficient and required size, processing speed, and its reliability of guiding the batch processing of containers. The extensive neural learning handles container optimality involves a dynamic selection of appropriate containers in cloud service providers. The cloud service providers along with container selection contain batch processing and stream processing allocates efficient container-specific selection appropriately. In the huge data segregation of data processed data that emphasizes multiple data scrutinizing. © 2021},
author_keywords={Batch processing;  Container selection processing;  Data scrutinizing;  Extensive neural learning;  Least expensive containers;  Stream processing},
keywords={Containers;  Data streams;  Distributed database systems, Batch processing;  Cloud service providers;  Cloud services;  Container selection processing;  Data scrutinizing;  Extensive neural learning;  Least expensive container;  Neural learning;  Processing performance;  Stream processing, Batch data processing},
publisher={Elsevier Ltd},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Węcel2021257,
author={Węcel, K. and Szmydt, M. and Stróżyna, M.},
title={Stream Processing Tools for Analyzing Objects in Motion Sending High-Volume Location Data},
journal={Business Information Systems},
year={2021},
volume={1},
pages={257-268},
doi={10.52825/bis.v1i.41},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129459342&doi=10.52825%2fbis.v1i.41&partnerID=40&md5=4b6bdceeec9f0e5c094cfca79d2e9360},
abstract={Recently we observe a significant increase in the amount of easily accessible data on transport and mobility. This data is mostly massive streams of high velocity, magnitude, and heterogeneity, which represent a flow of goods, shipments and the movements of fleet. It is therefore necessary to develop a scalable framework and apply tools capable of handling these streams. In the paper we propose an approach for the selection of software for stream processing solutions that may be used in the transportation domain. We provide an overview of potential stream processing technologies, followed by the method for choosing the selected software for real-time analysis of data streams coming from objects in motion. We have selected two solutions: Apache Spark Streaming and Apache Flink, and benchmarked them on a real-world task. We identified the caveats and challenges when it comes to implementation of the solution in practice. © Authors.},
author_keywords={AIS;  location data;  mobility;  stream processing;  transport},
publisher={Technische Informationsbibliothek (TIB)},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kang2021126,
author={Kang, P. and Lama, P. and Khan, S.U.},
title={SLO-aware Virtual Rebalancing for Edge Stream Processing},
journal={Proceedings - 2021 IEEE International Conference on Cloud Engineering, IC2E 2021},
year={2021},
pages={126-135},
doi={10.1109/IC2E52221.2021.00027},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123208675&doi=10.1109%2fIC2E52221.2021.00027&partnerID=40&md5=441c11e216a58b308a4e8a025b614277},
abstract={The Internet of Things (IoT) has enabled an abundance of geographically distributed physical devices or 'things' equipped with sensors and actuators to exchange information with the Cloud. However, this paradigm remains largely under-exploited for real-time analytic applications. The benefit of realtime data acquisition at the Edge becomes fruitless as it is not readily accessible to more powerful data analytic tools in the Cloud due to wide-area network delays. In this paper, we present VRebalance, a virtual resource orchestrator that provides an end-to-end performance guarantee for concurrent stream processing workloads at the Edge. VRebalance employs Bayesian Optimization $\mathcal{BO}$ to quickly identify near-optimal resource configurations. Experimental results with a real-time open-source IoT benchmark for Distributed Stream Processing Platforms (RIoTBench) and a representative stream processing engine (Apache Storm) demonstrate the superior performance, resource efficiency and adaptiveness of our $\mathcal{BO}$-based resource management system. VRebalance meets the performance SLO (service level objective) targets for stream processing workloads even in the presence of acute system dynamics. It decreases the SLO violation rate by at least 34% for static workloads and by 62.5% for dynamic workloads compared to a hill climbing method. Compared to Storm's default resource scaling mechanism, our method decreases the SLO violation rate by 83.7%. © 2021 IEEE.},
author_keywords={Bayesian Optimization;  Internet of Things;  Resource Management;  Stream Processing},
keywords={Benchmarking;  Data acquisition;  Internet of things;  Natural resources management;  Optimization;  Storms;  Wide area networks, Bayesian optimization;  Performance;  Physical devices;  Real-time analytics;  Rebalancing;  Resource management;  Sensors and actuators;  Service level objective;  Stream processing;  Violation rates, Resource allocation},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gomes20213069,
author={Gomes, A.S. and Oliveirinha, J. and Cardoso, P. and Bizarro, P.},
title={Railgun: Managing large streaming windows under mad requirements},
journal={Proceedings of the VLDB Endowment},
year={2021},
volume={14},
number={12},
pages={3069-3082},
doi={10.14778/3476311.3476384},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119976347&doi=10.14778%2f3476311.3476384&partnerID=40&md5=e6224b11eb851c7565e1b5a85f974a90},
abstract={Some mission critical systems, e.g., fraud detection, require accurate, real-time metrics over long time sliding windows on applications that demand high throughput and low latencies. As these applications need to run “forever” and cope with large, spiky data loads, they further require to be run in a distributed setting. We are unaware of any streaming system that provides all those properties. Instead, existing systems take large simplifications, such as implementing sliding windows as a fixed set of overlapping windows, jeopardizing metric accuracy (violating regulatory rules) or latency (breaching service agreements). In this paper, we propose Railgun, a fault-tolerant, elastic, and distributed streaming system supporting real-time sliding windows for scenarios requiring high loads and millisecond-level latencies. We benchmarked an initial prototype of Railgun using real data, showing significant lower latency than Flink and low memory usage independent of window size. Further, we show that Railgun scales nearly linearly, respecting our msec-level latencies at high percentiles (<250ms @ 99.9%) even under a load of 1 million events per second. © The authors.},
keywords={Rail guns, Data load;  Fraud detection;  High-low;  High-throughput;  Low latency;  Mission critical systems;  Property;  Real- time;  Streaming systems;  Time sliding windows, Real time systems},
publisher={VLDB Endowment},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Daghistani2021_92,
author={Daghistani, A. and Khayat, M. and Felemban, M. and Aref, W.G. and Ghafoor, A.},
title={Guard: Attack-Resilient Adaptive Load Balancing in Distributed Streaming Systems},
journal={IEEE Transactions on Dependable and Secure Computing},
year={2021},
doi={10.1109/TDSC.2021.3123071},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118594261&doi=10.1109%2fTDSC.2021.3123071&partnerID=40&md5=9ff79e222e4986712f329e561b2ec299},
abstract={The performance of distributed streaming systems relies on how even the workload is distributed among their machines. However, data and query workloads are skewed and change rapidly. Therefore, multiple adaptive load-balancing mechanisms have been proposed in the literature to rebalance distributed streaming systems according to the changes in their workloads. This paper introduces a novel attack model that targets adaptive load-balancing mechanisms of distributed streaming systems. The attack reduces the throughput and the availability of the system by making it stay in a continuous state of rebalancing. This paper proposes Guard, a component that detects and blocks attacks that target the adaptive load balancing of distributed streaming systems. Guard uses an unsupervised machine-learning technique to detect malicious users that are involved in the attack. Guard does not block any user unless it detects that the user is malicious. Guard does not depend on a specific application. Experimental evaluation for a high-intensity attack illustrates that Guard improves the throughput and the availability of the system by 85% and 86%, respectively. Moreover, Guard improves the minimum availability that the attacker achieves by 325%. IEEE},
author_keywords={Adaptation models;  Adaptive Load Balancing;  Adaptive systems;  Attack-Resilient;  Distributed Streaming Systems;  Load management;  Load modeling;  Malicious Activity;  Real-time systems;  Social networking (online);  Throughput},
keywords={Interactive computer systems;  Learning systems;  Online systems;  Throughput, Adaptation models;  Adaptive load balancing;  Attack-resilient;  Distributed streaming;  Distributed streaming system;  Load modeling;  Malicious activities;  Real - Time system;  Social networking (online);  Streaming systems, Real time systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Volnes202118,
author={Volnes, E. and Plagemann, T. and Goebel, V. and Kristiansen, S.},
title={EXPOSE: Experimental Performance Evaluation of Stream Processing Engines Made Easy},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12752 LNCS},
pages={18-34},
doi={10.1007/978-3-030-84924-5_2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113217028&doi=10.1007%2f978-3-030-84924-5_2&partnerID=40&md5=7bc9bd2e6cefa8c1d3c4ce02f51385a6},
abstract={Experimental performance evaluation of stream processing engines (SPE) can be a great challenge. Aiming to make fair comparisons of different SPEs raises this bar even higher. One important reason for this challenge is the fact that these systems often use concepts that require expert knowledge for each SPE. To address this issue, we present Expose, a distributed performance evaluation framework for SPEs that enables a user through a declarative approach to specify experiments and conduct them on multiple SPEs in a fair way and with low effort. Experimenters with few technical skills can define and execute distributed experiments that can easily be replicated. We demonstrate Expose by defining a set of experiments based on the existing NEXMark benchmark and conduct a performance evaluation of Flink, Beam with the Flink runner, Siddhi, T-Rex, and Esper, on powerful and resource-constrained hardware. © 2021, Springer Nature Switzerland AG.},
author_keywords={Distributed experiments;  Performance evaluation;  Stream processing},
keywords={Clouds;  Engines, Distributed performance;  Experimental performance evaluations;  Expert knowledge;  Stream processing engines;  Technical skills;  Use concept, Benchmarking},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang2021276,
author={Zhang, Z. and Liu, Z. and Jiang, Q. and Wu, Z. and Chen, J. and An, H.},
title={RDMA-Based Apache Storm for High-Performance Stream Data Processing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12639 LNCS},
pages={276-287},
doi={10.1007/978-3-030-79478-1_24},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112614078&doi=10.1007%2f978-3-030-79478-1_24&partnerID=40&md5=f43e806c669aed457b22bb0474e7e806},
abstract={Apache Storm is a scalable fault-tolerant distributed real-time stream-processing framework widely used in big data applications. For distributed data-sensitive applications, low-latency, high-throughput communication modules have a critical impact on overall system performance. Apache Storm currently uses Netty as its communication component, an asynchronous server/client framework based on TCP/IP protocol stack. The TCP/IP protocol stack has inherent performance flaws due to frequent memory copying and context switching. The Netty component not only limits the performance of the Storm but also increases the CPU load in the IPoIB (IP over InfiniBand) communication mode. In this paper, we introduce two new implementations for Apache Storm communication components with the help of RDMA technology. The performance evaluation on Mellanox QDR Cards (40 Gbps) shows that our implementations can achieve speedup up to 5 × compared with IPoIB and 10 × with 1 Gigabit Ethernet. Our implementations also significantly reduce the CPU load and increase the throughput of the system. © 2021, IFIP International Federation for Information Processing.},
author_keywords={Apache Storm;  Cloud computing;  Communication optimization;  InfiniBand;  RDMA;  Stream-processing framework},
keywords={Storms;  Transmission control protocol, Big data applications;  Communication components;  Communication mode;  Communication modules;  Context switching;  Gigabit Ethernet;  Real-time streams;  Sensitive application, Data streams},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Poeira2021143,
author={Poeira, D. and Carvalho, F.M.},
title={Deconstructing yield operator to enhance streams processing},
journal={Proceedings of the 16th International Conference on Software Technologies, ICSOFT 2021},
year={2021},
pages={143-150},
doi={10.5220/0010541001430150},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111740555&doi=10.5220%2f0010541001430150&partnerID=40&md5=6c3819d33e1051433b9a0f9673cdfaef},
abstract={Customizing streams pipelines with new user-defined operations is a well-known pattern regarding streams processing. However, programming languages face two challenges when considering streams extensibility: 1) provide a compact and readable way to express new operations, and 2) keep streams’ laziness behavior. From here, we may find a consensus around the adoption of the generator operator, i.e. yield, as a means to fulfil both requirements, since most state-of-the-art programming languages provide this feature. Yet, what is the performance overhead of interleaving a yield-based operation in streams processing? In this work we present a benchmark based on realistic use cases of two different web APIs, namely: Last.fm and world weather online, where custom yield-based operations may degrade the streams performance in twofold. We also propose a purely functional and minimalistic design, named tinyield, that can be easily adopted in any programming language and provides a concise way of chaining extension operations fluently, with low overhead in the evaluated benchmarks. The tinyield proposal was deployed in three different libraries, namely for Java (jayield), JavaScript (tinyield4ts) and .Net (tinyield4net). Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
author_keywords={Extensions;  Generators;  Iterators;  Lazy Sequences;  Streams;  Yield},
keywords={Benchmarking;  Functional programming, Generator operators;  Javascript;  Last.fm;  Low overhead;  Purely functional;  State of the art;  Web apis, Pipeline processing systems},
publisher={SciTePress},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Son202161,
author={Son, S. and Moon, Y.-S.},
title={A benchmark test for stateless stream partitioning over distributed network environments},
journal={Lecture Notes in Electrical Engineering},
year={2021},
volume={716},
pages={61-68},
doi={10.1007/978-981-15-9309-3_9},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098269639&doi=10.1007%2f978-981-15-9309-3_9&partnerID=40&md5=aebabb5c658830611ad7470781f400f8},
abstract={Distributed stream processing engines (DSPEs) provides various stateless stream partitioning to select the receiver tasks for each message regardless of the data fields. A representative DSPE, Apache Storm, provides the polarized stateless stream partitioning: Shuffle grouping considering the fairness only and Local-or-Shuffle grouping considering the locality only. The recently proposed Locality Aware grouping is a novel technique to solve this polarization. However, it is difficult to select an appropriate stream partitioning method considering various configurations of distributed stream applications, network capacity, and data size. In this paper, we benchmark the stateless stream partitioning methods from the perspective of different network bandwidths. To change bandwidths, we experiment on the most widely used the usual Ethernet equipment and the recent InfiniBand, a high-performance network equipment. We can use the benchmark results as the selection criteria for choosing the appropriate stream partitioning method according to the network bandwidth. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={Data stream;  Distributed processing;  InfiniBand;  Load balancing;  Locality;  Real-time processing},
keywords={Bandwidth;  Benchmarking;  Distributed parameter control systems, Distributed networks;  Distributed stream processing;  High performance networks;  Network bandwidth;  Novel techniques;  Partitioning methods;  Selection criteria;  Stream application, Data streams},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stein20202207,
author={Stein, O. and Blamey, B. and Karlsson, J. and Sabirsh, A. and Spjuth, O. and Hellander, A. and Toor, S.},
title={Smart Resource Management for Data Streaming using an Online Bin-packing Strategy},
journal={Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
year={2020},
pages={2207-2216},
doi={10.1109/BigData50022.2020.9378241},
art_number={9378241},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103834940&doi=10.1109%2fBigData50022.2020.9378241&partnerID=40&md5=06a36d97e1bcf3180e8099aa7ce8ed4e},
abstract={Data stream processing frameworks provide reliable and efficient mechanisms for executing complex workflows over large datasets. A common challenge for the majority of currently available streaming frameworks is efficient utilization of resources. Most frameworks use static or semi-static settings for resource utilization that work well for established use cases but lead to marginal improvements for unseen scenarios. Another pressing issue is the efficient processing of large individual objects such as images and matrices typical for scientific datasets. HarmonicIO has proven to be a good solution for streams of relatively large individual objects, as demonstrated in a benchmark comparison with the Apache Spark and Kafka streaming frameworks. We here present an extension of the HarmonicIO framework based on the online bin-packing algorithm. The main focus is to compare different strategies adapted in streaming frameworks for efficient resource utilization. Based on a real world use case from large-scale microscopy pipelines, we compare two different strategies of auto-scaling implemented in the HarmonicIO and Spark Streaming frameworks. © 2020 IEEE.},
author_keywords={Big Data;  Cloud Infrastructures;  Data Streaming;  Online Bin-packing;  Profiling;  Resource Management;  Scheduling;  Scientific Data analysis},
keywords={Data streams;  Large dataset, Benchmark comparison;  Complex workflows;  Data stream processing;  Individual objects;  Online bin packing;  Resource management;  Resource utilizations;  Utilization of resources, Information management},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rinaldi20201,
author={Rinaldi, L. and Torquati, M. and Mencagli, G. and Danelutto, M.},
title={High-throughput stream processing with actors},
journal={AGERE 2020 - Proceedings of the 10th ACM SIGPLAN International Workshop on Programming Based on Actors, Agents, and Decentralized Control, Co-located with SPLASH 2020},
year={2020},
pages={1-10},
doi={10.1145/3427760.3428338},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097780927&doi=10.1145%2f3427760.3428338&partnerID=40&md5=e72623ab8d10ef294b7177996ee8b193},
abstract={The steady growth of data volume produced as continuous streams makes paramount the development of software capable of providing timely results to the users. The Actor Model (AM) offers a high-level of abstraction suited for developing scalable message-passing applications. It allows the application developer to focus on the application logic moving the burden of implementing fast and reliable inter-Actors message-exchange to the implementation framework. In this paper, we focus on evaluating the model in high data rate streaming applications targeting scale-up servers. Our approach leverages Parallel Patterns (PP) abstractions to model streaming computations and introduces optimizations that otherwise could be challenging to implement without violating the Actor Model's semantics. The experimental analysis demonstrates that the new implementation skeletons we propose for our PPs can bring significant performance boosts (more than 2X) in high data rate streaming applications implemented in CAF. © 2020 ACM.},
author_keywords={Actor Model;  Data Stream Processing;  Multi-Cores;  Parallel Patterns;  Programming Model},
keywords={Data streams;  Message passing;  Semantics, Application developers;  Application logic;  Experimental analysis;  High level of abstraction;  Parallel patterns;  Stream processing;  Streaming applications;  Streaming computations, Decentralized control},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Harwood20201344,
author={Harwood, A. and Read, M.R. and Amarasinghe, G.N.},
title={Dragon: A lightweight, high performance distributed stream processing engine},
journal={Proceedings - International Conference on Distributed Computing Systems},
year={2020},
volume={2020-November},
pages={1344-1351},
doi={10.1109/ICDCS47774.2020.00177},
art_number={9355671},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101981403&doi=10.1109%2fICDCS47774.2020.00177&partnerID=40&md5=0be1ad3a884e690a56e646495c741a40},
abstract={—The performance of a distributed stream processing engine is traditionally considered in terms of fundamental measurements of latency and throughput. Recently, Apache Storm has demonstrated sub-millisecond latencies for inter-component tuple transmission, though it does so through aggressive throttling that leads to strict throughput limitations in order to keep tuple queues near empty. On the other hand, Apache Heron has excellent throughput characteristics, especially when operating near unstable conditions, but its inter-component latencies typically start around 10 milliseconds. Both of these systems require roughly 650MB of installation space. We have developed Dragon, loosely based on the same API as Storm and Heron, that is both lightweight, requiring just 7.5MB of installation space, and competitive in performance to Storm and Heron. In this paper we show experiments with all three systems using the Word Count benchmark. Dragon achieves throughput characteristics near to that of Heron and inter-component latencies less than 10ms under high load. In particular, Dragon’s maximum latency is significantly less that Storm’s maximum latency under high load. Finally Dragon managed to remain stable at higher effective throughput than Heron. We believe Dragon is a good “all-rounder” solution and is particularly suitable for Edge computing applications, given its small installation footprint. ©2020 IEEE},
author_keywords={Latency;  stream processing engine;  Throughput},
keywords={Distributed parameter control systems;  Engines, Computing applications;  Distributed stream processing;  Effective throughput;  High load;  Maximum latency;  Measurements of;  Unstable conditions, Storms},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sundar2020,
author={Sundar, M. and Kailasam, S. and Gonsalves, T.A.},
title={Benchmarking Distributed Stream Processing Frameworks for Real Time Classical Machine Learning Applications},
journal={2020 11th International Conference on Computing, Communication and Networking Technologies, ICCCNT 2020},
year={2020},
doi={10.1109/ICCCNT49239.2020.9225267},
art_number={9225267},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096578746&doi=10.1109%2fICCCNT49239.2020.9225267&partnerID=40&md5=9b19712382d387b6ed7ce3b7bea3c3ad},
abstract={As the volume of data generated is growing at an unprecedented rate, it becomes important to analyze this data in real-time. To handle the huge volume of data streaming at a high velocity, we not only require powerful machines but also means to distribute the computation involved on the multiple machines. There are several open-source distributed stream processing frameworks such as Apache {Storm, Flink, Spark} and Confluent Kafka for building real-time machine learning applications. Prior works benchmarked some of these platforms using low-level operations like filters, joins, windowed computations etc. Our work includes benchmarking these popular frameworks for their applicability to classical machine learning models: Online K-Means, Online Linear Regression and Online Logistic Regression. We study the following quantitative metrics of evaluation: throughput, latency, CPU, and memory usage. The experiments were conducted in both standalone and clusters setups to determine the scalability of the models. This study will help system designers choose the right model and the right framework, given a specific configuration on streaming data. © 2020 IEEE.},
author_keywords={classical machine learning;  distributed stream processing frameworks;  latency;  throughput},
keywords={Distributed parameter control systems;  Logistic regression;  Machine learning, Distributed stream processing;  Machine learning applications;  Machine learning models;  Multiple machine;  Online linear regression;  Quantitative metrics;  Streaming data;  Windowed computations, Benchmarking},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mebrek2020509,
author={Mebrek, W. and Bouzeghoub, A.},
title={A stream reasoning framework based on a multi-agents model},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2020},
pages={509-512},
doi={10.1145/3341105.3374111},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083031670&doi=10.1145%2f3341105.3374111&partnerID=40&md5=c430ebdbccffd6678c0ef59f51c688cc},
abstract={Processing on-the-fly high volume of data streams is increasingly needed. To cope with the heterogeneity of this data, RDF model is more and more being adopted leading to plethora of RDF Stream Processing (RSP) systems and languages dealing with issues such as continuous querying, incremental reasoning and complex event processing (CEP). However, most of them has implemented centralized approaches and therefore suffer from some limitations as collaboration, sharing, expressiveness and scalability. Multi-agents systems have widely proven their worth and efficiency in particular their intrinsic decentralized property along with their cooperation and communication mechanism. In this paper we propose a new framework MAS4MEAN (Multi-Agent System for streaM rEAsoNing) based on a multi-agents model to embrace their benefits and tackle the challenges of increasing the scalability and ease of deployment in highly dynamic environments. A preliminary experimental evaluation with a real-world dataset show promising results when compared to an existing work. © 2020 Owner/Author.},
author_keywords={Multi-agents systems;  RDF streams;  Stream processing;  Stream reasoning},
keywords={Data streams;  Scalability, Centralized approaches;  Communication mechanisms;  Complex event processing (CEP);  Dynamic environments;  Experimental evaluation;  Incremental reasoning;  Multi-agents models;  Multi-agents systems, Multi agent systems},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pieters2020,
author={Pieters, R.P. and Schrijvers, T.O.M.},
title={Faster coroutine pipelines: A reconstruction},
journal={Journal of Functional Programming},
year={2020},
doi={10.1017/S0956796820000192},
art_number={e22},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094195977&doi=10.1017%2fS0956796820000192&partnerID=40&md5=8f3ad7375d678e62a1d1b3383c6da088},
abstract={The three-continuation approach to coroutine pipelines efficiently represents a large number of connected components. Previous work in this area introduces this alternative encoding but does not shed much light on the underlying principles for deriving this encoding from its specification. This paper gives this missing insight by deriving the three-continuation encoding based on eliminating the mutual recursion in the definition of the connect operation. Using the same derivation steps, we are able to derive a similar encoding for a more general setting, namely bidirectional pipes. Additionally, we evaluate the encoding in an advertisement analytics benchmark where it is as performant as pipes, conduit, and streamly, which are other common Haskell stream processing libraries. © The Author(s), 2020. Published by Cambridge University Press.},
keywords={Encoding (symbols);  Signal encoding, Connected component;  Coroutine;  Haskell;  Mutual recursion;  Stream processing;  Underlying principles, Pipelines},
publisher={Cambridge University Press},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2020101,
author={Wang, X. and Jiang, C. and Fang, J. and Shu, K. and Zhang, R. and Qian, W. and Zhou, A.},
title={Evaluating Fault Tolerance of Distributed Stream Processing Systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12318 LNCS},
pages={101-116},
doi={10.1007/978-3-030-60290-1_8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093823067&doi=10.1007%2f978-3-030-60290-1_8&partnerID=40&md5=4ec2fde98b7be04e66c5d881e061317f},
abstract={Since failures in large-scale clusters can lead to severe performance degradation and break system availability, fault tolerance is critical for distributed stream processing systems (DSPSs). Plenty of fault tolerance approaches have been proposed over the last decade. However, there is no systematic work to evaluate and compare them in detail. Previous work either evaluates global performance during failure-free runtime, or merely measures throughout loss when failure happens. In this paper, it is the first work proposing an evaluation framework customized for quantitatively comparing runtime overhead and recovery efficiency of fault tolerance mechanisms in DSPSs. We define three typical configurable workloads, which are widely-adopted in previous DSPS evaluations. We construct five workload suites based on three workloads to investigate the effects of different factors on fault tolerance performance. We carry out extensive experiments on two well-known open-sourced DSPSs. The results demonstrate performance gap of two systems, which is useful for choice and evolution of fault tolerance approaches. © 2020, Springer Nature Switzerland AG.},
author_keywords={Benchmarking;  Fault tolerance;  Stream processing},
keywords={Big data;  Distributed parameter control systems;  Information management, Distributed stream processing;  Evaluation framework;  Fault tolerance mechanisms;  Fault tolerance performance;  Large-scale clusters;  Performance degradation;  Recovery efficiency;  System availability, Fault tolerance},
publisher={Springer Science and Business Media Deutschland GmbH},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shaikh2019,
author={Shaikh, S.A. and Lee, J. and Matono, A. and Kim, K.-S.},
title={A robust and scalable pipeline for the real-time processing and analysis of massive 3D spatial streams},
journal={ACM International Conference Proceeding Series},
year={2019},
doi={10.1145/3366030.3366105},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123041862&doi=10.1145%2f3366030.3366105&partnerID=40&md5=68bb2a868f31dd0786959496803287cb},
abstract={With the increase in the use of 3D scanner to sample the earth surface, there is a surge in the availability of 3D spatial data. 3D spatial data contains a wealth of information and can be of potential use if integrated, processed and analyzed in real-time. The 3D spatial data is generated as continuous data stream, however due to its size, velocity and inherent noise, it is processed offline. Many applications require real-time processing and analysis of spatial stream, for-instance, forest fire management, real-time road traffic analysis, disaster engulfed areas monitoring, etc., however they suffer from slow offline processing of traditional systems. This paper presents and demonstrates a robust and scalable pipeline for the real-time processing and analysis of 3D spatial streams. An experimental evaluation is also presented to prove the effectiveness of the proposed framework. © 2019 Association for Computing Machinery.},
author_keywords={3D spatial data;  Point cloud data;  Real-time analysis;  Real-time processing;  Stream processing},
keywords={Data streams;  Deforestation;  Information retrieval;  Pipeline processing systems;  Pipelines;  Street traffic control;  Web services;  Websites, Point cloud data;  Real time analysis;  Realtime processing;  Spatial data;  Stream processing, Real time systems},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2019202,
author={Li, K. and Liu, G. and Lu, M.},
title={A holistic stream partitioning algorithm for distributed stream processing systems},
journal={Proceedings - 2019 20th International Conference on Parallel and Distributed Computing, Applications and Technologies, PDCAT 2019},
year={2019},
pages={202-207},
doi={10.1109/PDCAT46702.2019.00046},
art_number={9029068},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083205486&doi=10.1109%2fPDCAT46702.2019.00046&partnerID=40&md5=db8f087e320264dbd6f88a8a1dda68a6},
abstract={The performances of modern distributed stream processing systems are critically affected by the distribution of the load across workers. Skewed data streams in real world are very common and pose a great challenge to these systems, especially for stateful applications. Key splitting, which allows a single key to be routed to multiple workers, is a great idea to achieve good balance of load in the cluster. However, it comes with the cost of increased memory consumption and computation overhead as well as network communication. In this paper, we present a new definition of metric to model the cost of key splitting for intra-operator parallelism in stream processing systems and provide a novel perspective to reduce replication factor while keeping both overall load imbalance and processing latency low. Similar to previous work, our approach treats the head and the tail of the distribution differently in order to reduce memory requirements. For the head, it uses our proposed notion of regional load imbalance to decide dynamically whether to make one more worker responsible for the heavy hitter or not. For the tail, it simply uses hash partitioning to keep the size of the routing table for the head as small as possible. Extensive experimental evaluation demonstrates that our approach provides superior performance compared to the state-of-the-art partitioning algorithms in terms of load imbalance, replication factor and latency over different levels of skewed stream distributions. © 2019 IEEE.},
author_keywords={Key splitting;  Load imbalance;  Stream partitioning;  Stream processing},
keywords={Distributed parameter control systems, Computation overheads;  Distributed stream processing;  Experimental evaluation;  Memory requirements;  Network communications;  Operator parallelism;  Partitioning algorithms;  Stream processing systems, Data streams},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Prasaad2019,
author={Prasaad, G. and Ramalingam, G. and Rajan, K.},
title={Scaling ordered stream processing on shared-memory multicores},
journal={ACM International Conference Proceeding Series},
year={2019},
doi={10.1145/3350489.3350495},
art_number={a6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072804219&doi=10.1145%2f3350489.3350495&partnerID=40&md5=4969564a42a11cec85b3d5d00e60a9c0},
abstract={Many modern applications require realtime processing of large volumes of high speed data. Such data processing can be specified in the form of a dataflow graph that exposes multiple opportunities for parallelizing its execution, in the form of data, pipeline and task parallelism. This paper focuses on the problem of effectively parallelizing ordered streaming computations by exploiting low overhead, shared memory parallelism on a multicore machine. We propose an adaptive runtime that dynamically maps the exposed parallelism in a streaming computation to that of the machine using scheduling heuristics. Further, we address some key problems in effectively realizing ordered data parallelism. We propose a new approach for parallelizing partitioned stateful operators that can handle load imbalance across partitions effectively and mostly avoid delays due to ordering constraints. We also present a low latency, non-blocking concurrent data structure to order outputs produced by concurrent workers on an operator. Finally, we perform an in-depth empirical evaluation illustrating the trade-offs and effectiveness of our concurrent data-structures and scheduling heuristics using micro-benchmarks and on the TPCx-BB benchmark. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
author_keywords={Concurrent data structures;  Dynamic scheduling;  Multicore;  Parallelism;  Shared memory;  Stream processing},
keywords={Concurrency control;  Data flow analysis;  Data streams;  Data structures;  Economic and social effects;  Information analysis;  Memory architecture;  Scheduling, Concurrent data structures;  Dynamic scheduling;  Multi core;  Parallelism;  Shared memory;  Stream processing, Pipeline processing systems},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lambachri2019,
author={Lambachri, T. and Hajjam El Hassani, A. and Andres, E.},
title={Aligning pattern extraction algorithms for the lambda architecture},
journal={2018 9th International Conference on Information, Intelligence, Systems and Applications, IISA 2018},
year={2019},
doi={10.1109/IISA.2018.8633698},
art_number={8633698},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062889300&doi=10.1109%2fIISA.2018.8633698&partnerID=40&md5=617765ad7b168a836fad2dee2a65eda5},
abstract={For quite some time now, data have become the new oil of the digital industry. The spread and evolution of information technologies and connectivity between people and devices have enabled a new dimension of big-data storage and analytics that could bring major improvements across industries. In this paper, we propose a new, frequent itemset mining approach. The challenge is to apply traditional extraction techniques in a distributed environment. The main originality of our mining method is to take benefits of a performant existing algorithm and use a novel data structure to maintain frequent sequential patterns coupled with a quick pruning strategy. The proposed approach has been implemented using Spark to further improve the efficiency of iterative computation. Numeric experiment results using standard benchmark datasets by comparing the proposed algorithm with the existing algorithm, FP-Growth, demonstrate that our approach has better efficiency and scalability. © 2018 IEEE},
author_keywords={Big data;  Frequent itemset mining;  Spark;  Stream processing},
keywords={Big data;  Digital storage;  Efficiency;  Electric sparks;  Extraction;  Iterative methods;  Mining, Benchmark datasets;  Distributed environments;  Extraction techniques;  Frequent itemset mining;  Frequent sequential patterns;  Iterative computation;  Pattern extraction;  Stream processing, Data mining},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shah2019361,
author={Shah, S.S.H. and Said, N. and Nayab, A. and Khan, W. and Shinwari, Z.A. and Jawwad, M. and Minallah, N.},
title={Performance comparison of chunk and peer scheduling algorithms of peer-To-peer streaming systems},
journal={Proceedings - 2018 International Conference on Frontiers of Information Technology, FIT 2018},
year={2019},
pages={361-366},
doi={10.1109/FIT.2018.00070},
art_number={8617019},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062428116&doi=10.1109%2fFIT.2018.00070&partnerID=40&md5=3f335ef4e9fefb4d197c5e0ac2d84c77},
abstract={With increasing popularity of Peer to Peer systems for video streaming, it is important that the expectations of the users regarding the quality of such systems are being met. In a P2P system, the media stream is divided into small data units known as chunks. Each peer in a Peer to Peer (P2P) system has to take two important decisions at a given time. First, which chunks are to be shared and second with which peer. This paper compares the performance of different combinations of chunk/peer schedulers in terms of chunk diffusion delay, average chunk distribution delay and max chunk distribution delay. By doing so, the best possible combination of the two schedulers for the given experimental setup is explored. The results obtained under the specified experimental setup show that when chunk scheduling algorithm Deadline Based Chunk Scheduler (DLc) is combined with different peer scheduling algorithms, the best results are obtained by its combination with Chunk Earliest Free Pair Scheduler (CEFp). For a constant peer scheduler CEFp combined with different chunk schedulers, the best results are obtained by combining it with Latest Blind Chunk Scheduler (LBc). Finally, with varying neighborhood size, the best results are obtained by the combination of DLC and Chunk Almost Free Peer Scheduler (CAFp). © 2018 IEEE.},
author_keywords={Chunk Scheduler;  flexibility;  Peer Scheduler;  Peer to Peer systems;  performance;  scalability;  SSSim},
keywords={Distributed computer systems;  Scalability;  Scheduling;  Scheduling algorithms, Chunk Scheduler;  flexibility;  Peer Scheduler;  Peer-to-Peer system;  performance;  SSSim, Peer to peer networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tran2019194,
author={Tran, G.P.C. and Walters, J.P. and Crago, S.P.},
title={Reducing tail latencies while improving resiliency to timing errors for stream processing workloads},
journal={Proceedings - 11th IEEE/ACM International Conference on Utility and Cloud Computing, UCC 2018},
year={2019},
pages={194-203},
doi={10.1109/UCC.2018.00028},
art_number={8603166},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061734047&doi=10.1109%2fUCC.2018.00028&partnerID=40&md5=bd5999c01778094402b765ba641fa207},
abstract={Stream processing is an increasingly popular model for online data processing that can be partitioned into streams of elements. It is commonly used in real-time data analytics services, such as processing Twitter tweets and Internet of Things (IoT) device feeds. Current stream processing frameworks boast high throughput and low average latency. However, users of these frameworks may desire lower tail latencies and better real-time performance for their applications. In practice, there are a number of errors that can affect the performance of stream processing applications, such as garbage collection and resource contention. For some applications, these errors may cause unacceptable violations of real-time constraints. In this paper we propose applying redundancy in the data processing pipeline to increase the resiliency of stream processing applications to timing errors. This results in better real-time performance and a reduction in tail latency. We present a methodology and apply this redundancy in a framework based on Twitter's Heron. Finally, we evaluate the effectiveness of this technique against a range of injected timing errors using benchmarks from Intel's Storm Benchmark. Our results show that redundant tuple processing can effectively reduce the tail latency, and that the number of missed deadlines can also be reduced by up to 94% in the best case. We also study the potential effects of duplication when applied at different stages in the topology. For the topologies in this paper, we further observe that duplication is most effective when computation is redundant at the first bolt. Finally, we evaluate the additional overhead that duplicating tuples brings to a stream processing topology. Our results also show that computation overhead scales slower than communication, and that the real-time performance is improved in spite of the overheads. Overall we conclude that redundancy through duplicated tuples is indeed a powerful tool for increasing the resiliency to intermittent runtime timing errors. ©2018 IEEE},
author_keywords={Fault-tolerance;  Garbage collection;  Real-time;  Resiliency;  Stream processing;  Tail latency},
keywords={Cloud computing;  Data Analytics;  Data handling;  Errors;  Fault tolerance;  Interactive computer systems;  Internet of things;  Redundancy;  Refuse collection;  Social networking (online);  Timing circuits;  Topology, Garbage collection;  Real time;  Resiliency;  Stream processing;  Tail latency, Pipeline processing systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Huang2019163515,
author={Huang, J. and Huang, J. and Gao, S. and Yang, B.},
title={Cost-Minimizing Online Algorithms for Geo-Distributed Data Analytics},
journal={IEEE Access},
year={2019},
volume={7},
pages={163515-163525},
doi={10.1109/ACCESS.2019.2951682},
art_number={8891688},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075799512&doi=10.1109%2fACCESS.2019.2951682&partnerID=40&md5=793f712ee1f79af738c0566e8197a99a},
abstract={Modern enterprises often manage geographically distributed datacenters around the globe. In such environment, datasets are naturally collected and stored in different data centers and were later queried for complex analytics. In this paper, we study the Wide-Area Data Analytics problem, which aims to efficiently control data movements and achieve low latency for overall queries processing, both constrained by limited and expensive network resources across datacenters. Previous papers focus on offline settings of single analytical queries and do not consider time in optimizing system performance, and therefore ignores the dynamics of data and task placement in terms of inter-DC bandwidth utilization. In this paper, we consider the online setting and formulate a cost-minimizing optimization problem over time for arbitrary Directed Acyclic Graph query processing. Considering dynamics of network resource usage, we developed two online algorithms, Online Switch Resist (OSR) and Most Fixed Horizon Control (MFHC) with good competitive ratios. We performed extensive simulations and comparative studies using the TPC-CH benchmark and verified the efficacy of proposed algorithms. The algorithm we proposed is better than the existing algorithm, and its performance approximates the theoretical optimal value. © 2013 IEEE.},
author_keywords={Approximate nested query;  distributed stream processing;  error guarantee;  resource allocation},
keywords={Directed graphs;  Distributed parameter control systems;  Resource allocation;  Wide area networks, Approximate nested query;  Band-width utilization;  Directed acyclic graph (DAG);  Distributed data analytics;  Distributed stream processing;  Dynamics of networks;  Extensive simulations;  Optimization problems, Data Analytics},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ren201989,
author={Ren, T. and Rao, G. and Zhang, X. and Feng, Z.},
title={SRSPG: A Plugin-based Spark Framework for Large-scale RDF Streams Processing on GPU},
journal={CEUR Workshop Proceedings},
year={2019},
volume={2456},
pages={89-92},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073203408&partnerID=40&md5=b6f70ae5b9837fbc8b9ea8365f824603},
abstract={In this paper, we propose a plugin-based Spark framework (SRSPG) for large-scale RDF streams processing on GPU. Within this framework, We convert RDF streams to a RDF graph in a unified and simple way. Then we can apply various SPARQL query engines to process continuous queries and utilize GPU to accelerate queries. Computation Module provides a Spark-based Join algorithm utilizing GPU for parallel joining, obtaining the final results. Besides, we provide Compute Resource Management to balance the scheduling and task execution between GPU and memory resources. Finally, we evaluate our work bulit on gStore and RDF-3X on the LUBM benchmark. The experimental results show that SRSPG is effective for real-time processing of large-scale RDF streams. Copyright 2019 for this paper by its authors.},
keywords={Scheduling;  Semantic Web, Computation modules;  Compute resources;  Continuous queries;  Join algorithm;  Memory resources;  Realtime processing;  Sparql queries;  Task executions, Graphics processing unit},
publisher={CEUR-WS},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kolev2019431,
author={Kolev, B. and Akbarinia, R. and Jimenez-Peris, R. and Levchenko, O. and Masseglia, F. and Patino, M. and Valduriez, P.},
title={Pipelined implementation of a parallel streaming method for time series correlation discovery on sliding windows},
journal={DATA 2019 - Proceedings of the 8th International Conference on Data Science, Technology and Applications},
year={2019},
pages={431-436},
doi={10.5220/0008191304310436},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072961252&doi=10.5220%2f0008191304310436&partnerID=40&md5=b9a2f554cbb7111bbd969e2835a53509},
abstract={This paper addresses the problem of continuously finding highly correlated pairs of time series over the most recent time window. The solution builds upon the ParCorr parallel method for online correlation discovery and is designed to run continuously on top of the UPM-CEP data streaming engine through efficient streaming operators. The implementation takes advantage of the flexible API of the streaming engine that provides low level primitives for developing custom operators. Thus, each operator is implemented to process incoming tuples on-the-fly and hence emit resulting tuples as early as possible. This guarantees a real pipelined flow of data that allows for outputting early results, as the experimental evaluation shows. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
author_keywords={Data Stream Processing;  Distributed Computing;  Time Series Correlation},
keywords={Distributed computer systems;  Engines;  Pipelines;  Time series, Data stream processing;  Data streaming;  Experimental evaluation;  Highly-correlated;  Parallel method;  Pipelined implementation;  Sliding Window;  Streaming engines, Data handling},
publisher={SciTePress},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pohl2019267,
author={Pohl, C. and Sattler, K.-U.},
title={Adaptive Partitioning and Order-Preserved Merging of Data Streams},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11695 LNCS},
pages={267-282},
doi={10.1007/978-3-030-28730-6_17},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072826567&doi=10.1007%2f978-3-030-28730-6_17&partnerID=40&md5=8a30cd8489c71014556ed3dcc1cca3de},
abstract={Partitioning is a key concept for utilizing modern hardware, especially to exploit parallelism opportunities from many-core CPUs. In data streaming scenarios where parameters like tuple arrival rates can vary, adaptive strategies for partitioning solve the problem of overestimating or underestimating query workloads. While there are many possibilities to partition the data flow, threads running partitions independently from each other lead to unordered output inevitably. This is a considerable difficulty for applications where tuple order matters, like in stream reasoning or complex event processing scenarios. In this paper, we address this problem by combining an adaptive partitioning approach with an order-preserving merge algorithm. Since reordering output tuples can only worsen latency, we mainly focus on the throughput of queries while keeping the delay on individual tuples minimal. We run micro-benchmarks as well as the Linear Road benchmark, demonstrating correctness and effectiveness of our approach while scaling out on a single Xeon Phi many-core CPU up to 256 partitions. © 2019, Springer Nature Switzerland AG.},
author_keywords={Adaptive partitioning;  Many-core;  Order preservation;  Parallelism;  Stream processing;  Xeon Phi},
keywords={Information use;  Program processors, Adaptive partitioning;  Many core;  Order preservation;  Parallelism;  Stream processing;  Xeon Phi, Information systems},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gradvohl201848,
author={Gradvohl, A.L.S.},
title={Metrics and tool for evaluating data stream processing systems},
journal={Proceedings - 2018 IEEE 6th International Conference on Future Internet of Things and Cloud Workshops, W-FiCloud 2018},
year={2018},
pages={48-55},
doi={10.1109/W-FiCloud.2018.00014},
art_number={8488174},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056548097&doi=10.1109%2fW-FiCloud.2018.00014&partnerID=40&md5=b9b13e4e2468f67aa7515ac2615b43f1},
abstract={With the Internet of Things and many other data sources continuously generating events, service providers who analyze these data need fast and flexible software. These software, called data stream processing systems, can be created from scratch or, as is more often the case today, use frameworks that already implement many of the functionality required for stream processing. In fact, programmers implement these software as distributed systems today because they need frameworks to deal with huge, fast and high throughput streams. That is the reason why a centralized system would hardly be able to handle data streams without eventually discarding some events. Therefore, it is important that, before putting a data stream processing system in production, we can simulate the scenarios that this system will handle and analyze its behavior. In this sense, this paper presents the general characteristics of the data stream processing systems, as well as some of the main metrics for the analysis of this sort of system. In addition, we present a new tool called B2-4DaSP that facilitates the task of analyzing data stream processing systems. We also illustrate some examples of the B2-4DaSP use for the analysis of two simple data stream applications. © 2018 IEEE.},
author_keywords={Benchmark;  Data stream processing;  Metrics},
keywords={Benchmarking;  Internet of things, Centralized systems;  Data stream processing;  Data-sources;  Distributed systems;  High throughput;  Metrics;  Service provider;  Stream processing, Data handling},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Prasuna2018854,
author={Prasuna, T. and Pravallika, K.G. and Babu, D.C. and Sindhura, V.},
title={A novel approach for improved data replication using HDFS},
journal={2018 3rd IEEE International Conference on Recent Trends in Electronics, Information and Communication Technology, RTEICT 2018 - Proceedings},
year={2018},
pages={854-859},
doi={10.1109/RTEICT42901.2018.9012371},
art_number={9012371},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081788400&doi=10.1109%2fRTEICT42901.2018.9012371&partnerID=40&md5=0d3ccca3fc5ea251d1908e4601fe5e40},
abstract={HDFS (Hadoop Distributed File System) is intended to store huge dataset values with accurate file location with high reliability and data streaming to the application is done at high bandwidth. HDFS deals with high fault tolerance with the use of replication of data. Many researches have been done on fitting the data in the exact location. The problem occurred in Hadoop distributed file system is difficulty with information space for storage. It is the most complicated problem which reduce the performance of the file system. To overcome this issue the proposed system aims to have the better data replication which depends upon the access count estimation in Hadoop framework. The proposed system creates the better replicas and to solve the data locality problem with improved arrangement of data replicas and to assign the task for efficient workers to complete the Map Reduce task to obtain the better results. By comparison with the existing system, the proposed system performs the better replication and solves the data locality problem. An experiment has been performed for evaluating the proposed technique with default technique and previously used replication techniques using a benchmark. With respect to the results obtained the proposed method obtained a better throughput when compared to the previous techniques. © 2018 IEEE.},
author_keywords={Access Prediction;  Data locality;  Hadoop},
keywords={Facsimile;  Fault tolerance;  File organization, Data locality;  Existing systems;  Hadoop;  Hadoop distributed file systems;  Hadoop frameworks;  High fault tolerances;  Information spaces;  Replication techniques, Digital storage},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kuo2018179,
author={Kuo, Y.-H. and Lee, Y.-H. and Huang, K.-C. and Lai, K.-C.},
title={Critical task scheduling for data stream computing on heterogeneous clouds},
journal={Lecture Notes in Electrical Engineering},
year={2018},
volume={464},
pages={179-186},
doi={10.1007/978-981-10-7398-4_19},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045984936&doi=10.1007%2f978-981-10-7398-4_19&partnerID=40&md5=76470adc235950c04ba4d7cb71b3aa4f},
abstract={Internet of Things is an emerging paradigm to enable easy data collection and exchange among a wide variety of devices. When the scale of Internet of Things enlarges, the cloud computing system could be applied to mine these big data generated by Internet of Things. This paper proposes a task scheduling approach for time-critical data streaming applications on heterogeneous clouds. The proposed approach takes the tasks in critical stages into consideration, and re-schedules these tasks to appropriate resources to shorten their processing time. In general, selecting the time-critical task to give more resources may remove the execution bottleneck. A small-scale cloud system including 3 servers is built for experiments. The performance of the proposed approach is evaluated by three micro-benchmarks. Preliminary experimental results demonstrate the performance improvement of the critical task scheduling approach. © 2018, Springer Nature Singapore Pte Ltd.},
author_keywords={Cloud computing;  Critical path;  Data stream computing;  Mesos;  Spark;  Task scheduling},
keywords={Benchmarking;  Cloud computing;  Computation theory;  Electric sparks;  Internet of things;  Multitasking;  Scheduling algorithms, Critical Paths;  Data collection;  Data stream;  Mesos;  Micro-benchmark;  Performance improvements;  Processing time;  Task-scheduling, Big data},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gkolemis2018157,
author={Gkolemis, E. and Doka, K. and Koziris, N.},
title={Automatic scaling of resources in a storm topology},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10739 LNCS},
pages={157-169},
doi={10.1007/978-3-319-74875-7_10},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042547398&doi=10.1007%2f978-3-319-74875-7_10&partnerID=40&md5=5805886f1a9d29e0472c748d5571b74c},
abstract={In the Big Data era, the batch processing of large volumes of data is simply not enough - data needs to be processed fast to support continuous reactions to changing conditions in real-time. Distributed stream processing systems have emerged as platforms of choice for applications that rely on real-time analytics, with Apache Storm [2] being one of the most prevalent representatives. Whether deployed on physical or virtual infrastructures, distributed stream processing systems are expected to make the most out of the available resources, i.e., achieve the highest throughput or lowest latency with the minimum resource utilisation. However, for Storm - as for most such systems - this is a cumbersome trial-and-error procedure, tied to the specific workload that needs to be processed and requiring manual tweaking of resource-related topology parameters. To this end, we propose ARiSTO, a system that automatically decides on the appropriate amount of resources to be provisioned for each node of the Storm workflow topology based on user-defined performance and cost constraints. ARiSTO employs two mechanisms: a static, model-based one, used at bootstrap time to predict the resource-related parameters that better fit the user needs and a dynamic, rule-based one that elastically auto-scales the allocated resources in order to maintain the desired performance even under changes in load. The experimental evaluation of our prototype proves the ability of ARiSto to efficiently decide on the resource-related configuration parameters, maintaining the desired throughput at all times. © Springer International Publishing AG 2018.},
keywords={Batch data processing;  Cloud computing;  Distributed parameter control systems;  Real time systems;  Storms;  Topology, Configuration parameters;  Continuous reactions;  Distributed stream processing;  Experimental evaluation;  Real-time analytics;  Resource utilisation;  Trial-and-error procedures;  Virtual infrastructures, Big data},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dazzi201829,
author={Dazzi, P. and Mordacchini, M.},
title={Noa-aid: Network overlays for adaptive information aggregation, indexing and discovery at the edge},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10659 LNCS},
pages={29-41},
doi={10.1007/978-3-319-75178-8_3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042469662&doi=10.1007%2f978-3-319-75178-8_3&partnerID=40&md5=f80602a46608632d4b5cef438568e801},
abstract={This paper presents NOA-AID a network architecture for targeting highly distributed systems, composed of a large set of distributed stream processing devices, aimed at adaptive information indexing, aggregation and discovery in streams of data. The architecture is organized on two layers. The upper layer is aimed at supporting the information discovery process by providing a distributed index structure. The lower layer is mainly devoted to resource aggregation based on epidemic protocols targeting highly distributed and dynamic scenarios, well suited to stream-oriented scenarios. We present a theoretical study on the costs of information management operations, also giving an empirical validation of such findings. Finally, we presented an experimental evaluation of the ability of our solution to be effective and efficient in retrieving meaningful information in streams on a highly-dynamic and distributed scenario. © Springer International Publishing AG, part of Springer Nature 2018.},
author_keywords={Adaptivity;  Distributed indexing;  IoT;  Network overlay information aggregation in streams;  Stream},
keywords={Distributed parameter control systems;  Indexing (of information);  Information management;  Memory architecture;  Network architecture, Adaptivity;  Distributed index structures;  Distributed indexing;  Distributed stream processing;  Experimental evaluation;  Information aggregation;  Information discovery;  Stream, Indexing (materials working)},
publisher={Springer Verlag},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gad2016569,
author={Gad, R. and Kappes, M. and Medina-Bulo, I.},
title={Local programming language barriers in stream-based systems},
journal={Proceedings - IEEE Symposium on Computers and Communications},
year={2016},
volume={2016-August},
pages={569-574},
doi={10.1109/ISCC.2016.7543798},
art_number={7543798},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985997821&doi=10.1109%2fISCC.2016.7543798&partnerID=40&md5=f8ae1a0a217be8da1b4347fa7e78c8e4},
abstract={Stream-based data processing systems, such as Complex Event Processing or data stream mining systems, may be composed of several components which may be implemented in various programming languages. In distributed scenarios, computer networks typically represent important bottlenecks. However, the performance of data exchange in local contexts may be as important as the performance of data exchange via computer networks. Local programming language barriers may represent important bottlenecks for components that are located on the same computer system. In distributed scenarios, it may be beneficial to relocate components on a single physical host for exploiting the higher local data throughput. The properties of stream-based systems pose challenges like high throughput requirements but also open up optimization potential such as leveraging batched transfers. We performed an experimental analysis of ways for bridging local programming language barriers using the examples of C, Java, and Python and analyzed the impact of batched forwarding. While local data exchange can be expected to offer a higher throughput than exchange across networks, our results show that batch forwarding can increase the local throughput by factors of up to 47.6 and we measured net throughputs up to 39.5 Gbps. © 2016 IEEE.},
author_keywords={Data Stream Processing;  Programming Languages;  Throughput Performance},
keywords={Ada (programming language);  C (programming language);  Complex networks;  Computer hardware description languages;  Computer networks;  Computer programming;  Computer programming languages;  Computer systems programming;  Data communication systems;  Data handling;  Electronic data interchange;  Java programming language;  Throughput, Complex event processing;  Data processing systems;  Data stream mining;  Data stream processing;  Experimental analysis;  High throughput;  Optimization potential;  Throughput performance, Distributed computer systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ray2016336,
author={Ray, M. and Lei, C. and Rundensteiner, E.A.},
title={Poster: SPASS: Scalable event stream processing leveraging sharing opportunities},
journal={DEBS 2016 - Proceedings of the 10th ACM International Conference on Distributed and Event-Based Systems},
year={2016},
pages={336-339},
doi={10.1145/2933267.2933288},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978658293&doi=10.1145%2f2933267.2933288&partnerID=40&md5=c469c2405be9bc8a9fc9ea96a2dc2280},
abstract={Complex Event Processing (CEP) offers high-performance event analytics in time-critical decision-making applications. Yet supporting high-performance event processing has become increasingly difficult due to the increasing size and complexity of event pattern workloads. In this work, we propose the SPASS framework that leverages time-based event correlations among queries for sharing computation tasks among sequence queries in a workload. We show the NP-hardness of our CEP pattern sharing problem by reducing it from the Minimum Substring Cover problem. The SPASS system finds a shared pattern plan in polynomial-time covering all sequence patterns while still guaranteeing an optimality bound. Further, the SPASS system assures concurrent maintenance and reuse of sub-patterns in the shared pattern plan. Our experimental evaluation confirms that the SPASS framework achieves over 16-fold performance gain compared to the state-of-the-art solutions. © 2016 ACM.},
author_keywords={Complex event processing;  Sequence patterns;  Sharing},
keywords={Polynomial approximation;  Software architecture, Complex event processing;  Complex event processing (CEP);  Computation tasks;  Event correlation;  Event stream processing;  Experimental evaluation;  Sequence patterns;  Sharing, Decision making},
publisher={Association for Computing Machinery, Inc},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhao201620,
author={Zhao, Y. and Zhang, Y. and Yao, Y. and Li, Y. and Liu, P.},
title={Cocktail: A hybrid system combining Hadoop and Storm},
journal={Proceedings of 2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2015},
year={2016},
pages={20-25},
doi={10.1109/IAEAC.2015.7428510},
art_number={7428510},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966340043&doi=10.1109%2fIAEAC.2015.7428510&partnerID=40&md5=c527e38775e06c16d979930611fc10bb},
abstract={Hadoop and Storm are playing a significant role in Cloud Computing and either of them has its own applicable area. Cocktail is a new hybrid system that combines Hadoop and Storm into one single system, leveraging the functions of two computing frameworks. The design and implementation of Cocktail includes a SQL-like query language making the implementation of details transparent for users, an intelligent framework selector based on cost model to choose appropriate framework automatically, and an efficient resource scheduling and task execution framework. Cocktail has a wide range of application scenarios from batch processing to stream computing, using Storm to process real-time data and Hadoop to process large-scale data. We compare the performance, throughput and scalability of Cocktail with SummingBird to demonstrate the practicability and capability. According to benchmark, for small-scale data, the performance of Cocktail is close to Summingbird based on Storm and 20%∼40% faster than Summingbird based on Hadoop. And for large-scale data, Cocktail's throughput is 40% higher than Summingbird's throughout based on Storm. © 2015 IEEE.},
author_keywords={Hadoop;  Hybrid System;  Storm},
keywords={Batch data processing;  Benchmarking;  Hybrid systems;  Query languages, Application scenario;  Computing frameworks;  Design and implementations;  Hadoop;  Large scale data;  Resource-scheduling;  Stream computing;  Task executions, Storms},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gudumasu2016,
author={Gudumasu, S. and Hamza, A. and Asbun, E. and He, Y. and Ye, Y.},
title={Layer-based buffer aware rate adaptation design for SHVC video streaming},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9971},
doi={10.1117/12.2235795},
art_number={99711M},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006847272&doi=10.1117%2f12.2235795&partnerID=40&md5=2b7c1550a5db89c8611af27b39841aa7},
abstract={This paper proposes a layer based buffer aware rate adaptation design which is able to avoid abrupt video quality fluctuation, reduce re-buffering latency and improve bandwidth utilization when compared to a conventional simulcast based adaptive streaming system. The proposed adaptation design schedules DASH segment requests based on the estimated bandwidth, dependencies among video layers and layer buffer fullness. Scalable HEVC video coding is the latest state-of-art video coding technique that can alleviate various issues caused by simulcast based adaptive video streaming. With scalable coded video streams, the video is encoded once into a number of layers representing different qualities and/or resolutions: a base layer (BL) and one or more enhancement layers (EL), each incrementally enhancing the quality of the lower layers. Such layer based coding structure allows fine granularity rate adaptation for the video streaming applications. Two video streaming use cases are presented in this paper. The first use case is to stream HD SHVC video over a wireless network where available bandwidth varies, and the performance comparison between proposed layer-based streaming approach and conventional simulcast streaming approach is provided. The second use case is to stream 4K/UHD SHVC video over a hybrid access network that consists of a 5G millimeter wave high-speed wireless link and a conventional wired or WiFi network. The simulation results verify that the proposed layer based rate adaptation approach is able to utilize the bandwidth more efficiently. As a result, a more consistent viewing experience with higher quality video content and minimal video quality fluctuations can be presented to the user. © 2016 SPIE.},
author_keywords={DASH;  HEVC;  Segment scheduling;  SHVC;  Video streaming},
keywords={Bandwidth;  Codes (symbols);  Communication channels (information theory);  Image coding;  Image processing;  Millimeter waves;  Video signal processing;  Video streaming;  Wi-Fi, Adaptive video streaming;  Available bandwidth;  Band-width utilization;  DASH;  HEVC;  Performance comparison;  SHVC;  Video Streaming Applications, Scalable video coding},
publisher={SPIE},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chakraborty201677,
author={Chakraborty, R. and Majumdar, S.},
title={A priority based resource scheduling technique for multitenant storm clusters work in progress paper},
journal={Simulation Series},
year={2016},
volume={48},
number={8},
pages={77-82},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994602797&partnerID=40&md5=51ef839b2990c03cecb2beadd809e545},
abstract={In this work in progress paper, we present our ongoing effort towards devising a priority based resource scheduling technique and framework for apache storm. Apache Storm is a popular distributed real time stream processing engine which has been widely adopted by key players in the industry including YAHOO and Twitter. An application running in storm is called a topology that is characterized by a Directed Acyclic Graph (DAG). To run multiple of such topologies in a storm cluster, storm provides with default, out of the box scheduler called Isolation Scheduler. Isolation Scheduler assigns resources to topologies based on static resource configuration and does not provide any means to prioritize topologies based on their varying business priority. As a result, performance degradation, even complete starvation of topologies with high business priority is possible when available cluster resources are insufficient. A priority based resource scheduling strategy is proposed in this paper to overcome this problem. A preliminary performance evaluation is performed to demonstrate effectiveness of the proposed scheduler over the default storm Isolation Scheduler.},
author_keywords={Apache storm;  Big data;  Distributed computing;  Distributed stream processing (DSP);  Event processing;  Priority scheduling;  Resource management},
keywords={Big data;  Data streams;  Directed graphs;  Distributed computer systems;  Distributed parameter control systems;  Information management;  Storms;  Topology, Directed acyclic graph (DAG);  Distributed stream processing;  Event Processing;  Performance degradation;  Priority scheduling;  Real-time streams;  Resource management;  Resource-scheduling, Scheduling},
publisher={The Society for Modeling and Simulation International},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li2015110,
author={Li, X. and Yang, X. and Xu, X.},
title={Programming model and resource management of distributed stream architecture},
journal={Guofang Keji Daxue Xuebao/Journal of National University of Defense Technology},
year={2015},
volume={37},
number={6},
pages={110-115},
doi={10.11887/j.cn.201506021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955092126&doi=10.11887%2fj.cn.201506021&partnerID=40&md5=eb9041236aa176477c1ae160bae30fdc},
abstract={While providing big data computing services using Internet resources, there remains a big challenge to researchers, including heterogeneity of Internet resources, dynamics of Internet resources and long latency of Internet communication. Current influent distributed computing models still have some shortage. A novel distributed stream computing model was proposed based on the traditional stream computing model, including the distributed stream programming model and resource management can efficiently support multiple parallel execution modes. The prototype system implemented on the 10 CPU-GPU heterogeneous nodes. Seven different benchmarks used in the simulation experiment. The experimental result shows that the distributed stream architecture can achieve the speedup of at least on average over the local serial computing, with significant potential for applications. © 2015, National University of Defense Technology. All right reserved.},
author_keywords={Big data;  Distributed computing;  Programming model;  Stream architecture},
keywords={Big data;  Distributed computer systems;  Internet;  Natural resources management;  Resource allocation, Computing services;  Distributed computing models;  Heterogeneous nodes;  Internet communication;  Parallel executions;  Programming models;  Resource management;  Stream architecture, Computer architecture},
publisher={National University of Defense Technology},
language={Chinese},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Burtsev201465,
author={Burtsev, A. and Mishrikoti, N. and Eide, E. and Ricci, R.},
title={Weir: A streaming language for performance analysis},
journal={Operating Systems Review (ACM)},
year={2014},
volume={48},
number={1-118},
pages={65-70},
doi={10.1145/2525528.2525537},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955292233&doi=10.1145%2f2525528.2525537&partnerID=40&md5=1098a37186b027ce7dfdab225531446b},
abstract={For modern software systems, performance analysis can be a challenging task. The software stack can be a complex, multi-layer, multi-component, concurrent, and parallel environment with multiple contexts of execution and multiple sources of performance data. Although much performance data is available, because modern systems incorporate many mature data-collection mechanisms, analysis algorithms suffer from the lack of a unifying programming environment for processing the collected performance data, potentially from multiple sources, in a convenient and script-like manner. This paper presents Weir, a streaming language for systems performance analysis. Weir is based on the insight that performanceanalysis algorithms can be naturally expressed as stream-processing pipelines. In Weir, an analysis algorithm is implemented as a graph composed of stages, where each stage operates on a stream of events that represent collected performance measurements. Weir is an imperative streaming language with a syntax designed for the convenient construction of stream pipelines that utilize composable and reusable analysis stages. To demonstrate practical application, this paper presents the authors' experience in using Weir to analyze performance in systems based on the Xen virtualization platform. Copyright © 2013 ACM.},
keywords={Computational linguistics;  Computer software reusability;  Hydraulic structures;  Pipeline processing systems;  Pipelines;  Weirs, Analysis algorithms;  Data collection mechanism;  Multiple contexts;  Parallel environment;  Performance analysis;  Performance measurements;  Programming environment;  Systems performance analysis, Computer hardware description languages},
publisher={Association for Computing Machinery},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mao2014771,
author={Mao, H. and Tian, C. and Sun, J. and Yan, J. and Wu, W. and Huang, B.},
title={Shadow VoD: Performance Evaluation as a Capability in Production P2P-CDN Hybrid VoD Networks},
journal={Proceedings - 2014 IEEE International Conference on Ubiquitous Intelligence and Computing, 2014 IEEE International Conference on Autonomic and Trusted Computing, 2014 IEEE International Conference on Scalable Computing and Communications and Associated Symposia/Workshops, UIC-ATC-ScalCom 2014},
year={2014},
pages={771-776},
doi={10.1109/UIC-ATC-ScalCom.2014.77},
art_number={7307040},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949545596&doi=10.1109%2fUIC-ATC-ScalCom.2014.77&partnerID=40&md5=dbcdee7d43c1d33904dca71988416793},
abstract={Video-on-Demand (VoD) services have achieved great success recently. Most such streaming systems are P2P-CDNhybrid systems. To ensure reliable performance, the most efficient way is to subject those VoD streaming networks to large-scale, realistic performance evaluations. Our previous Shadow Stream system is a production Internet live streaming network with performance evaluation as a built-in capability. In this paper, we extend the same idea into the VoD services. There exists significant difference between live and VoD, hence Shadow Stream cannot be directly used in VoD context. Firstly, clients in P2PVoD service are not synchronized in viewing progress, secondly, in VoD there exists interactive operations (e.g., Pause and drag), thirdly, the different play points of users also bring difficulty to replacing departed real clients. In this paper, we solve all above mentioned challenges. We implement Shadow VoD and demonstrate its benefits through extensive evaluations. © 2014 IEEE.},
keywords={Peer to peer networks;  Trusted computing;  Ubiquitous computing;  Video streaming, Interactive operations;  Live streaming;  Reliable performance;  Streaming networks;  Streaming systems;  Video on demand services, Video on demand},
publisher={Institute of Electrical and Electronics Engineers Inc.},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hanhirova201459,
author={Hanhirova, J. and Hirvisalo, V.},
title={Performance simulation of heterogeneous computing systems},
journal={Modelling and Simulation 2014 - European Simulation and Modelling Conference, ESM 2014},
year={2014},
pages={59-64},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922319565&partnerID=40&md5=5d7588984a0f5148cff6042389303e74},
abstract={We have developed a simulation environment (PSE) for performance analysis of heterogeneous computing systems. Our focus is on. systems with many core accelerators. As multiprocessor chips are constantly evolving, simulation is a viable tool to explore different configurations and to understand the hardware-software interaction. PSE is a resource reservation based simulator for layered models that describe the resource provisioning and resource usage within a system. To illustrate our methodology and our simulation environment, we present a small-scale demonstrative experiment. The demonstrative experiment shows how accelerated processing can be modeled. In the model, we connect a GPU into a NPU for video stream processing.},
author_keywords={Communication processors;  Computer performance;  Heterogeneous computing systems;  Performance analysis;  Simulation interfaces},
keywords={Modal analysis;  Video signal processing;  Video streaming, Communication processors;  Computer performance;  Hardware-software interactions;  Heterogeneous computing system;  Many-core accelerators;  Performance analysis;  Performance simulation;  Simulation environment, Computer software},
publisher={EUROSIS},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mory201283,
author={Mory, M. and Siegmund, N. and Blankenburg, A. and Saake, G.},
title={Towards Interoperability of distributed interactive simulations through node-based openGL stream processing},
journal={CEUR Workshop Proceedings},
year={2012},
volume={915},
pages={83-87},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891915861&partnerID=40&md5=4c23d0e5de98c7ccc7ab26fff22a9b54},
abstract={Coupling heterogeneous simulation environments is a requirement in the engineering domain. A novel approach exploits simulations' visualizations to setup distributed simulations: OpenGL stream processors intercept the dialogue between OpenGL clients and servers, and (distributedly) process the request-reply stream. As a first step towards an holisitic approach, we describe the Vanadium node-based OpenGL stream processing framework from the viewpoint of an integrated interoperability model. Results are a systematic examination of the framework's capabilities and limits; identification of open research questions; and an initial benchmark for architectural comparison with other OpenGL stream processing frameworks.},
keywords={Distributed interactive simulation;  Distributed simulations;  Engineering domains;  Heterogeneous simulation;  Interoperability modeling;  Research questions;  Stream processing;  Stream processor, Application programming interfaces (API), Interoperability},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hassan2012,
author={Hassan, Md.M. and Neng, C.K. and Suan, L.C.},
title={Performance analysis of video streaming on different hybrid CDN & P2P infrastructure},
journal={IET Conference Publications},
year={2012},
volume={2012},
number={614 CP},
doi={10.1049/cp.2012.2087},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880155472&doi=10.1049%2fcp.2012.2087&partnerID=40&md5=103c4e3a55fea782bb121464347ecb1b},
abstract={With the rapid expansion of network application over the Internet, the deployment of live streaming systems become more and more popular, significantly enhancing the user experience. As the numbers of users are expanding significantly, the traditional Client-Server architecture can't afford the number of growing users. Therefore the video quality of service and network scalability is become so deprived. Based on the demand of users and drawbacks of client-server architecture, institutes and researchers have provided effective solutions to contrive new application systems called Content Delivery Network (CDN) and Peerto-Peer (P2P) network. Recently, researchers have proposed a hybrid approach that amalgamates both CDN and P2P was proposed in the literature. The goal of this paper is to simulate and investigate the performance of hybrid CDN and P2P on live-casting video distribution, and contrast it against our propounded hybrid CDN and P2P with hierarchical arrangement called Dynamic Mobile Server (DMS), in terms of average frame loss ratio, packet loss ratio and peak signal noise ratio.},
author_keywords={Content delivery network;  Dynamic mobile server;  Peak signal noise ratio;  Peer-to-peer network;  Proxy server},
keywords={Client-server architectures;  Content delivery network;  Mobile servers;  Network applications;  Peak signal noise ratio;  Peer to Peer (P2P) network;  Performance analysis;  Proxy server, Client server computer systems;  Information dissemination;  Quality of service;  Video streaming;  Wireless telecommunication systems, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang20112065,
author={Zhang, Y.},
title={Data parallel programming model for many-core architectures},
journal={IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
year={2011},
pages={2065-2068},
doi={10.1109/IPDPS.2011.378},
art_number={6009018},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455262069&doi=10.1109%2fIPDPS.2011.378&partnerID=40&md5=ba29b55a0416b6f6c34a8d78f773a3d2},
abstract={Emerging accelerating architectures, such as GPUs, have proved successful in providing significant performance gains to various application domains. This is done by exploiting data parallelism in existing algorithms. However, programming in a data-parallel fashion imposes extra burdens to programmers, who are used to writing sequential programs. New programming models and frameworks are needed to reach a balance between programmability, portability and performance. We start from stream processing domain and propose GStream, a general-purpose, scalable data streaming framework on GPUs. The contributions of GStream are as follows: (1) We provide powerful, yet concise language abstractions suitable to describe conventional algorithms as streaming problems. (2) We project these abstractions onto GPUs to fully exploit their inherent massive data-parallelism. (3) We demonstrate the viability of streaming on accelerators. Experiments show that the proposed framework provides flexibility, programmability and performance gains for various benchmarks from a collection of domains, including but not limited to data streaming, data parallel problems, numerical codes and text search. This work lays a foundation to our future work to develop more general data parallel programming models for many-core architectures. © 2011 IEEE.},
keywords={Application domains;  Conventional algorithms;  Data parallel;  Data parallelism;  Data streaming;  Data-parallel programming;  Many-core architecture;  Numerical code;  Performance Gain;  Programmability;  Programming models;  Sequential programs;  Stream processing;  Text search, Abstracting;  Algorithms;  Benchmarking;  Data reduction;  Distributed parameter networks;  Program processors, Parallel programming},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Oiki2011,
author={Oiki, T. and Suzumura, T.},
title={Tonegawa: Highly scalable distributed web server with data stream processing},
journal={IEEE International Conference on Communications},
year={2011},
doi={10.1109/iccw.2011.5963582},
art_number={5963582},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052003663&doi=10.1109%2ficcw.2011.5963582&partnerID=40&md5=6c5368b7673fb1267845f5aa9b4f074c},
abstract={We developed Tonegawa, which is a distributed Web server on top of a stream processing system called System S under development by IBM Research. Tonegawa takes staged execution approach and a web request is processed over multiple stages, each of which can be processed by one multi-core node or even distributed to a cluster of nodes. We conducted the performance evaluation of Tonegawa with the use of both static contents and dynamic contents, and showed that Tonegawa on multiple nodes can be utilized in the case where large computational effort is required. In addition, we demonstrated the qualitative software productivity with regards to the implementation of a web server using such a stream-style programming, which decreases potential bugs and brings highly extensibility to the system. © 2011 IEEE.},
keywords={Cluster of nodes;  Computational effort;  Data stream processing;  Distributed web server;  Dynamic content;  Multi core;  Multiple nodes;  Performance evaluation;  Software productivity;  Stream processing systems;  Web requests;  Web servers, Data handling;  Microprocessor chips;  Program debugging;  Software engineering;  Web services, User interfaces},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2010709,
author={Chen, Q. and Hsu, M.},
title={Data stream analytics as cloud service for mobile applications},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6427 LNCS},
number={PART 2},
pages={709-726},
doi={10.1007/978-3-642-16949-6_4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649995938&doi=10.1007%2f978-3-642-16949-6_4&partnerID=40&md5=8a04744e4354ce2f079ca94a6dfebe36},
abstract={Many mobile applications are based on cloud services such as location service, messaging service, etc. Currently most cloud services are based on statically prepared information rather than the real-time analytics results of dynamically captured events. A paradigm shift is to take Continuous Stream Analytics (CSA) as a cloud service, which, however, poses several specific challenges in scalability, latency, time-window semantics and transaction control. In this work we extend the SQL query engine to unify the processing of static relations and dynamic streams for providing the platform support of CSA service. This platform is significantly differentiated from the current generation of stream processing systems which are in general built separately from the database engine thus unable to take advantage of the functionalities already offered by the existing data management technology, and suffer from the overhead of inter-platform data access and movement. To capture the window semantics in CSA, we introduce the cycle-based query model and support it in terms of the cut-and-rewind query execution mechanism. This mechanism allows a SQL query to run cycle by cycle for processing the unbounded stream data chunk by chunk, but without shutting the query instance down between chunks for continuously maintaining the application state across the execution cycles, as required by sliding-window oriented operations. We also propose the cycle-based transaction model with cycle-based isolation and visibility. To scale-up analytics computation, we introduce the parallel infrastructure with multi-engines cooperated and synchronized based the common data chunking criteria without centralized coordination. To scale-up service provisioning, we investigate how to stage the continuously generated analytics results efficiently through metadata manipulation without physical data moving and copying. We have prototyped our approach by extending the PostgreSQL, resulting in a new kind of tightly integrated, highly efficient platform for providing CSA service. We tested the throughput and latency of this service using a well-known stream processing benchmark and with WebOS based Palm phones. The test results show that the proposed approach is highly competitive. Providing CSA cloud service using HP Neoview parallel database engine is currently explored. © 2010 Springer-Verlag.},
keywords={Cloud services;  Current generation;  Data access;  Data management;  Data stream;  Database engine;  Dynamic streams;  Execution cycles;  Location services;  Messaging services;  Mobile applications;  Paradigm shifts;  Parallel Database;  Physical data;  PostgreSQL;  Query execution;  Query model;  Scale-up;  Scale-up services;  Sliding-window;  SQL query;  Stream data;  Stream processing;  Stream processing systems;  Test results;  Time windows;  Transaction model;  Current generation;  Location services;  Management technologies;  Messaging services;  Mobile applications;  Real-time analytics;  Stream processing;  Stream processing systems, Internet;  Metadata;  Semantics;  Web services;  Distributed database systems;  Engines;  Information management;  Mobile computing;  Mobile telecommunication systems;  Query processing;  Semantics;  Web services, Hydraulics;  Location based services},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Giacomazzi201089,
author={Giacomazzi, P. and Poli, A.},
title={Push-pull techniques in peer-to-peer video streaming systems with tree/forest topology},
journal={2010 International Congress on Ultra Modern Telecommunications and Control Systems and Workshops, ICUMT 2010},
year={2010},
pages={89-95},
doi={10.1109/ICUMT.2010.5676653},
art_number={5676653},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951501996&doi=10.1109%2fICUMT.2010.5676653&partnerID=40&md5=b39fcd0c47f077e84c535fcb21aed336},
abstract={Peer-to-peer video streaming systems are overlay networks used to distribute, among other types of content, live video content to large sets of users by relying on computing and network resources directly provided by users that are receiving video streaming services. Peer-to-peer video streaming systems with tree or forest topology are typically push-based, since the video content is provided by parent peers with no need for periodical requests. In this paper we analyze the impact of two complementing pull-based mechanisms aiming at improving the overall performance of the overlay network. Results show that the proposed hybrid push-pull approaches can be beneficial when the stability of the system is low, i.e., the average permanence time of peers is short. ©2010 IEEE.},
author_keywords={Backup parents;  Peer-to-peer;  Performance analysis;  Retransmission;  Tree;  Video streaming},
keywords={Backup parents;  Peer-to-peer;  Performance analysis;  Retransmissions;  Tree, Acoustic streaming;  Control systems;  Distributed computer systems;  Overlay networks;  Topology;  Video recording;  Video streaming;  Videotex, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Giacomazzi201096,
author={Giacomazzi, P. and Poli, A.},
title={Performance analysis of optimization techniques in peer-to-peer video streaming systems with tree/forest topology},
journal={2010 International Congress on Ultra Modern Telecommunications and Control Systems and Workshops, ICUMT 2010},
year={2010},
pages={96-102},
doi={10.1109/ICUMT.2010.5676652},
art_number={5676652},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951469812&doi=10.1109%2fICUMT.2010.5676652&partnerID=40&md5=cf0a5b345ba4febba8a79f7f521ac649},
abstract={Peer-to-peer video streaming systems are overlay networks used to distribute, among other types of content, live video content to large sets of users by relying on network and computing resources directly provided by users that are receiving the video stream. In this paper, we analyze the impact of two optimization techniques that can be adopted in peer-to-peer video streaming systems, with tree or forest topology, to cope with the negative effects of user's leaves. We carry out a performance analysis of these systems, analyzing their sensitivity to the most critical system parameters, when nearly-permanent nodes, i.e., peers with a smaller-than-average leave rate, are used to optimize the overlay topology. ©2010 IEEE.},
author_keywords={Optimization techniques;  Peer-to-peer;  Performance analysis;  Permanent nodes;  Tree;  Video streaming},
keywords={Optimization techniques;  Peer-to-peer;  Performance analysis;  Permanent nodes;  Tree, Acoustic streaming;  Control systems;  Distributed computer systems;  Optimization;  Overlay networks;  Topology;  Video streaming;  Videotex, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Deng2010,
author={Deng, H. and Zhong, L. and Ye, M.},
title={Assessing forecasting model performance for distributed stream processing systems},
journal={International Conference on Internet Technology and Applications, ITAP 2010 - Proceedings},
year={2010},
doi={10.1109/ITAPP.2010.5566395},
art_number={5566395},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958123135&doi=10.1109%2fITAPP.2010.5566395&partnerID=40&md5=9ac58a40d7ca1d09e4ee09a86a67efb9},
abstract={Load forecasting plays an important role in the load balancing of the distributed stream processing systems. So, the performance and cost of three models: weighted moving average (WMA), exponential smoothing (ES) and grey model (GM(1,1)) are empirically evaluated by running three typical test cases on the load traces of the distributed stream processing systems and their results are reviewed according to three metrics: mean absolute percentage errors (MAPE), root of mean square error (RMSE), processing cost. According to the metrics of MAPE and RMSE, GM(1,1) performs best while WMA and ES perform much better than GM(1,1) according to the processing cost. However, when the load fluctuates dramatically, the prediction precision of the above models is low. ©2010 IEEE.},
author_keywords={Data stream;  Linear time series model;  Load balancing;  Load forecasting;  Performance evaluation},
keywords={Data stream;  Linear time series model;  Load forecasting;  Load-Balancing;  Performance evaluation, Data communication systems;  Distributed parameter control systems;  Hydraulics;  Internet;  Parallel architectures;  System theory;  Time series, Electric load forecasting},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Giacomazzi2009,
author={Giacomazzi, P. and Poli, A.},
title={Rewarding techniques in peer-to-peer video streaming systems with tree and forest topology},
journal={2009 International Conference on Ultra Modern Telecommunications and Workshops},
year={2009},
doi={10.1109/ICUMT.2009.5345467},
art_number={5345467},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549194167&doi=10.1109%2fICUMT.2009.5345467&partnerID=40&md5=5321c2b812381d8120e1b428f248fe42},
abstract={Peer-to-peer networks are an increasingly popular solution for the distribution of media content to a large number of users, with limited investments for network infrastructures. The distribution of a real time video stream imposes strict performance requirements such as small playback delays and few frame losses. However, performances are greatly affected by the upload bandwidth of the access networks of peers. In this paper we propose a set of rewarding techniques able to cleverly exploit the use of peers' upload bandwidth, and provide a sensitivity analysis through which we determine the conditions under which the rewarding techniques are beneficial. The study, carried out through simulation, considers a general peer-to-peer video streaming reference model with tree/forest topology. From our point of view, the proposed rewarding technique is simple enough to be used as a benchmark for the evaluation of more sophisticated approaches. ©2009 IEEE.},
author_keywords={Peer-to-peer;  Performance analysis;  Rewarding;  Tree;  Video streaming},
keywords={Access network;  Frame loss;  Media content;  Network infrastructure;  Peer to peer;  Peer-to-peer video streaming;  Performance analysis;  Performance requirements;  Playback delay;  Real-time video streams;  Reference models, Acoustic streaming;  Distributed computer systems;  Sensitivity analysis;  Topology;  Video streaming;  Videotex, Peer to peer networks},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu200925,
author={Liu, J. and Wu, Q. and Liu, W.},
title={Temporal restriction query optimization for event stream processing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2009},
volume={5731 LNCS},
pages={25-35},
doi={10.1007/978-3-642-03996-6_3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349305377&doi=10.1007%2f978-3-642-03996-6_3&partnerID=40&md5=28bc839ea1cc66139e841ff977145039},
abstract={The trend that organizations are linking Service Oriented Architecture (SOA) efforts closely to real-time processes makes research and industrial community increasingly focus on the SOA and Event Stream Processing (ESP) connection. ESP needs to correlate multiple continuous events involved in complex temporal relationship and attribute logic relationship to more abstract complex events in richer semantic. Due to high speed arrival rate of events and vast volume of registered complex event queries, memory consumption and incremental event query evaluation demand a comprehensive dedicate event stream processing framework with low-latency and high scalability. In this paper, we study problems of query optimization for ESP, especially topics on temporal restriction query. We first propose a framework to integrate ESP features with business process management and monitor. We then describe a query plan-based approach to efficiently execute ESP queries. Our approach uses algebraic conversion to efficiently handle temporal restriction queries, which are a key component of complex event processing, and computes temporal relevance condition at compile time to obtain event relevance time for a given expression. We demonstrate the effectiveness of our approach through a performance evaluation of our prototype implementation. © 2009 Springer Berlin Heidelberg.},
keywords={Arrival rates;  Business process management;  Compile time;  Complex event processing;  Complex temporal relationships;  Event streams;  Industrial communities;  Key component;  Low-latency;  Memory consumption;  Performance evaluation;  Plan-based;  Prototype implementations;  Query evaluation;  Query optimization;  Real-time process;  Temporal relevance;  Temporal restrictions, Enterprise resource management;  Information management;  Information services;  Service oriented architecture (SOA);  Software prototyping;  Technical presentations;  Temporal logic, Industrial research},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2008,
author={Liu, H. and Riley, G. and Ingle, R.},
title={Mathematical model and analysis of peer-to-peer IPTV},
journal={2008 2nd International Symposium on Advanced Networks and Telecommunication Systems, ANTS 2008},
year={2008},
doi={10.1109/ANTS.2008.4937793},
art_number={4937793},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650567524&doi=10.1109%2fANTS.2008.4937793&partnerID=40&md5=d64ac520bcc9c47bfcad8fc907530add},
abstract={Peer-to-peer overlay is an attractive solution to distribute video streams over large-scale IP networks. A number of algorithms and frameworks have been proposed. But generic and theoretical analysis of P2P streaming is scarce in the literature. In this paper, we present a novel probability model, making the formal analysis and performance evaluation of P2P IPTV accessible. With the help of the proposed model, we then reveal that the efficiency of P2P streaming system is closely correlated to piece diversity.},
keywords={Attractive solutions;  Formal analysis;  Large-scale IP networks;  P2P streaming;  Peer to peer;  Peer-to-peer overlays;  Performance evaluation;  Probability models;  Video streams, Computer networks;  Mathematical models;  Telecommunication systems, Television broadcasting},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nishikawa2008655,
author={Nishikawa, H. and Tomiyasu, H. and Uchida, H.},
title={VLSI design of networking-oriented chip multi-processor: CUE-v3},
journal={Proceedings of the 2008 International Conference on Parallel and Distributed Processing Techniques and Applications, PDPTA 2008},
year={2008},
pages={655-661},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-62749127955&partnerID=40&md5=7d90f473d0b8b868f8b7dd6c0ebc61f7},
abstract={To realize secure and safe information communication environment, the authors are carrying out CUE (Coordinating Users' requirements and Engineering constraints) project and have designed a data-driven chip-multiprocessor CUE-v3 for ad hoc and ubiquitous communication environment available even in the case of emergency. Since CUE-series data-driven processors were designed to be an embedded programmable component as well as a multi-processor element, particular design considerations were taken to achieve pipelined stream processing and real-time processing for multimedia communication environment. In CUE project, an emulation/simulation facility RESCUE (Real-time Execution System for CUE-series data-driven processors) was also built to develop scalable chip multiprocessors in self-evolutional manner. Through evaluations by RESCUE, the latest version named CUE-v2 was developed. CUE-v2 is build as a hybrid processor enabling simultaneous processing of data-driven and control-driven threads to achieve higher performance for parallel processing and to avoid bottlenecks in sequential parts of real-time programs frequently encountered in time-sensitive applications. After discussing requirements to a platform for the ad hoc and ubiquitous communication environment, data-driven chip multi-processor CUE-v3 has been developing by integrating 4 CUE-v2's. In this paper, the CUE-v3 chip multiprocessor architecture including processing element and inter-connection network is firstly introduced. VLSI design and its evaluation will be finally addressed showing effectiveness of the CUE-v3 architecture with special emphasis on scalability and multi-processing capability. The authors have already confirmed scalability of CUE-v3 chip multi-processor architecture through VLSI design and performance evaluations and even if it sees moderately, the CUE-v3 architecture will achieve scalable performance improvement up to 16 processing elements.},
author_keywords={Ad hoc;  Chip-multi-processor;  Dataflow;  Realtime;  Ubiquitous},
keywords={Ad hoc;  Chip-multi-processor;  Dataflow;  Realtime;  Ubiquitous, Ad hoc networks;  Applications;  Architectural design;  Communication;  Computer architecture;  Concurrency control;  Data processing;  Design;  Distributed parameter networks;  Microprocessor chips;  Multimedia systems;  Program processors;  Scalability;  Systems analysis;  Time series, Pipeline processing systems},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mansour2008476,
author={Mansour, H. and Krishnamurthy, V. and Nasiopoulos, P.},
title={Delay-aware rate control for multi-user scalable video streaming over mobile wireless networks},
journal={IWCMC 2008 - International Wireless Communications and Mobile Computing Conference},
year={2008},
pages={476-481},
doi={10.1109/IWCMC.2008.83},
art_number={4599982},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949091600&doi=10.1109%2fIWCMC.2008.83&partnerID=40&md5=efe80e1cc4afdc4620a559753c5e9df1},
abstract={In this paper, we propose a delay and capacity constrained multi-user scalable video streaming scheme that improves the average end-to-end distortion of transmitted video streams compared to traditional streaming strategies. Wireless video streaming applications are characterized by their bandwidth-intensity, delay-sensitivity, and loss-tolerance. Our main contributions include: (i) an analytical expression for packet delay and play-out deadline of unequal erasure protection (UXP) protected scalable video, (ii) an analysis of the performance of delay-aware, capacity-aware rate allocation for optimized UXP streaming scenarios, (iii) proof that unequal error protection causes a rate-constrained optimization problem to be non-convex. Performance evaluations using a 3GPP network simulator show that, for different channel capacities and packet loss rates, delay-aware non-stationary rate-allocation delivers significant gains which range between 1.65dB to 2dB in average Y-PSNR of the received video streams over delay-unaware strategies. These gains come at a cost of increased off-line computation which is performed prior to the streaming session and therefore, do not affect the run-time performance of the streaming system. © 2008 IEEE.},
author_keywords={Streaming delay analysis;  SVC;  UXP;  Wireless video streaming},
keywords={Analytical expressions;  Constrained optimization problems;  End-to-end distortion;  Mobile wireless networks;  Multi users;  Network Simulator;  Non-stationary;  Off-line computation;  Packet delays;  Packet loss rates;  Performance evaluations;  Rate allocation;  Rate Control;  Run-time;  Scalable video;  Scalable video streaming;  Streaming delay analysis;  Streaming session;  SVC;  Unequal error protection;  UXP;  Wireless communications;  Wireless video streaming, Acoustic streaming;  Channel capacity;  Constraint theory;  Electric breakdown;  Error analysis;  Image coding;  Mobile computing;  Optimization;  Sensitivity analysis;  Telecommunication;  Video streaming;  Videotex;  Wireless networks;  Wireless telecommunication systems, Constrained optimization},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chan2008263,
author={Chan, P.H. and Lee, J.Y.B.},
title={An efficient disk-array-based server design for a multicast video streaming system},
journal={International Journal of Embedded Systems},
year={2008},
volume={3},
number={4},
pages={263-270},
doi={10.1504/IJES.2008.022397},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149333390&doi=10.1504%2fIJES.2008.022397&partnerID=40&md5=8b6cbfddee7b589fb5fb06ef1e7b718a},
abstract={Recently, new Video-on-Demand (VoD) architectures using batching, patching and periodic broadcasting are introduced that are much more scalable than traditional unicast VoD systems. However the problem of designing an efficient server to implement these new multicast VoD architectures has received little attention. While existing server designs using round-based schedulers can still be used, results show that such designs are sub-optimal as they do not exploit the characteristics of fixed-schedule periodic broadcasting channels. This work addresses this challenge by presenting an efficient server design that can increase the system capacity by up to 60% compared to traditional video server designs. Copyright © 2008, Inderscience Publishers.},
author_keywords={Disk scheduling;  Multicast;  Performance analysis;  Video server},
keywords={Multicasting;  Scheduling;  Video on demand, Disk array;  Disk scheduling;  Multicast video streaming;  Performance analysis;  Periodic broadcasting;  System Capacity;  Video on demands (VoD);  Video servers, Video streaming},
publisher={Inderscience Publishers},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Asif2006,
author={Asif, M.S. and Omer, M. and Luna, A. and Sheikh, N.M.},
title={A multiprocessor framework for rapid-prototyping and evaluation of soft transceivers},
journal={2006 IEEE GCC Conference, GCC 2006},
year={2006},
doi={10.1109/IEEEGCC.2006.5686260},
art_number={5686260},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79851493152&doi=10.1109%2fIEEEGCC.2006.5686260&partnerID=40&md5=97c1475f0a98a1e39e4c0de53daea7ef},
abstract={Recent years have seen rapid evolution in the architectures being explored for realizing high-speed software-defined radios. There is, however, a distinct need for a low-cost programmable platform where algorithms for base-band transceivers can be rapidly prototyped and tested with real-world data, streaming in from diverse sources of telecommunication traffic. This paper explores an analytical method for laying out such a generic platform. It investigates the constraints involved in realizing such a platform, and the minimum functionality needed within the solution so as to provide adequate scalability to allow the implementation of a wide variety of communication algorithms. The paper concludes with a case study of a multi-channel communication system that has been successfully implemented on the proposed platform, highlighting the performance benchmarks it had to meet in order to prove suitable for the task of communication system evaluation.},
author_keywords={Analog Front End (AFE);  Buffers;  Data converters;  DSP;  MIPS;  PAM (pulse amplitude modulation)},
keywords={Analog front end;  Buffers;  Data converters;  DSP;  MIPS;  PAM (pulse amplitude modulation), Algorithms;  Amplitude modulation;  Communication systems;  Plasma waves;  Telecommunication traffic;  Transceivers, Pulse amplitude modulation},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Huang2005241,
author={Huang, J. and Feng, W.-C. and Walpole, J. and Jouve, W.},
title={An experimental analysis of DCT-based approaches for fine-grain multi-resolution video},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2005},
volume={5680},
pages={241-252},
doi={10.1117/12.592317},
art_number={24},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-21844460825&doi=10.1117%2f12.592317&partnerID=40&md5=15391e2931f3c49f900fd3e1fa478b64},
abstract={This paper presents the architectural trade-offs to support fine-grain multi-resolution video over a wide range of resolutions. In the future, video streaming systems will have to support video adaptation over an extremely large range of display requirements (e.g. 90×60 to 1920×1080), While several techniques have been proposed for multi-resolution video adaptation, which is also known as spatial scalability, they have focused mainly on limited spatial resolutions. In this paper, we examine the ability of current techniques to support wide-range spatial scalability. Based upon experiments with real video, we propose an architecture that can support wide-range adaptation more effectively. Our results indicate that multiple encodings with limited spatial adaptation from each encoding provides the best trade-off between efficient coding and the ability to adapt the stream to various resolutions. © 2005 SPIE and IS&T.},
author_keywords={DCT;  Multi-resolution video},
keywords={Cameras;  Image analysis;  Image coding;  Image compression;  Information analysis;  Real time systems, DCT;  Multi-resolution;  Video;  Video adaptation, Video signal processing},
language={English},
document_type={Conference Paper},
source={Scopus},
}
